%!TEX program = xelatex
\documentclass[twoside,a4paper]{report}
\usepackage[ngerman, english]{babel}
\usepackage{comment}
\usepackage{csquotes}
\usepackage{amsmath,amsfonts,amssymb,amsthm,bbm,mathrsfs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{showidx}
\usepackage{pgf,tikz,pgfplots}
\usetikzlibrary{arrows}
\pgfplotsset{compat=1.16}
\makeindex

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{property}[theorem]{Property}
\newtheorem{properties}[theorem]{Properties}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}

\numberwithin{equation}{chapter}

\hypersetup{%
  colorlinks = true
}

%Afkortingen voor wiskundige symbolen
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\C}{\mathbb{C}}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\1}{\mathbbm{1}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\G}{\mathcal{G}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\range}{range}

\newcommand{\Pmod}{\mathcal{P}^*}
\newcommand{\Psafe}{\tilde{\P}}
\newcommand{\Uonesafe}{\tilde{\1}_U}
\newcommand{\Usafe}{\tilde{U}}

\newcommand{\EnvIndSafe}{\1_{\{B=2A\}}}
\newcommand{\DieInd}{\1_{\{Y=3\}}}
\newcommand{\DieIndSafe}{\tilde{\1}_{\{Y=3\}}}
\newcommand{\ChildInd}{\1_{\{U=bg\}}}
\newcommand{\ChildIndSafe}{\tilde{\1}_{\{U=bg\}}}
\newcommand{\ChildTwoInd}{\1_{\{U=bb\}}}
\newcommand{\ChildTwoIndSafe}{\tilde{\1}_{\{U=bb\}}}
\newcommand{\GeneralInd}{\1_{\{U=u'\}}}
\newcommand{\GeneralGenInd}{\1_{\{U\in\Y'\}}}
\newcommand{\GeneralGenIndSafe}{\tilde{\1}_{\{U\in\Y'\}}}

\title{Thesis}
\author{Mathijs Kolkhuis Tanke}
\date{\today}

\begin{document}
\begin{titlepage}
\maketitle
\end{titlepage}

\begin{abstract}

\end{abstract}

\tableofcontents

\newpage

\chapter{Introduction}
Probability theory is one of the most important and most researched fields in mathematics. It is entirely based on three axioms first stated by Andrey Kolmogorov \cite{Kolmogorov33}, namely that the probability of an event is non-negative, the probability measure is a unit measure and the probability measure is countably additive on disjoint sets. These three axioms however do not prevent the existence of certain paradoxes, like the Borel-Kolmogorov paradox, Monty Hall's problem and the Sleeping Beauty problem. Some paradoxes arise from wrongly using probability theory, others are a misinterpretation of results.

This thesis focuses mainly paradoxes arising from conditional probability, such as the Borel-Kolmogorov paradox, Monty Hall's problem and the two envelope problem. We study will these problems and address their paradoxical nature. Ultimately it will be shown that all paradoxes arise by incorrectly applying conditional probability.

Take a look at the Borel-Kolmogorov paradox first. A sphere is equipped with the uniform probability measure. Suppose a point exists on a great circle of the sphere, but you do not know where that point is. Is there a probability distribution on this great circle for that point? If one models this problem using longitudes, the conditional distribution on the great circle is uniform. However, if one models this problem using meridians, the conditional distribution is a cosine. Borel \cite{Borel09} and Kolmogorov \cite{Kolmogorov33} both addressed this problem and Kolmogorov gave the following answer:

\foreignblockquote{ngerman}[Andrey Kolmogoroff, \cite{Kolmogorov33}]{Dieser Umstand zeigt, daß der Begriff der bedingten Wahrscheinlichkeit in bezug auf eine isoliert gegebene Hypothese, deren Wahrscheinlichkeit gleich Null ist, unzulässig ist: nur dann erhält man auf einem Meridiankreis eine Wahrscheinlichkeitsverteilung für [Breite] Theta, wenn dieser Meridiankreis als Element der Zerlegung der ganzen Kugelfläche in Meridiankreise mit den gegebenen Polen betrachtet wird.}

In summary, Kolmogorov states that the concept of conditional probability on a great circle is inadmissible, since the event that the point lies on a great circle of the sphere has zero measure. Furthermore, the sole reason of a cosine distribution on a great circle arising when considering meridians is that the meridian circle serves as an element of the decomposition of the whole spherical surface in meridian circles with the poles considered.

Despite Kolmogorov's explanation this problem is still upon debate. Recently Gyenis, Hofer-Szabó and Rédei \cite{Gyenis17} studied this problem and provided more insight and an in my opinion satisfying discussion to the problem, eventually drawing the same conclusion as Kolmogorov already did. This problem is more elaborately discussed in chapter~\ref{chap:BorelKolmogorov}.

Another paradox arising from conditional probability is Monty Hall's problem. In Monty Hall's problem, a player is facing three doors called $a$, $b$ and $c$. One door opens to a car and the other two have goats. Suppose the player initially chooses door $a$. The game master then opens either door $b$ or $c$, but always a door with a goat. The player now faces two doors and is asked whether he wants to switch. One possible solution is that the player faces two doors without any preference for either door, thus the probability is 50\%. Another possible solution is that first the player had a 33\% chance of correctly guessing the door with the car. If for example door $c$ is opened, door $b$ remains with a conditional probability of 67\% of having the car. Which solution is correct?

Let $\X=\{a,b,c\}$ be the set of doors and $U$ the random variable denoting the location of the car. Conditional probability then states
\begin{equation*}\label{eq:IntMonty}
\P[U=b\mid U\in\{a,b\}]=\frac{\P[U=b,U\in\{a,b\}]}{\P[U\in\{a,b\}]}=\frac{\frac{1}{3}}{\frac{2}{3}}=\frac{1}{2},
\end{equation*}
supporting the claim that a probability of 50\% is the correct answer. However, the space conditioned on door $c$ is opened is $\{a,b\}$ and the space conditioned on door $b$ is opened is $\{a,c\}$. If door $a$ has the car, the game master can open either door and the set of events we must condition on is $\{\{a,b\},\{a,c\}\}$. These two events do not form a partition, preventing us from using traditional conditional probability.

Thus in addition to Kolmogorov's statement, we must not only be wary for conditioning on events with measure 0, we cannot condition on arbitrary events at all. I propose that when using conditional probability, one must provide a pair of a sub-$\sigma$-algebra and the event from that $\sigma$-algebra to condition on. Regarding Monty Hall's problem there is no $\sigma$-algebra on $\{a,b,c\}$ containing the set $\{\{a,b\},\{a,c\}\}$. In the case of the Borel-Kolmogorov paradox providing sub-$\sigma$-algebras immediately make clear why the conditional distribution on a great circle is not unique, as both calculations are supported by different sub-$\sigma$-algebras.

The crux of the Monty Hall problem is that the initial distribution of the car is unknown and the player does not know with which probability door $b$ is opened given the car is behind door $a$. Thus there is a set of different probability distributions where one is the correct distribution, but we do not know which one is. Using the theory of safe probability we can obtain a strategy that give equal results for all distributions in a specified model. This theory is introduced by Grünwald \cite{Grunwald18} and here in chapter~\ref{chap:SafeProp}. Safe probability is then applied in chapter~\ref{chap:DiscPara} to problems as the Monty Hall problem and to the two envelope problem in chapter~\ref{chap:TwoEnvelope}.

In chapter~\ref{chap:DiscPara} the results of the analysis of Monty Hall's problem are generalized to a theorem, which can be used to provide safe distributions for other problems like the boy or girl problem.
\begin{theorem*}
Let $\X$ and $\Y$ be countable. Let $U$ be a random variable on $\Y$ and $V$ be a random variable on $\X$. Let $p_u\in[0,1]$ with $\sum_{u\in\Y}p_u=1$ and let
\begin{equation*}
\Pmod\subseteq\{\P\mid\forall u\in\Y:\P[U=u]=p_u\}
\end{equation*}
be our model of probability distributions on $\X\times\Y$. Suppose there is a \begin{equation*}
u'\in\bigcap_{\P\in\Pmod}\bigcap_{v\in\range_{\P}(V)}\range_{\P}(U|V=v).
\end{equation*}
Let $\Psafe$ be a distribution on $\X\times\Y$ with $\Psafe[U=u'|V=v]=p_{u'}$ for all $v\in\X$. This $\Psafe$ is safe for $\GeneralInd|[V]$.

If for all $v\in\X$ a $\P\in\Pmod$ exists such that $v\in\range_{\P}(V)$ holds, then this requirement on $\Psafe$ is necessary for safety for $\langle\GeneralInd\rangle|\langle V\rangle$.
\end{theorem*}

This theorem and its notation will be explained, proven and applied in chapter~\ref{chap:DiscPara}, but it essentially states that in the case of the Monty Hall problem if one assumes the car is initially distributed evenly between the doors, the probability of the car being behind the originally chosen door $a$ can be assumed to be $\frac{1}{3}$. Using this assumption one must always switch to the other door as the probability of the car being behind that door can be assumed to be $\frac{2}{3}$, resulting in a 67\% chance of winning the car. Note that this assumption is with probability 0 the correct distribution. However, this assumption always yields to a 67\% chance of winning the car independent on the probability of opening door $b$ when the car is behind door $a$.

Another paradox we will treat is the two envelope problem. There are two envelopes, where one is filled with value $a$ and the other with value $b$. The only thing the player knows is that either $a=2b$ or $b=2a$. An envelope is given to the player, with the player receiving either envelope with equal probability. Call this envelope $A$ and suppose the player observes value $a$. Should he switch to envelope $B$ or keep the contents of $A$?

At first glance, he should. Either $b=2a$ or $b=\frac{a}{2}$ holds with equal probability, thus \[\E[B|A=a]=\frac{1}{2}\cdot2a+\frac{1}{2}\cdot\frac{a}{2}=\frac{5}{4}a.\]
However, the player has received an envelope at random and only knows the contents of that particular envelope. So he could also have been given envelope $B$ with value $b$, for which $\E[A|B=b]=\frac{5}{4}b$ holds. Therefore no matter what envelope the player receives, he should switch. This solution must clearly be wrong and it is. When conditioning on $A=a$, then either $b=2a$ or $a=2b$ holds with probability $1$ in the conditioned probability space. This renders the previous calculation of $\E[B|A=a]$ to be incorrect. However, we do not know which event takes place.

Now let $x=\min\{a,b\}$ be the lowest value, then either $a=x$ or $a=2x$ holds with equal probability. For both envelopes we now have
\[
\E[A|B=b]=\E[B|A=a]=\frac{1}{2}x+\frac{1}{2}\cdot2x=\frac{3}{2}x.
\]
Both envelopes thus have on average the same contents, as is expected when the envelopes are handed out uniformly. Unfortunately, we do not know the value of $x$. Furthermore, if $x$ is picked from an unbounded set and we assume that given an observation the other envelope must hold twice as much or half the value with equal probability, then $x$ must be taken from a uniform probability distribution on the unbounded set. Such distribution does not exist. We conclude that the value of $\E[B|A=a]$ does depend on a prior distribution on the chosen value $x$.

There are two ways to address this problem. Using safe probability, like in the case of Monty Hall, we will device a strategy where the player wins $\frac{3}{2}\E[X]$ on average, where $X$ is any prior for the lowest value in the envelopes on the positive number line with finite expectation. This strategy is in fact that the player must flip a fair coin and switch when it lands heads. Another method is using a switching strategy introduced by McDonell and Abbott~\cite{McDonnell09,Abbott10,McDonnell11}. Let $f\colon (0,\infty)\to[0,1]$ be a function that takes an observation $a$ and equips it with a probability. The player must switch with probability $f(a)$ when observing value $a$. When $f$ is a decreasing function and strictly decreases on an interval where prior $X$ has positive measure, then switching using $f$ will always return on average a higher value than $\frac{3}{2}\E[X]$. If $f$ is a threshold switch, for example $f=\1_{(0,a^*]}$, then for some priors $X$ the strategy $f$ is actually the best strategy available. However, when playing the game, the player does not know how much strategy $f$ outperforms switching using safe probability. For any means prior $X$ gives $f$ an arbitrarily small advantage over safe probability, making it an unnecessarily complicated method to marginally increase the average value won.

The two envelope problem will be discussed further in chapter~\ref{chap:TwoEnvelope}.


\begin{comment}
A third still unsolved problem is the Sleeping Beauty problem. In this problem Sleeping Beauty is willing to do an experiment. At Sunday she goes to sleep. A fair coin is tossed with the following results: Sleeping Beauty wakes on Monday with heads and wakes on Monday and Tuesday with tails. If she is awake, she must state her credence of the coin giving heads. After the question, she goes to sleep using an amnesia-inducing drug which makes her forget that she has waken before. At Wednesday she wakes and the experiment is over.

A first argument of the probability distribution of the coin is given by the halfers. When Sleeping Beauty wakes, she has no clue what day it is. She knows that the coin is fair and has no reason guess otherwise, thus she must say the the coin is still fair.\\
Another argument is given by the thirders. Namely that there are three possible events, namely awake on Monday and coin has heads, awake on Monday and coin has tails and as last awake on Tuesday and coin has tails. She does not know which event she attends, thus the events happen with uniform probability. There is one event with the coin giving heads, thus the probability of the coin having heads is 33\$.

I propose a solution that the thirders use a different model than the halfers. The question concerns the probability distribution of the coin, which remains 50\%. The thirders answer the question `what is the probability of guessing correctly if I guess that the coin landed heads', which is 33\%. This solution will be discussed with more depth in chapter~\ref{chap:SleepingBeauty}.
\end{comment}

Every problem has a different reason for getting paradoxical results. There is one recurring theme with all paradoxes, namely when analysing these problems most of the times the underlying $\sigma$-algebra is not taken into account. This yields to various conflicting results, as conditioning on events that cannot be conditioned on to not recognizing that multiple probability distributions are possible. The theme of is thesis this therefore that when doing probability theory, the underlying probability space and $\sigma$-algebra must never be ignored and not providing a sub-$\sigma$-algebra with a conditional distribution must become a bad habit instead of an accepted practice.



\chapter{The Borel-Kolmogorov paradox}\label{chap:BorelKolmogorov}
The first paradox we will study in more detail is the Borel-Kolmogorov paradox. It is first studied by Borel \cite{Borel09} and Kolmogorov \cite{Kolmogorov33} and has sparked debate over the centuries afterwards\footnote{Bronnen lezen en invoeren.}. We will take a look the following version of the paradox.

Suppose a random variable has a uniform probability distribution on the unit sphere. If one conditions on a great circle, what will the resulting conditional distribution be? If the great circle is viewed as a longitude, the conditional distribution will be uniform as well. However, if the great circle is modelled as a meridian, the conditional distribution has a cosine as density function. We have two different conditional distributions on the same set, thus which one is correct?

As Kolmogorov \cite{Kolmogorov33} pointed out, the great circle has zero measure and therefore conditioning on a great circle must not be allowed. Our analysis will concern conditioning on zero sets using measure-theoretic conditional expectations as such conditioning is defined in that setting. The article of Gyenis, Hofer-Szabó and Rédei \cite{Gyenis17} is followed to compute the conditional distributions, who come to the same conclusion as Kolmogorov but with a more elaborate explanation.

Before we can start with the analysis, we need to define conditional expectations in the measure-theoretic setting. We will then prove that the conditional distributions on a great circle can differ and conclude from the results that conditioning on zero sets cannot be admissible. The Borel-Kolmogorov paradox will then be renamed to the \emph{Borel-Kolmogorov phenomenon}, as it is more an example on why you should not condition on zero sets than it is a paradox.

\section{Conditional expectation}
First we need to define conditional expectations. We will use the definitions given by David Williams \cite{Williams91}.

Let $\Omega$ be an arbitrary sample space and let $\mathcal{F}$ be a $\sigma$-algebra on $\Omega$. The indicator function $\1_A$ on a set $A$ is defined as
\begin{equation}
\1_A(x)=\begin{cases}1,&x\in A,\\0,&x\not\in A.\end{cases}
\end{equation}
Let $\B(A)$ be the $\sigma$-algebra of all Borel-measurable sets on $A\subseteq\R^n$. We will start with the definition of a random variable.

\begin{definition}[Random variable]
Let $\X$ be a measurable space. A function $X\colon\Omega\to\X$ is an \emph{$\X$-valued random variable} if it is $\F$-measurable, thus if $X^{-1}(A)\in\F$ holds for all $A\in\G$ where $\G$ is a $\sigma$-algebra on $\X$.
\end{definition}

We can now define the conditional expectation as definition 9.2 of \cite{Williams91}.

\begin{definition}[Conditional expectation]\label{def:conexp}
Let $X$ be an $\F$-measurable random variable with finite $\E[|X|]$. Let $\G$ be a sub-$\sigma$-algebra of $\F$. There exists a random variable $Y$ such that 
\begin{enumerate}
\item $Y$ is $\G$-measurable,
\item $\E[|Y|]$ is finite,
\item for every $G\in\G$ we have 
\begin{equation}
\int_G Yd\P=\int_G Xd\P.
\end{equation}
\end{enumerate}
$Y$ is called a \emph{version of the conditional expectation} $\E[X|\G]$ of $X$ given $\G$, written as $Y=\E[X|\G]$ almost surely. Moreover, if $\tilde{Y}$ is another random variable with these properties, then $\tilde{Y}=Y$ equal almost surely. Since two versions of $\E[X|\G]$ coincide almost surely, $Y$ is also called \emph{the} conditional expectation $\E[X|\G]$.
\end{definition}

Note that when $\G=\sigma(Z)$ is the smallest $\sigma$-algebra generated by a set $Z\in\F$, we also write $\E[X|\G]=\E[X|Z]$. This generalizes to $\E[X|\G]=\E[X|Z_1,Z_2,\ldots]$ when $\G=\sigma(Z_1,Z_2,\ldots)$.

The traditional definition of conditional expectation is that when $X$ and $Y$ are two random variables on $\R$, $f_{X,Y}$ is the joint density function of $X$ and $Y$ and $f_Y$ is the density function of $X$, the conditional expectation $\E[X|Y=y]$ is defined as
\begin{equation}
\E[X|Y=y]=\int_\R x\frac{f_{X,Y}(x,y)}{f_Y(y)}dx.
\end{equation}
Definition~\ref{def:conexp} agrees on this traditional usage. Take $\G=\sigma(\{Y=y\})$. The smallest $\sigma$-algebra generated by all $\omega\in\R$ such that $Y(\omega)=y$ and define
\begin{equation}
g(y)=\int_\R x\frac{f_{X,Y}(x,y)}{f_Y(y)}dx,
\end{equation}
then $g(Y)$ is a version of $\E[X|Y]$. The proof and a more general statement can be found in section~9.6 of \cite{Williams91}.

Definition~\ref{def:conexp} extends very nicely to the definition of conditional probability. Note that $\E[\1_F]=\P[F]$ holds for all $F\in\F$ and this carries over to conditional probabilities.

\begin{definition}[Conditional probability]
If $\G$ is a sub-$\sigma$-algebra of $\F$ and $F\in\F$, then the \emph{conditional probability} $\P[F|\G]$ is a version of $\E[\1_F|\G]$.
\end{definition}

\section{Formal description of the probability space}
\begin{figure}
\begin{center}
\input{figures/BorelTransform.tikz}
\caption{A visualization of coordinate transformation~\eqref{eq:BorelPolar} from Euclidean to spherical coordinates. The rectangle is the set $S$. The horizontal lines in the rectangle like $C$ parametrize the longitudes. The vertical lines in the rectangle like $M$ parametrize the meridians. Note that the two blue vertical lines of $M$ are distance $\pi$ apart, this is needed to fully parametrize a meridian.}
\label{fig:BorelVis}
\end{center}
\end{figure}

To analyse the Borel-Kolmogorov paradox, we need to formalize our probability space. The following construction is visualized in figure~\ref{fig:BorelVis}. Let $S$ be the unit sphere. For easier calculations, we equip $S$ with polar coordinates. Define $S=[0,2\pi)\times[0,\pi]$, then $S$ is described in the Euclidean space with the function
\begin{equation}\label{eq:BorelPolar}
S\to\R^3:(\phi,\psi)\mapsto(\cos\phi\sin\psi,\sin\phi\sin\psi,\cos\psi).
\end{equation}
Let $\B=\B(S)$ be the Borel-$\sigma$-algebra on $S$. The uniform distribution on $S$ is defined as
\begin{equation}
\P[B]=\frac{1}{4\pi}\iint_B\sin\psi d\psi d\phi
\end{equation}
for a $B\in\B$. The triple $(S,\B,\P)$ form a probability space.

The set of longitudes is described by 
\begin{equation}
\mathcal{C}=\{[0,2\pi)\times\{\psi\}\mid\psi\in[0,\pi]\}
\end{equation}
and the set of meridians is described by
\begin{equation}
\mathcal{M}=\{\{\phi,\phi+\pi\}\times[0,\pi]\mid\phi\in[0,\pi)\}.
\end{equation}

Note that the function in \eqref{eq:BorelPolar} is not a bijection. The image of $S$ is not one-to-one on the north and south pole, but this is a null set and will not cause any problems in our case. The reason why \eqref{eq:BorelPolar} is not made a formal bijection is that it eases notation.

\section{Conditional distributions}
We will now explore various ways to describe the conditional distribution on a great circle.
\subsection{Naive conditional probability}\label{sec:BorelNaive}
The first method is the naive method using traditional conditional probability. The probability space here is $(S,\B,\P)$. Let $F\in\B$ be a great circle.

Note that $\P[F]=0$, as a great circle always has zero measure. To be precise, let $f\colon S\to\R^3$ be the coordinate transformation of \eqref{eq:BorelPolar}. All great circles $F'$ have a rotation $O\colon\R^3\to\R^3$ such that $(f^{-1}\circ O\circ f)(F')=[0,2\pi)\times\left\{\frac{\pi}{2}\right\}$. Rotations are orthogonal and have determinant $1$, thus
\begin{align}
\P[F]&=\frac{1}{4\pi}\iint_F\sin\psi d\psi d\phi\\
&=\frac{1}{4\pi}\int_0^{2\pi}\int_{\frac{1}{2}\pi}^{\frac{1}{2}\pi}\sin\psi \det(f^{-1}\circ O\circ f)d\psi d\phi\\
&=\frac{1}{4\pi}\int_0^{2\pi}\int_{\frac{1}{2}\pi}^{\frac{1}{2}\pi}\sin\psi d\psi d\phi=0.
\end{align}
The inverse $f^{-1}$ is well-defined here since $f$ is a bijection locally around the circle $[0,2\pi)\times\left\{\frac{\pi}{2}\right\}$. Let $E\subset F$ be a measurable subset of $F$, then the conditional probability of $E$ given $F$ equals
\begin{equation}
\P[E|F]=\frac{\P[E\cap F]}{\P[F]},
\end{equation}
which is not defined as $\P[F]=0$.

Thus classical Bayesian interpretation of conditional probability will not give an answer. Furthermore, a single great circle does not yield enough information to compute any conditional probability. In terms of definition~\ref{def:conexp} the sub-$\sigma$-algebra considered here is $\G=\{\emptyset,S,B,S\setminus B\}$, which turns out to be too small.

\subsection{Conditional probability on longitudes}\label{sec:BorelLong}
Since four-element sub-$\sigma$-algebras are too small, we need to condition on larger sub-$\sigma$-algebras. One option is the $\sigma$-algebra of longitudes. Let
\begin{equation}
\mathfrak{C}=\sigma\left(\left\{[0,2\pi)\times A\mid A\in\B([0,\pi])\right\}\right)
\end{equation}
be the $\sigma$-algebra of all measurable subsets of longitudes. We can then calculate the conditional expectation of $\E[X|\mathfrak{C}]$.

\begin{proposition}
Let $X$ be $\B$-measurable. The conditional expectation of $\E[X|\mathfrak{C}]$ is given by
\begin{equation}
\E[X|\mathfrak{C}](\phi,\psi)=\frac{1}{2\pi}\int_0^{2\pi}X(\phi',\psi)d\phi'
\end{equation}
with $(\phi,\psi)\in S$.
\end{proposition}
\begin{proof}
The proof is taken from \cite{Gyenis17}. Take $A\in\B([0,2\pi))$ and consider $C=[0,2\pi)\times A\in\mathfrak{C}$. Since $\P$ is the uniform measure on the surface of the unit sphere, we have 
\begin{equation}
\int_C Xd\P=\frac{1}{4\pi}\int_A\int_0^{2\pi}X(\phi,\psi)\sin\psi d\phi d\psi
\end{equation}
by the standard spherical to Euclidean coordinate transformation. We can now apply the same coordinate transformation on the integral of $\E[X|\mathfrak{C}]$:
\begin{equation}
\int_C\E[X|\mathfrak{C}]d\P=\frac{1}{4\pi}\int_A\int_0^{2\pi}\E[X|\mathfrak{C}](\phi,\psi)\sin\psi d\phi d\psi.
\end{equation}
Filling in $\E[X|\mathfrak{C}]$ and rewriting yields
\begin{align}
\int_C\E[X|\mathfrak{C}]d\P&=\frac{1}{4\pi}\int_A\int_0^{2\pi}\E[X|\mathfrak{C}](\phi,\psi)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_A\int_0^{2\pi}\frac{1}{2\pi}\left(\int_0^{2\pi}X(\phi',\psi)d\phi'\right)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_A\left(\int_0^{2\pi}\frac{1}{2\pi}d\phi\right)\int_0^{2\pi} X(\phi',\psi)\sin\psi d\phi' d\psi\\
&=\frac{1}{4\pi}\int_A\int_0^{2\pi}X(\phi,\psi)\sin\psi d\phi d\psi=\int_C Xd\P.
\end{align}
Note that $\mathfrak{C}$ is generated by sets like $C$, thus $\E[X|\mathfrak{C}]$ is a well-defined version of the $\mathfrak{C}$-conditional expectation of $X$.
\end{proof}

This derivation is slightly different from the one in \cite{Gyenis17}. There they claim that the volume of the unit sphere is $2\pi$, whereas they should have claimed that the surface area of the unit sphere is $4\pi$. Luckily this constant is not actually used in the computations, thus their mistake has no impact on the rest of their article.

As we now have a $\mathfrak{C}$-conditional expectation, we can look at the measure space $(S,\mathfrak{C})$. Let $\psi'\in(0,\pi)$ be arbitrary, then from $\mathfrak{C}$ we can take a longitude $C=[0,2\pi)\times\{\psi'\}$ and a measurable arc $A=[\phi_1,\phi_2]\times\{\psi'\}\subset C$. Let $\P'$ be the probability measure taking $1$ on $C$ and $0$ on $C^c$, then the conditional probability $\bar{\P}$ of points being on $A$ given there is a point on $C$ is
\begin{align}
\bar{\P}[A]&=\int_S\P[A|\mathfrak{C}]d\P'=\frac{1}{2\pi}\int_S\int_0^{2\pi} \1_A(\phi',\psi)d\phi' d\P'(\phi,\psi)\label{eq:BorelLongConProb}\\
&=\frac{1}{2\pi}\left(\int_{S\setminus C}\int_{\phi_1}^{\phi_2}0d\phi' d\P'+\int_{C}\int_{\phi_1}^{\phi_2}1d\phi' d\P' \right)\\
&=\frac{\phi_2-\phi_1}{2\pi}.
\end{align}
Thus the conditional probability distribution on $C$ is uniform, resulting in all longitudes having uniform conditional probability measure.

\subsection{Conditional probability on meridians}\label{sec:BorelMer}
A great circle can not only be described as a longitude, but also as a meridian. Therefore it is interesting whether describing great circles as meridians yield to the same conditional distribution. Let 
\begin{equation}
\mathfrak{M}=\sigma(\{A\times[0,\pi]\mid A\in\B([0,2\pi))\})
\end{equation}
be the $\sigma$-algebra of measurable subsets of meridians. We can now calculate the conditional expectation $\E[X|\mathfrak{M}]$.
\begin{proposition}
Let $X$ be $\B$-measurable. The conditional expectation of $\E[X|\mathfrak{M}]$ is given by
\begin{equation}
\E[X|\mathfrak{M}](\phi,\psi)=\frac{1}{2}\int_0^\pi X(\phi,\psi')\sin\psi'd\psi'
\end{equation}
with $(\phi,\psi)\in S$.
\end{proposition}
\begin{proof}
The proof is largely taken from \cite{Gyenis17}. Let $A\in\B([0,2\pi))$ be measurable and consider $M=A\times[0,\pi]\in\mathfrak{M}$. Coordinate transformation between polar coordinates and the uniform measure on the circle $\P$ and further rewrites yield
\begin{align}
\int_M \E[X|\mathfrak{M}]d\P&=\frac{1}{4\pi}\int_0^\pi\int_A\E[X|\mathfrak{M}](\phi,\psi)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_0^\pi\int_A\left(\frac{1}{2}\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'\right)\sin\psi d\phi d\psi\\
&=\frac{1}{8\pi}\left(\int_0^\pi\sin\psi d\psi\right)\int_A\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'd\phi\\
&=\frac{1}{4\pi}\int_A\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'd\phi\\
&=\int_M Xd\P.
\end{align}

Note that $\mathfrak{M}$ is generated by sets like $M$, thus $\E[X|\mathfrak{M}]$ is a well-defined version of the $\mathfrak{M}$-conditional expectation of $X$.
\end{proof}

Note that this representation is different from equations (81) and (112) of \cite{Gyenis17}, where equation 81 is
\begin{equation}
\E[X|\mathfrak{M}](\phi,\psi)=\frac{1}{2}\int_0^{2\pi}X(\phi,\psi')|\sin\psi'|d\psi'
\end{equation}
and equation 112 is
\begin{equation}
\E[X|\mathfrak{M}](\phi,\psi)=\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi',
\end{equation}
as those versions are wrong for the following reasons:
\begin{enumerate}
\item In the verification \cite{Gyenis17} claims that $\int_0^\pi\sin\theta d\theta=1$ holds between equations (107) and (108) and between equations (115) and (116), while that integral actually has value $2$.
\item The conditional distribution of \cite{Gyenis17} is verified on a half meridian arc on page 2614, while this distribution must be verified on a full circle in order to be compared with the longitudes. Verification on a full meridian, e.g.~computation of $q(C)$ in formula (85), quickly reveals that the conditional distribution of \cite{Gyenis17} integrates to $2$.
\item The random variable $X$ is integrated on the domain $[0,2\pi)$, while $X$ is only defined on $[0,2\pi)\times[0,\pi]\in\mathfrak{M}$. The set $A\times[0,2\pi)$ is not an element of $\mathfrak{M}$.
\item Between equations (105) and (106) of \cite{Gyenis17}, they implicitly claim that the identity $X(\phi,\theta)=X(\phi,\theta+\pi)$ holds for all $\F$-measurable $X$, where now notation of \cite{Gyenis17} is used. This is most certainly false without any more restrictions on $X$.
\item Before equation (85) a measure $q_\mathcal{M}$ is defined on a whole meridian. Since the integral is taken from $\psi=0$ to $\psi=2\pi$, the integral of $q_\mathcal{M}$ over $S$ becomes $1$. However, as pointed out earlier, $\psi>\pi$ is not in our domain. Thus the integral must be split up in two arcs with $q_{\mathcal{M}}$ taking value $\frac{1}{2}$ on each arc.
\item The normalization constant is used in equation (103) of \cite{Gyenis17} is $2\pi$, where it must be $4\pi$. This does however not impact further calculations, like the same mistake made from equation (109) to (110) in the section of longitudes.
\end{enumerate}

We can verify that our version is the correct one. Take first the meridian $M=\{\phi',\phi'+\pi\}\times[0,\pi]\in\mathfrak{M}$ with $\phi'\in[0,\pi)$. Let $\psi_1^*,\psi_2^*\in\R$ be arbitrary with $\psi_2^*-2\pi\leq\psi_1^*\leq\psi_2^*$, define the angles $\psi_1=\psi_1^*\mod2\pi$ and $\psi_2=\psi_2^*\mod2\pi$ and define arc $A\subseteq M$ as
\begin{equation}
A=\begin{cases}
\{\phi'\}\times[\psi_1,\psi_2],&\psi_1,\psi_2\leq\pi,\\
\{\phi'\}\times[\psi_1,\pi]\cup\{\phi'+\pi\}\times[\psi_2-\pi,\pi],&\psi_1\leq \pi, \psi_2>\pi,\\
\{\phi'+\pi\}\times[\psi_1-\pi,\psi_2-\pi],&\psi_1,\psi_2>\pi,\\
\{\phi'\}\times[0,\psi_1]\cup\{\phi'+\pi\}\times[0,\psi_2-\pi],&\psi_2\leq\pi,\psi_1>\pi.
\end{cases}
\end{equation}
This definition is exhaustive, yet it provides all possible arcs on a meridian while restricting ourselves to the domain $[0,2\pi)\times[0,\pi]$. Now, analogous to the longitudes, let $\P'$ be the uniform measure taking $1$ on meridian $M$ and $0$ on $M^c$. The conditional probability of $\hat{\P}$ of points being on $A$ with $\psi_1,\psi_2\leq\pi$ given there is a point on $M$ is given by
\begin{align}
\hat{\P}[A]&=\int_S\P[A|\mathfrak{M}]d\P'=\frac{1}{2}\int_S\int_0^\pi\1_A(\phi,\psi')\sin\psi'd\psi'd\P'(\phi,\psi)\\
&=\frac{1}{2}\left(\int_{S\setminus M}\int_{\phi_1}^{\phi_2}0d\psi'd\P'+\int_{M}\1_{\{\phi'\}\times[0,\pi]}\int_{\phi_1}^{\phi_2}\sin\psi'd\psi'd\P'\right)\\
&=\frac{1}{4}\int_{\phi_1}^{\phi_2}\sin\psi'd\psi'd\P'=\frac{\cos\psi_1-\cos\psi_2}{4}
\end{align}
since $\int_{\{\phi'\}\times[0,\pi]}d\P'=\frac{1}{2}$. On the other possible arcs of $M$ the probability $\hat{\P}[A]$ with $\psi_1,\psi_2$ as in the definition of $A$ becomes
\begin{equation}\label{eq:BorelMerConProb}
\hat{\P}[A]=\begin{cases}
\frac{1}{4}\left(\cos\psi_1-\cos\psi_2\right),&\psi_1,\psi_2\leq\pi,\\
\frac{1}{4}\left(2+\cos\psi_1-\cos\psi_2\right),&\psi_1\leq \pi, \psi_2>\pi,\\
\frac{1}{4}\left(\cos\psi_2-\cos\psi_1\right),&\psi_1,\psi_2>\pi,\\
\frac{1}{4}\left(2-\cos\psi_1+\cos\psi_2\right),&\psi_2\leq\pi,\psi_1>\pi.
\end{cases}
\end{equation}
Now one can immediately check that $\hat{\P}$ is well-defined on $M$ as
\begin{equation}
\hat{\P}[M]=\int_S\P[A|\mathfrak{M}]d\P'=\frac{1}{2}\int_M\int_0^\pi\sin\psi'd\psi'd\P'=-\frac{1}{2}\left(\cos\pi-\cos0\right)=1.
\end{equation}

Clearly this conditional distribution on meridians is not uniform. However, one should expect they are, as a meridian is just a rotation of a longitude and the points on the sphere are spread uniformly. An explanation of this difference is given in section~\ref{sec:BorelExplained}.

\subsection{Combining longitudes and meridians}
Another question one could ask is the following: if I define $\Sigma=\sigma(\mathfrak{C},\mathfrak{M})$ as the smallest $\sigma$-algebra containing both measurable subsets of meridians and longitudes, what is then the conditional probability distribution on a great circle given $\Sigma$? The answer is that in this approach the distributions in sections~\ref{sec:BorelLong} and~\ref{sec:BorelMer} can be recovered as limiting distributions of a sequence of Bayesian conditional probabilities defined in section~\ref{sec:BorelNaive}.

First we'll further analyse the new $\sigma$-algebra $\Sigma$. Consider an arbitrary rectangle $(a,b)\times(c,d)\subset[0,2\pi)\times[0,\pi]$. Since by definition $(a,b)\times[0,\pi]\in\mathfrak{M}$ and $[0,2\pi)\times(c,d)\in\mathfrak{C}$, we have
\[(a,b)\times(c,d)=\left((a,b)\times[0,\pi]\right)\cap\left([0,2\pi)\times(c,d)\right)\in\Sigma.\]
Thus all Borel-measurable sets on our sphere are contained in $\Sigma$. Furthermore, as $\mathfrak{C}$ and $\mathfrak{M}$ contain only Borel-measurable sets, $\Sigma=\sigma(\mathfrak{C},\mathfrak{M})$ can only have Borel-measurable sets. Therefore $\Sigma=\B$ is the $\sigma$-algebra of Borel-measurable sets on the sphere.

Now take the following approach. Pick $x\in(0,\pi)$. Define the two rectangles $C_\epsilon=[0,2\pi)\times[x-\epsilon,x+\epsilon]$ and $R_\epsilon=[a,b]\times[x-\epsilon,x+\epsilon]\subseteq C_\epsilon$ for all ${\epsilon\in(0,\min\{\pi-x,x\})}$, then what is the limiting conditional probability of $R_0$ given $C_0$? The conditional probability of $R_0$ given $C_0$ can be modeled as the limit of the sequence $\P[R_\epsilon|C_\epsilon]$ with $\epsilon\to0$, which will take on the uniform distribution on $C_0$ as calculated in equation~\ref{eq:BorelLongConProb}.
\begin{proposition}\label{prop:BorelLongBayes}
Define the two rectangles $C_\epsilon=[0,2\pi)\times[x-\epsilon,x+\epsilon]$ and $R_\epsilon=[a,b]\times[x-\epsilon,x+\epsilon]\subset C_\epsilon$ for all $\epsilon\in(0,\min\{\pi-x,x\})$ with $x\in(0,\pi)$. The limiting conditional probability of $R_0$ given $C_0$ equals
\begin{equation}
\P[R_0|C_0]=\lim_{\epsilon\downarrow0}\P[R_\epsilon|C_\epsilon]=\frac{b-a}{2\pi}.
\end{equation}
\end{proposition}
\begin{proof}
Let $\epsilon\in(0,\min\{\pi-x,x\})$. Let $C_\epsilon=[0,2\pi)\times[x-\epsilon,x+\epsilon]\in\mathfrak{C}$. The probability $\P[R_\epsilon|C_\epsilon]$ equals
\begin{equation}
\P[R_\epsilon|C_\epsilon]=\frac{\P[R_\epsilon\cap C_\epsilon]}{\P[C_\epsilon]}=\frac{\int_{a}^{b}\int_{x-\epsilon}^{x+\epsilon}\sin\psi d\psi d\phi}{\int_{0}^{2\pi}\int_{x-\epsilon}^{x+\epsilon}\sin\psi d\psi d\phi}=\frac{b-a}{2\pi}.
\end{equation}
Thus $\P[R_\epsilon|C_\epsilon]=\frac{b-a}{2\pi}$ holds for all possible $\epsilon$, resulting in the limiting value
\begin{equation}
\P[R_0|C_0]=\lim_{\epsilon\downarrow0}\P[R_\epsilon|C_\epsilon]=\frac{b-a}{2\pi}.
\end{equation}
\end{proof}

A same proposition can be found for the meridians.

\begin{proposition}\label{prop:BorelMerBayes}
Let $x\in(0,2\pi)$ and $\epsilon\in(0,\min\{2\pi-x,x\})$. Consider $M_\epsilon=[x-\epsilon,x+\epsilon]\times[0,\pi]$ and $R_\epsilon=[x-\epsilon,x+\epsilon]\times[a,b]\subseteq M_\epsilon$, then the limiting conditional probability of $R_0$ given $M_0$ is
\begin{equation}
\P[R_0|M_0]=\lim_{\epsilon\downarrow0}\P[R_\epsilon|M_\epsilon]=\frac{1}{2}(\cos a-\cos b)
\end{equation}
\end{proposition}
\begin{proof}
The conditional probability $\P[R_\epsilon|M_\epsilon]$ equals
\begin{equation}
\P[R_\epsilon|M_\epsilon]=\frac{\P[R_\epsilon\cap M_\epsilon]}{\P[M_\epsilon]}=\frac{\int_{x-\epsilon}^{x+\epsilon}\int_a^b\sin\psi d\psi d\phi}{\int_{x-\epsilon}^{x+\epsilon}\int_0^{\pi}\sin\psi d\psi d\phi}=\frac{1}{2}\left(\cos a-\cos b\right).
\end{equation}
The limit of $\epsilon\downarrow0$ remains $\P[R_0|M_0]=\frac{1}{2}\left(\cos a-\cos b\right)$.
\end{proof}

Note that the probability in proposition~\ref{prop:BorelMerBayes} has $\frac{1}{2}$ as normalization factor instead of $\frac{1}{4}$, as $M_\epsilon$ converges to only a half meridian. By symmetry the distribution on the whole meridian follows the absolute cosine function.

The conditional probabilities on $\mathfrak{C}$ and $\mathfrak{M}$ therefore can be approached by Bayesian conditional probabilities on $\B$. This gives rise to the following conjecture.
\begin{conjecture}\label{con:BorelConjecture}
Let $(X,\F,\P)$ be a probability space. Let $F\in\mathcal{F}$ have zero measure and let $E\subseteq F$ be measurable. Let $\G\subset\F$ be a sub-$\sigma$-algebra containing $E$ and $F$ with $\G\neq\F$. Let $\{F_n\}_{n\in\N}\subset\G$ be a sequence converging to $F$ and $\{E_n\}_{n\in\N}\subset\G$ be a sequence converging to $E$ with for all $n\in\N$ the sets $E_n$ and $F_n$ have positive measure, $E_n\subseteq F_n$ holds and the limit $\lim_{n\to\infty}\P[E_n|F_n]$ exists. Then
\begin{equation}
\P[E|F]:=\lim_{n\to\infty}\P[E_n|F_n]=\int_X\E[\1_F|\mathcal{G}]d\P'(E)
\end{equation}
where $\P'$ is the probability measure taking $1$ on $F$ and $\E[\cdot|\G]$ is a version of the conditional expectation on $\G$.
\end{conjecture}

If conjecture~\ref{con:BorelConjecture} is true, then all conditional probabilities on null sets can be computed by limiting Bayesian conditional probabilities. It seems to hold as for the Borel-Kolmogorov paradox when considering longitudes we can take $\F=\B$ and $\G=\mathfrak{C}$ and when considering meridians we can take $\F=\B$ and $\G=\mathfrak{M}$. It however is false as the following proposition will point out.

\begin{proposition}\label{prop:BorelConFalse}
Conjecture~\ref{con:BorelConjecture} is false.
\end{proposition}
\begin{proof}
In sections \ref{sec:BorelLong} and \ref{sec:BorelMer} we have seen that the parametrization using longitudes gave the uniform distribution and the parametrization using meridians gave a cosine. However, in section~\ref{sec:BorelLong} the set $\{0,2\pi\}\times\left\{\frac{\pi}{2}\right\}$ is considered, where section~\ref{sec:BorelMer} treats the set $\{0,\pi\}\times[0,\pi]$. Those sets differer, however we can find a set which has two different probabilities using a rotation.

Call $f$ the transformation from polar to Euclidean coordinates from equation~\ref{eq:BorelPolar}. Let $O$ be a rotation such that
\begin{equation}
(f^{-1}\circ O\circ f)\left([0,\pi]\times\left\{\frac{\pi}{2}\right\}\right)=\{\pi\}\times[0,\pi],
\end{equation}
thus where the largest half longitude is rotated to a half meridian. Recall that $\mathfrak{C}$ is the $\sigma$-algebra of longitudes. We now rotate those longitudes to get the following $\sigma$-algebra:
\begin{equation}
\mathfrak{C}'=\left\{F'\in\B\mid F'=\left(f^{-1}\circ O^{-1}\circ f\right)(F),F\in\mathfrak{C}\right\}.
\end{equation}
Now $F=\{\pi\}\times[0,\pi]$ is an element of both $\mathfrak{C}'$ and $\mathfrak{M}$. We will prove that different methods of converging to $F$ yield to different $\P[E|F]$ for a measurable $E$. We will take the set $E=\{\pi\}\times\left[\frac{1}{4}\pi,\frac{3}{4}\pi\right]$ as our example.

First take a look at $\mathfrak{C}'$. Let $\epsilon\in\left(0,\frac{\pi}{2}\right)$ be arbitrary and consider the sets $C_\epsilon=[0,\pi]\times\left[\frac{\pi}{2}-\epsilon,\frac{\pi}{2}+\epsilon\right]$ and $R_\epsilon=\left[\frac{1}{4}\pi,\frac{3}{4}\pi\right]\times\left[\frac{\pi}{2}-\epsilon,\frac{\pi}{2}+\epsilon\right]$. Let $\{R'_{n^{-1}}\}_{n\in\N}$ be with $R'_{n^{-1}}=(f^{-1}\circ O^{-1}\circ f)(R_{n^{-1}})$ and let the sequence $\{C'_{n^{-1}}\}_{n\in\N}$ be with $C'_{n^{-1}}=(f^{-1}\circ O^{-1}\circ f)(C_{n^{-1}})$. Since $R_{n^{-1}},C_{n^{-1}}\in\mathfrak{C}$ holds for all $n\in\N$, we get $\{R'_{n^{-1}}\}_{n\in\N},\{C'_{n^{-1}}\}_{n\in\N}\subset\mathfrak{C'}$. Furthermore, $f^{-1}\circ O^{-1}\circ f$ has determinant $1$ since $O$ is a rotation and $f$ is almost surely a bijection on $F$, resulting to
\begin{align}
\P[R'_\epsilon|C'_\epsilon]&=\frac{\P[R'_\epsilon]}{\P[C'_\epsilon]}=\frac{\iint_{R'_\epsilon}\sin\psi d\psi d\phi}{\iint_{C'_\epsilon} \sin\psi d\psi d\phi}=\frac{\iint_{R_\epsilon}g(\sin\psi)\cdot1 d\psi d\phi}{\iint_{C_\epsilon}g(\sin\psi)\cdot1d\psi d\phi}\\
&=\frac{\int_{\frac{1}{4}\pi}^{\frac{3}{4}\pi}\int_{\frac{\pi}{2}-\epsilon}^{\frac{\pi}{2}+\epsilon}g(\sin\psi)\cdot1 d\psi d\phi}{\int_{0}^{\pi}\int_{\frac{\pi}{2}-\epsilon}^{\frac{\pi}{2}+\epsilon}g(\sin\psi)\cdot1d\psi d\phi}=\frac{\frac{3}{4}\pi-\frac{1}{4}\pi}{\pi}=\frac{1}{2}
\end{align}
where $g(x)$ is the result of the transformation $f^{-1}\circ O^{-1}\circ f$ on the integrand. The function $g$ is not written out explicitly, as the integral can be split up as a multiplication of integrals and the resulting integrals in the numerator en denominator with $g$ in the integrand are equal, thus cancel out. The result is $\P[R'_0|C'_0]=\frac{1}{2}$, as $\P[R'_\epsilon|C'_\epsilon]$ is constant in $\epsilon$. The sequence $\{R'_{n^{-1}}\}_{n\in\N}$ converges to $E$ and the sequence $\{C'_{n^{-1}}\}_{n\in\N}$ converges to $F$, thus we have $\P[E|F]=\frac{1}{2}$ as well.

For $\mathfrak{M}$ we will use proposition~\ref{prop:BorelMerBayes}. Let $x=\pi$ and $\epsilon\in(0,\pi)$, then we have $M_\epsilon\in\mathfrak{M}$ and $R_\epsilon=[\pi-\epsilon,\pi+\epsilon]\times\left[\frac{1}{4}\pi,\frac{3}{4}\pi\right]\subset M_\epsilon$. Furthermore, the sequence $\{M_{n^{-1}}\}_{n\in\N}$ converges to $F$ and $\{R_{n^{-1}}\}_{n\in\N}$ converges to $E$, thus proposition~\ref{prop:BorelMerBayes} states that
\begin{equation}
\P[E|F]=\frac{1}{2}\left(\cos\left(\frac{1}{4}\pi\right)-\cos\left(\frac{3}{4}\pi\right)\right)=\frac{\sqrt{2}}{2}.
\end{equation}

We now have both $\P[E|F]=\frac{1}{2}$ and $\P[E|F]=\frac{1}{2}\sqrt{2}$, which are clearly not equal. Conjecture~\ref{con:BorelConjecture} is therefore false.
\end{proof}

This demonstrates that when having an $F\in\F$ with zero measure, one cannot uniquely define $\P[E|F]$. A conditional expectation needs to be accompanied by a sub-$\sigma$-algebra, otherwise contradicting results can arise.

Conjecture~\ref{con:BorelConjecture} can be used to guess a conditional expectation. The measure-theoretic definition of conditional expectation is only a list of properties, not a constructive definition. In the case of a $\mathfrak{C}$-conditional expectation, looking at proposition~\ref{prop:BorelLongBayes} and its proof we observe that the resulting probability is $\frac{b-a}{2\pi}$, thus uniform. Furthermore, $\phi$ is the only variable that is integrated, thus one can guess $\phi$ is the only needed variable. Therefore one can suspect that
\begin{equation}
\E\left[\1_{[a,b]\times\left\{\frac{\pi}{2}\right\}}\middle|\mathfrak{C}\right](\phi,\psi)=\frac{1}{2\pi}\int_0^{2\pi}\1_{[a,b]\times\left\{\frac{\pi}{2}\right\}}(\phi',\psi)d\phi'=\frac{b-a}{2\pi}\1_{\left\{\frac{\pi}{2}\right\}}(\psi)
\end{equation}
holds and the guess can be generalized to
\begin{equation}
\E[X|\mathfrak{C}](\phi,\psi)=\frac{1}{2\pi}\int_0^{2\pi}X(\phi',\psi)d\phi'.
\end{equation}
This is in fact the same $\mathfrak{C}$-conditional expectation of $X$ by proposition~\ref{prop:BorelLongBayes}. The same steps can be taken to guess $\E[X|\mathfrak{M}]$ using proposition~\ref{prop:BorelMerBayes}. 

Thus we can conclude that it is wise to look at a converging sequence of Bayesian conditional probabilities for a potential conditional expectation when conditioning on zero sets. However, it must be noted that the resulting conditional probability is not uniquely defined, must be checked whether it complies to the actual definition of conditional probability and depends on the chosen sub-$\sigma$-algebra.

\section{The Borel-Kolmogorov paradox explained}\label{sec:BorelExplained}
Following the arguments of \cite{Gyenis17} we will argue that the difference in conditional distribution is no paradox, but a misinterpretation of conditional probability.

A first explanation is quite intuitive. All meridians intersect at the north and south pole of the sphere, whereas longitudes do not intersect. Therefore the $\sigma$-algebras $\mathfrak{M}$ and $\mathfrak{C}$ are vastly different. If points are spread uniformly on the meridians, the distribution of mass on the whole sphere will not be uniform and the density of mass will be highest at the poles. This is simulated by \cite{Weisstein} and his simulations support this statement.

Let $\bar{\P}$ be the conditional probability measure on a longitude, defined by equation~\ref{eq:BorelLongConProb} and let $\hat{\P}$ be the conditional probability measure on a meridian, defined by equation~\ref{eq:BorelMerConProb}. If the spaces $(S, \mathfrak{C}, \bar{\P})$ and $(S, \mathfrak{M}, \hat{\P})$ are isomorphic with a measurable bijection, then $\hat{\P}$ and $\bar{\P}$ must have equal distribution and the results of sections~\ref{sec:BorelLong} and \ref{sec:BorelMer} must be paradoxical and contradictory. If there is a measurable bijection from a meridian to a longitude, their conditional probability distributions must agree. This is not the case here.
\begin{theorem}[\cite{Gyenis17}]
Let $f\colon S\to S$ be a measurable bijection with measurable inverse. There is no Boolean algebra isomorphism $h_f\colon\mathfrak{C}\to\mathfrak{M}$ that is determined by $f$.
\end{theorem}
\begin{proof}
The proof can be found in \cite{Gyenis17}, but we will repeat it here. Suppose such isomorphism $h_f$ exists. All longitudes $C$ in $\mathfrak{C}$ are the only atoms of $\mathfrak{C}$, therefore $h_f(C)$ are the only atoms of $\mathfrak{M}$ as well, making $h_f(C)$ meridians. We will now prove that $h_f(C)$ cannot be a meridian.

Let $m_0=\{(0,0), (0,\pi)\}$ be the set of north and south poles. Let $c_0\in\mathfrak{C}$ be such that $h_f(c_0)=m_0$. Then $c_0$ consists of two elements as well, thus we can choose longitude $C$ not on the north or south poles such that $C\cap c_0=\emptyset$. Note that $h_f(C)$ is a meridian, thus $m_0\subset h_f(C)$ as all meridians pass through the north and south poles. Since $h_f$ is a Boolean algebra isomorphism, we have $h_f(C\cap c_0)=h_f(C)\cap h_f(c_0)$. This all can be combined in the following line:
\begin{equation}
\emptyset=h_f(\emptyset)=h_f(C\cap c_0)=h_f(C)\cap h_f(c_0)=h_f(C)\cap m_0=m_0.
\end{equation}
This is clearly a contradiction.

Let $C$ be the north or south pole of the sphere. Since $h_f$ is a bijection, $h_f(C)$ is only a single point as well. Thus $h_f(C)$ cannot be an atom of $\mathfrak{M}$. Therefore there is a contradiction here as well.

Thus no single atom of $\mathfrak{C}$ result into an atom of $\mathfrak{M}$ by $h_f$ and therefore function $h_f$ cannot exist.
\end{proof}

Therefore every measurable bijection between $(S,\mathfrak{C})$ and $(S,\mathfrak{M})$ has no Boolean algebra isomorphism $h_f\colon\mathfrak{C}\to\mathfrak{M}$ such that longitudes in $\mathfrak{C}$ are mapped to meridians on $\mathfrak{M}$. This theorem holds on all subalgebras of $\mathfrak{C}$ and $\mathfrak{M}$ as well, as is proven by \cite{Gyenis17}.

Now it should be clear why the conditional distribution on the longitudes and meridians of the sphere differ, since we are dealing with two entirely different structured spaces. Therefore, the question `why is the conditional distribution on the longitudes different from the conditional distribution on the meridians' is easy to answer; difference in spaces. To verify that two different methods of conditioning on the same set yield to the correct conditional probability, one must ask the following question: `if one models a distribution on the sample space using conditional expectation on a sub-$\sigma$-algebra, does the resulting conditional probability distribution extend to the original distribution on the whole sample space?' As earlier mentioned, Weisstein \cite{Weisstein} demonstrated that the answer to that question is yes. The uniform distribution on longitudes and the cosine on meridians both extend to the uniform distribution on the whole sphere. Thus when dealing with different conditional distributions on measure zero sets, one should rather ask a question like the last one, as the answer to that question must always be yes. Further reading and a more in-depth analysis can be found in \cite{Gyenis17}.


\section{Conclusion}
After analysing the Borel-Kolmogorov paradox, the first and most important conclusion we can take is that when conditioning, especially on a set $F$ of measure zero, one needs to give the accompanying sub-$\sigma$-algebra of the event that is conditioned on. Otherwise the Borel-Kolmogorov phenomenon appears, where one can give different sub-$\sigma$-algebras containing $F$ such that the resulting conditional probability distribution is different.

Secondly, we must accept that conditional probability on sets of measure zero are not uniquely defined. That uniqueness only appears when conditioning on sets of positive measure, as then the classical rule $\P[E|F]=\frac{\P[E\cap F]}{\P[F]}$ gives a version of the conditional probability. If a sub-$\sigma$-algebra can be used to model the entire sample space, for example the sub-$\sigma$-algebra of longitudes with the sphere as sample space, then the conditional probability must be able to extend to the original probability distribution. For example providing the uniform distribution on all longitudes gives back the uniform distribution on the sphere, as well as applying the cosine distribution on the meridians results into the uniform distribution on the sphere.

Thirdly, a conditional distribution on a zero set must be calculated using the measure-theoretic conditional expectation. Conjecture~\ref{con:BorelConjecture} can help with finding the correct conditional expectation, however proposition~\ref{prop:BorelConFalse} proves that the resulting conditional probability does not need to be unique. Therefore conjecture~\ref{con:BorelConjecture} cannot be used to calculate a conditional probability and then call it a day, you need to check whether the distribution you found actually satisfies all conditions of conditional probability.

At last, as Kolmogorov \cite{Kolmogorov33} already stated in his work, there is nothing paradoxical going on in the Borel-Kolmogorov paradox. The space of the sphere with the longitudes as $\sigma$-algebra is not homeomorphic with the space of the sphere and the meridians as $\sigma$-algebra. The not-uniqueness of the conditional probability on a zero set is thus as expected and we must give the sub-$\sigma$-algebra when conditioning on sets of measure zero.


\chapter{Safe probability}\label{chap:SafeProp}

\chapter{Discrete paradoxes}\label{chap:DiscPara}

\chapter{The two envelopes problem}\label{chap:TwoEnvelope}

\chapter{The Sleeping Beauty problem}\label{chap:SleepingBeauty}

\bibliographystyle{alpha}
\bibliography{../Referenties/Referenties}

\appendix

\newpage

\printindex

\end{document}