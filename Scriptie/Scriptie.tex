%!TEX program = xelatex
\documentclass[a4paper]{report}
\usepackage[ngerman, english]{babel}
\usepackage{csquotes}
\usepackage{amsmath,amsfonts,amssymb,amsthm,bbm,mathrsfs}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{showidx}
\usepackage{pgf,tikz,pgfplots}
\usetikzlibrary{arrows, positioning}
\pgfplotsset{compat=1.16}
\makeindex

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem*{theoremmain*}{Theorem \ref{thm:DiscMainThm}}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{property}[theorem]{Property}
\newtheorem{properties}[theorem]{Properties}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}

\numberwithin{equation}{chapter}

\hypersetup{%
  colorlinks = true
}

%Afkortingen voor wiskundige symbolen
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\C}{\mathbb{C}}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\1}{\mathbbm{1}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\G}{\mathcal{G}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\acc}{acc}

\newcommand{\Pmod}{\mathcal{P}^*}
\newcommand{\Psafe}{\tilde{\P}}
\newcommand{\Uonesafe}{\tilde{\1}_U}
\newcommand{\Usafe}{\tilde{U}}

\newcommand{\EnvIndSafe}{\1_{\{B=2A\}}}
\newcommand{\MontyInd}{\1_{\{U=a\}}}
\newcommand{\DieInd}{\1_{\{U=3\}}}
\newcommand{\DieIndSafe}{\tilde{\1}_{\{Y=3\}}}
\newcommand{\ChildInd}{\1_{\{U=bg\}}}
\newcommand{\ChildIndSafe}{\tilde{\1}_{\{U=bg\}}}
\newcommand{\ChildTwoInd}{\1_{\{U=bb\}}}
\newcommand{\ChildTwoIndSafe}{\tilde{\1}_{\{U=bb\}}}
\newcommand{\GeneralInd}{\1_{\{U=u\}}}
\newcommand{\GeneralGenInd}{\1_{\{U\in\X'\}}}
\newcommand{\GeneralGenIndSafe}{\tilde{\1}_{\{U\in\X'\}}}

\title{Thesis}
\author{Mathijs Kolkhuis Tanke}
\date{\today}

\begin{document}
\begin{titlepage}
\pagenumbering{gobble}

\maketitle
\end{titlepage}

\pagenumbering{roman}

\begin{abstract}
This is my thesis.
\end{abstract}


\pagenumbering{gobble}
\tableofcontents

\chapter{Introduction}
\pagenumbering{arabic}
Probability theory is one of the most important and most researched fields in mathematics. It is in essence based on just three axioms first stated by Andrey Kolmogorov \cite{Kolmogorov33}, namely that the probability of an event is non-negative, the probability measure is a unit measure and the probability measure is countably additive on disjoint sets. These three axioms however do not prevent the existence of certain paradoxes, like the Borel-Kolmogorov paradox, Monty Hall's problem and the Sleeping Beauty problem. Some paradoxes arise from wrongly using probability theory, others are a misinterpretation of results.

This thesis focuses mainly paradoxes arising from conditional probability, such as the Borel-Kolmogorov paradox, Monty Hall's problem and the two envelope problem. We study will these problems and address their paradoxical nature. Ultimately it will be shown that all paradoxes arise by incorrectly applying conditional probability.

Take a look at the Borel-Kolmogorov paradox first. A sphere is equipped with the uniform probability measure. Suppose a point exists on a great circle of the sphere, but you do not know where that point is. Is there a probability distribution on this great circle for that point? If one models this problem using latitudes, the conditional distribution on the great circle is uniform. However, if one models this problem using meridians, the conditional distribution is a cosine.

Borel \cite{Borel09} and Kolmogorov \cite{Kolmogorov33} both addressed this problem and Kolmogorov gave the following answer:
\foreignblockquote{ngerman}[Andrey Kolmogoroff, \cite{Kolmogorov33}]{Dieser Umstand zeigt, daß der Begriff der bedingten Wahrscheinlichkeit in bezug auf eine isoliert gegebene Hypothese, deren Wahrscheinlichkeit gleich Null ist, unzulässig ist: nur dann erhält man auf einem Meridiankreis eine Wahrscheinlichkeitsverteilung für [Breite] Theta, wenn dieser Meridiankreis als Element der Zerlegung der ganzen Kugelfläche in Meridiankreise mit den gegebenen Polen betrachtet wird.}
In summary, Kolmogorov states that the concept of conditional probability on a great circle is inadmissible, since the event that the point lies on a great circle of the sphere has zero measure. Furthermore, the sole reason of a cosine distribution on a great circle arising when considering meridians is that the meridian circle serves as an element of the decomposition of the whole spherical surface in meridian circles with the poles considered.

Despite Kolmogorov's explanation this problem is still upon debate. Recently Gyenis, Hofer-Szabó and Rédei \cite{Gyenis17} studied this problem and provided more insight and an in my opinion satisfying discussion to the problem, eventually drawing the same conclusion as Kolmogorov already did. This problem is more elaborately discussed in Chapter~\ref{chap:BorelKolmogorov} where we will expand the analysis of \cite{Gyenis17} and not only consider latitudes and meridians, but their combination as well. Furthermore, we will look whether a conditional probability on a zero set can be uniquely approached by traditional conditional probability on sets of decreasing measure as stated in Conjecture~\ref{con:BorelConjecture}. This conjecture turns out to be false, again affirming that conditional probabilities on sets of measure zero are not uniquely defined.

Another paradox arising from conditional probability is Monty Hall's problem. In Monty Hall's problem, a player is facing three doors called $a$, $b$ and $c$. One door has a car behind and the other two have goats. Suppose the player initially chooses door $a$. The game master then opens either door $b$ or $c$, but always a door with a goat. The player now faces two doors and is asked whether he wants to switch. One possible solution is that the player faces two doors without any preference for either door, thus the probability is 50\%. Another possible solution is that first the player had a 33\% chance of correctly guessing the door with the car. If for example door $c$ is opened, door $b$ remains with a conditional probability of 67\% of having the car. Which solution is correct?

Let $\X=\{a,b,c\}$ be the set of doors and $U$ the random variable denoting the location of the car. Conditional probability then states
\begin{equation*}\label{eq:IntMonty}
\P[U=b\mid U\in\{a,b\}]=\frac{\P[U=b,U\in\{a,b\}]}{\P[U\in\{a,b\}]}=\frac{\frac{1}{3}}{\frac{2}{3}}=\frac{1}{2},
\end{equation*}
supporting the claim that a probability of 50\% is the correct answer. However, the space conditioned on door $c$ is opened is $\{a,b\}$ and the space conditioned on door $b$ is opened is $\{a,c\}$. If door $a$ has the car, the game master can open either door and the set of events we must condition on is $\{\{a,b\},\{a,c\}\}$. These two events do not form a partition, preventing us from using traditional conditional probability.

Thus in addition to Kolmogorov's statement, we must not only be wary for conditioning on events with measure 0, we cannot condition on arbitrary events at all. I propose that when using conditional probability, one must provide a pair of a sub-$\sigma$-algebra and the event from that $\sigma$-algebra to condition on. Regarding Monty Hall's problem there is no $\sigma$-algebra on $\{a,b,c\}$ containing the set $\{\{a,b\},\{a,c\}\}$. In the case of the Borel-Kolmogorov paradox providing sub-$\sigma$-algebras immediately make clear why the conditional distribution on a great circle is not unique, as both calculations are supported by different sub-$\sigma$-algebras.

The crux of the Monty Hall problem is that the initial distribution of the car is unknown and the player does not know with which probability door $b$ is opened given the car is behind door $a$. Thus there is a set of different possible probability distributions where one is the correct distribution. Using the theory of \emph{safe probability} we can obtain a strategy that give equal results for all distributions in a specified model. This theory is introduced by Grünwald \cite{Grunwald18} and summarized here in Chapter~\ref{chap:SafeProp}. Safe probability is then applied in Chapter~\ref{chap:DiscPara} to problems as the Monty Hall problem and to the two envelope problem in Chapter~\ref{chap:TwoEnvelope}.

In Chapter~\ref{chap:DiscPara} the results of the analysis of Monty Hall's problem are generalized to a theorem, which can be used to provide safe distributions for other problems like the boy or girl problem.
\begin{theoremmain*}
Let $\X$ be countable and $\Y$ be finite. Let $U$ be an $\X$-valued random variable and $V$ be a $\Y$-valued random variable. Let $p_u\in[0,1]$ with $\sum_{u\in\X}p_u=1$. Let
\begin{equation}
\Pmod\subseteq\{\P\mid\forall u\in\X:\P[U=u]=p_u\}
\end{equation}
be our model of probability distributions on $\X\times\Y$ where $|\Y|$ distributions $\P_1,\ldots,\P_{|\Y|}\in\Pmod$ impose $|\Y|$ linearly independent vectors $(\P_i[V=v])_{v\in\Y}$ with $i\in\{1,\ldots,|\Y|\}$. Let $u\in\X$ be arbitrary and let $\Psafe$ be a distribution on $\X\times\Y$, then the following are equivalent:
\begin{enumerate}
    \item For all $v\in\Y$ we have $\Psafe[U=u|V=v]=p_{u}$.
    \item $\Psafe$ is safe for $\GeneralInd|[V]$.
    \item $\Psafe$ is safe for $\langle\GeneralInd\rangle|\langle V\rangle$.
\end{enumerate}
\end{theoremmain*}

This theorem and its notation will be explained, proven and applied in Chapter~\ref{chap:DiscPara}, but it essentially states the following: in the case of the Monty Hall problem if one assumes the car is initially distributed evenly between the doors, the probability of the car being behind the originally chosen door $a$ can be assumed to be $\frac{1}{3}$. Using this assumption one must always switch to the other door as the probability of the car being behind that door can be assumed to be $\frac{2}{3}$, resulting in a 67\% chance of winning the car. It is not at all clear whether this is the correct distribution. However, this assumption always yields to a 67\% chance of winning the car independent on the probability of opening door $b$ when the car is behind door $a$.

Another paradox we will treat is the two envelope problem. There are two envelopes, where one is filled with value $a$ and the other with value $b$. The only thing the player knows is that either $a=2b$ or $b=2a$. An envelope is given to the player, with the player receiving either envelope with equal probability. Call this envelope $A$ and suppose the player observes value $a$. Should he switch to envelope $B$ or keep the contents of $A$?

At first glance, he should. Either $b=2a$ or $b=\frac{a}{2}$ holds with equal probability, thus \[\E[B|A=a]=\frac{1}{2}\cdot2a+\frac{1}{2}\cdot\frac{a}{2}=\frac{5}{4}a.\]
However, the player has received an envelope at random and only knows the contents of that particular envelope. So he could also have been given envelope $B$ with value $b$, for which $\E[A|B=b]=\frac{5}{4}b$ holds. Therefore no matter what envelope the player receives, he should switch. This solution must clearly be wrong and it is. When conditioning on $A=a$, then either $b=2a$ or $a=2b$ holds with probability $1$ in the conditioned probability space. This renders the previous calculation of $\E[B|A=a]$ to be incorrect. However, we do not know which event takes place.

Now let $x=\min\{a,b\}$ be the lowest value, then either $a=x$ or $a=2x$ holds with equal probability. For both envelopes we now have
\[
\E[A|B=b]=\E[B|A=a]=\frac{1}{2}x+\frac{1}{2}\cdot2x=\frac{3}{2}x.
\]
Both envelopes thus have on average the same contents, as is expected when the envelopes are handed out uniformly. Unfortunately, we do not know the value of $x$. Furthermore, if $x$ is picked from an unbounded set and we assume that given an observation the other envelope must hold twice as much or half the value with equal probability, then $x$ must be taken from a uniform probability distribution on the unbounded set. Such distribution does not exist. We conclude that the value of $\E[B|A=a]$ does depend on a prior distribution on the chosen value $x$.

There are two ways to address this problem. Using safe probability, like in the case of Monty Hall, we will devise a strategy where the player wins $\frac{3}{2}\E[X]$ on average, where $X$ the actual unknown distribution for the lowest value in the envelopes on the positive number line with finite expectation. This strategy is in fact that the player must flip a fair coin and switch when it lands heads, which is quite intuitive.\\
Another method is using a switching strategy introduced by McDonell and Abbott~\cite{McDonnell09,Abbott10,McDonnell11}. Let $f\colon (0,\infty)\to[0,1]$ be a function that takes an observation $a$ and equips it with a probability. The player must switch with probability $f(a)$ when observing value $a$. When $f$ is a decreasing function and strictly decreases on an interval where distribution $X$ has positive measure, then switching using $f$ will always return on average a higher value than $\frac{3}{2}\E[X]$. If $f$ is a threshold switch, for example $f=\1_{(0,a^*]}$, then for some distributions $X$ the strategy $f$ is actually the best strategy available. However, when playing the game, the player does not know how much strategy $f$ outperforms switching using safe probability. By any means, distribution $X$ gives $f$ an arbitrarily small advantage over safe probability, making it an unnecessarily complicated method to marginally increase the average value won.

The two envelope problem will be discussed further in Chapter~\ref{chap:TwoEnvelope}.

For every problem there is a different reason for getting paradoxical results. There is one recurring theme with all paradoxes, namely when analysing these problems most of the times the underlying $\sigma$-algebra is not taken into account. This yields to various conflicting results, as conditioning on events that cannot be conditioned on to not recognizing that multiple probability distributions are possible. The theme of is thesis this therefore that when doing probability theory, the underlying probability space and $\sigma$-algebra must never be ignored and not providing a sub-$\sigma$-algebra with a conditional distribution must become a bad habit instead of an accepted practice.

\chapter{The Borel-Kolmogorov paradox}\label{chap:BorelKolmogorov}
The first paradox we will study in more detail is the Borel-Kolmogorov paradox. It was first studied by Borel \cite{Borel09} and Kolmogorov \cite{Kolmogorov33} and has sparked debate over the centuries afterwards\footnote{Bronnen lezen en invoeren.}. We will take a look at the following version of the paradox.

Suppose a random variable has a uniform probability distribution on the unit sphere. If one conditions on a great circle, what will the resulting conditional distribution be? If the great circle is viewed as a latitude, the conditional distribution will be uniform as well. However, if the great circle is modelled as a meridian, the conditional distribution has a cosine as density function. We have two different conditional distributions on the same set, thus which one is correct?

The basic solution to the paradox is to use measure-theoretic conditional probability, so that one is given not just a set like a great circle to condition on but rather a set and an accompanying sub-$\sigma$-algebra containing this set. The definition of measure-theoretic conditional probability now provides an answer that is unique and meaningful up to a set of zero measure. When conditional probability is defined traditionally based on density functions, it is defined on every point even though each such point has measure zero. One may now ask the following question: can we generally extend the measure-theoretic conditional probability to be defined on all points once a sub-$\sigma$-algebra is provided, by treating the conditional probability at that point as a limit of traditional conditional probabilities where one conditions on smaller and smaller sets? In Section~\ref{sec:BorelCombining} we provide Conjecture~\ref{con:BorelConjecture} that would imply this, but we will then show this conjecture is wrong. If we condition on both a given meridian and a suitably rotated latitude, then two phenomena occur:
\begin{enumerate}
\item the only sub-$\sigma$-algebra that we can provide is the full Borel $\sigma$-algebra and conditioning on the full $\sigma$-algebra leads to no effect and
\item the required limit is undefined as the limit of smaller and smaller meridians takes on a different value than the limit of smaller and smaller rotated latitudes.
\end{enumerate}

Kolmogorov \cite{Kolmogorov33} pointed out that the great circle has zero measure and according to him conditioning on a great circle must not be allowed. Our analysis will concern conditioning on zero sets using measure-theoretic conditional expectations as such conditioning is defined there. The article of Gyenis, Hofer-Szabó and Rédei \cite{Gyenis17} is followed to compute the conditional distributions on the great circle, viewing that circle both as latitude and as meridian. The authors draw the same conclusion as in \cite{Kolmogorov33} but with a far more elaborate explanation. We will take it one step further to also consider the $\sigma$-algebra of both the meridians and latitudes and to consider conditional distributions on rotated latitudes making them coincide with meridians. We will then conclude that once again conditioning on measure zero sets must not be admissible, as it will lead to contradicting results.

After drawing these conclusions, we will rename the Borel-Kolmogorov paradox to the \emph{Borel-Kolmogorov phenomenon}, as it is more an example on why you should not condition on zero sets than it is a paradox.

\section{Conditional expectation}
First we need to define conditional expectations. We use the standard measure-theoretic definitions as given by David Williams \cite{Williams91}.

Let $\Omega$ be an arbitrary sample space and let $\mathcal{F}$ be a $\sigma$-algebra on $\Omega$. The indicator function $\1_A$ on a set $A$ is defined as
\begin{equation}
\1_A(x)=\begin{cases}1,&x\in A,\\0,&x\not\in A.\end{cases}
\end{equation}
Let $\B(A)$ be the $\sigma$-algebra of all Borel-measurable sets on $A\subseteq\R^n$. We will start with the definition of a random variable.

\begin{definition}[Random variable]
Let $\X$ be a measurable space. A function $X\colon\Omega\to\X$ is an \emph{$\X$-valued random variable} if it is $\F$-measurable, thus if $X^{-1}(A)\in\F$ holds for all $A\in\G$ where $\G$ is a $\sigma$-algebra on $\X$.
\end{definition}

We can now define the conditional expectation as definition 9.2 of \cite{Williams91}.

\begin{definition}[Conditional expectation]\label{def:conexp}
Let $X$ be an $\F$-measurable random variable with finite $\E[|X|]$. Let $\G$ be a sub-$\sigma$-algebra of $\F$. There exists a random variable $Y$ such that
\begin{enumerate}
\item $Y$ is $\G$-measurable,
\item $\E[|Y|]$ is finite,
\item for every $G\in\G$ we have
\begin{equation}
\int_G Yd\P=\int_G Xd\P.
\end{equation}
\end{enumerate}
$Y$ is called a \emph{version of the conditional expectation} $\E[X|\G]$ of $X$ given $\G$, written as $Y=\E[X|\G]$ almost surely. Moreover, if $\tilde{Y}$ is another random variable with these properties, then $\tilde{Y}=Y$ equal almost surely. Since two versions of $\E[X|\G]$ coincide almost surely, $Y$ is also called \emph{the} conditional expectation $\E[X|\G]$.
\end{definition}

Note that when $\G=\sigma(Z)$ is the smallest $\sigma$-algebra generated by a set $Z\in\F$, we also write $\E[X|\G]=\E[X|Z]$. This generalizes to $\E[X|\G]=\E[X|Z_1,Z_2,\ldots]$ when $\G=\sigma(Z_1,Z_2,\ldots)$.

Definition~\ref{def:conexp} extends very nicely to the definition of conditional probability. Note that $\E[\1_F]=\P[F]$ holds for all $F\in\F$ and this carries over to conditional probabilities.

\begin{definition}[Conditional probability]
If $\G$ is a sub-$\sigma$-algebra of $\F$ and $F\in\F$, then the \emph{conditional probability} $\P[F|\G]$ is a version of $\E[\1_F|\G]$.
\end{definition}

\subsection{Comparison to traditional definitions}
The traditional definition of traditional density-based conditional expectation and probability is the following: when $X$ and $Y$ are two random variables on $\R$, $f_{X,Y}$ is the joint density function of $X$ and $Y$ and $f_Y$ is the density function of $X$, the conditional expectation $\E[X|Y=y]$ is defined as
\begin{equation}
\E[X|Y=y]=\int_\R x\frac{f_{X,Y}(x,y)}{f_Y(y)}dx.
\end{equation}
Definition~\ref{def:conexp} agrees on this traditional usage although it can be modified on a set of measure zero, as we will see now. Take $\G=\sigma(\{Y=y\})$. The smallest $\sigma$-algebra generated by all $\omega\in\R$ such that $Y(\omega)=y$ and define
\begin{equation}
g(y)=\int_\R x\frac{f_{X,Y}(x,y)}{f_Y(y)}dx,
\end{equation}
then $g(Y)$ is a version of $\E[X|Y]$. The proof and a more general statement can be found in section~9.6 of \cite{Williams91}.

The traditional conditional expectation can also be extended to traditional conditional probability.
\begin{definition}[Traditional conditional probability]
Let $(\Omega,\F, \P)$ be a probability space. Let $F\in\F$ be with positive measure and let $E\in\F$ be measurable. The \emph{traditional conditional probability} of $E$ given $F$ is given by
\begin{equation}
\P[E|F]=\frac{\P[E\cap F]}{\P[F]}.
\end{equation}

If $F$ has measure zero, the traditional conditional probability of $E$ given $F$ is undefined.
\end{definition}

\section{Formal description of the probability space}
\begin{figure}
\centering{
\input{figures/BorelTransform.tikz}
}
\caption{A visualization of coordinate transformation~\eqref{eq:BorelPolar} from Euclidean to spherical coordinates. The rectangle represents the set $S$. The horizontal lines in the rectangle like $C$ parametrize the latitudes. The vertical lines in the rectangle like $M$ parametrize the meridians. Note that the two blue vertical lines of $M$ are distance $\pi$ apart, this is needed to fully parametrize a meridian.}
\label{fig:BorelVis}
\end{figure}

To analyse the Borel-Kolmogorov paradox, we need to formalize our probability space. The following construction is visualized in Figure~\ref{fig:BorelVis}. Let $S$ be the unit sphere. For easier calculations, we equip $S$ with polar coordinates. Define $S=[0,2\pi)\times[0,\pi]$, then $S$ is described in the Euclidean space with the function
\begin{equation}\label{eq:BorelPolar}
S\to\R^3:(\phi,\psi)\mapsto(\cos\phi\sin\psi,\sin\phi\sin\psi,\cos\psi).
\end{equation}
Let $\B=\B(S)$ be the Borel-$\sigma$-algebra on $S$. The uniform distribution on $S$ is defined as
\begin{equation}
\P[B]=\frac{1}{4\pi}\iint_B\sin\psi d\psi d\phi
\end{equation}
for a $B\in\B$. The triple $(S,\B,\P)$ form a probability space.

The set of latitudes is described by
\begin{equation}
\mathcal{C}=\{[0,2\pi)\times\{\psi\}\mid\psi\in[0,\pi]\}
\end{equation}
and will be horizontal lines in the rectangle of Figure~\ref{fig:BorelVis}. The set of meridians is described by
\begin{equation}
\mathcal{M}=\{\{\phi,\phi+\pi\}\times[0,\pi]\mid\phi\in[0,\pi)\}
\end{equation}
and will be the vertical lines in the rectangle of Figure~\ref{fig:BorelVis}.

Note that the function in \eqref{eq:BorelPolar} is not a bijection. The image of $S$ is not one-to-one on the north and south pole, but this is a null set and will not cause any problems in our case. The reason why \eqref{eq:BorelPolar} is not made a formal bijection is that it eases notation.

\section{Conditional distributions}
We will now explore various ways to describe the conditional distribution on a great circle.
\subsection{Traditional conditional probability}\label{sec:BorelNaive}
The first method is the naive method using traditional conditional probability. The probability space here is $(S,\B,\P)$. Let $F\in\B$ be a great circle.

Note that $\P[F]=0$, as a great circle always has zero measure. To be precise, let $f\colon S\to\R^3$ be the coordinate transformation of \eqref{eq:BorelPolar}. All great circles $F'$ have a rotation $O\colon\R^3\to\R^3$ such that $(f^{-1}\circ O\circ f)(F')=[0,2\pi)\times\left\{\frac{\pi}{2}\right\}$. Rotations are orthogonal and have determinant $1$, thus
\begin{align}
\P[F]&=\frac{1}{4\pi}\iint_F\sin\psi d\psi d\phi\\
&=\frac{1}{4\pi}\int_0^{2\pi}\int_{\frac{1}{2}\pi}^{\frac{1}{2}\pi}\sin\psi \det(f^{-1}\circ O\circ f)d\psi d\phi\\
&=\frac{1}{4\pi}\int_0^{2\pi}\int_{\frac{1}{2}\pi}^{\frac{1}{2}\pi}\sin\psi d\psi d\phi=0.
\end{align}
The inverse $f^{-1}$ is well-defined here since $f$ is a bijection locally around the circle $[0,2\pi)\times\left\{\frac{\pi}{2}\right\}$. Let $E\subset F$ be a measurable subset of $F$, then the traditional conditional probability of $E$ given $F$ equals
\begin{equation}
\P[E|F]=\frac{\P[E\cap F]}{\P[F]},
\end{equation}
which is not defined as $\P[F]=0$.

Thus the traditional interpretation of conditional probability will not give an answer. Furthermore, a single great circle does not yield enough information to compute any conditional probability. In terms of Definition~\ref{def:conexp} the sub-$\sigma$-algebra considered here is $\G=\{\emptyset,S,B,S\setminus B\}$, which turns out to be too small.

\subsection{Conditional probability on latitudes}\label{sec:BorelLong}
Since four-element sub-$\sigma$-algebras are too small, we need to condition on larger sub-$\sigma$-algebras. One option is the $\sigma$-algebra of latitudes. Let
\begin{equation}
\mathfrak{C}=\sigma\left(\left\{[0,2\pi)\times A\mid A\in\B([0,\pi])\right\}\right)
\end{equation}
be the $\sigma$-algebra of all measurable subsets of latitudes. We can then calculate the conditional expectation of $\E[X|\mathfrak{C}]$.

\begin{proposition}
Let $X$ be $\B$-measurable. The conditional expectation of $\E[X|\mathfrak{C}]$ is given by
\begin{equation}
\E[X|\mathfrak{C}](\phi,\psi)=\frac{1}{2\pi}\int_0^{2\pi}X(\phi',\psi)d\phi'
\end{equation}
with $(\phi,\psi)\in S$.
\end{proposition}
\begin{proof}
The proof is taken from \cite{Gyenis17}. Take $A\in\B([0,2\pi))$ and consider $C=[0,2\pi)\times A\in\mathfrak{C}$. Since $\P$ is the uniform measure on the surface of the unit sphere, we have
\begin{equation}
\int_C Xd\P=\frac{1}{4\pi}\int_A\int_0^{2\pi}X(\phi,\psi)\sin\psi d\phi d\psi
\end{equation}
by the standard spherical to Euclidean coordinate transformation. We can now apply the same coordinate transformation on the integral of $\E[X|\mathfrak{C}]$:
\begin{equation}
\int_C\E[X|\mathfrak{C}]d\P=\frac{1}{4\pi}\int_A\int_0^{2\pi}\E[X|\mathfrak{C}](\phi,\psi)\sin\psi d\phi d\psi.
\end{equation}
Filling in $\E[X|\mathfrak{C}]$ and rewriting yields
\begin{align}
\int_C\E[X|\mathfrak{C}]d\P&=\frac{1}{4\pi}\int_A\int_0^{2\pi}\E[X|\mathfrak{C}](\phi,\psi)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_A\int_0^{2\pi}\frac{1}{2\pi}\left(\int_0^{2\pi}X(\phi',\psi)d\phi'\right)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_A\left(\int_0^{2\pi}\frac{1}{2\pi}d\phi\right)\int_0^{2\pi} X(\phi',\psi)\sin\psi d\phi' d\psi\\
&=\frac{1}{4\pi}\int_A\int_0^{2\pi}X(\phi,\psi)\sin\psi d\phi d\psi=\int_C Xd\P.
\end{align}
Note that $\mathfrak{C}$ is generated by sets like $C$, thus $\E[X|\mathfrak{C}]$ is a well-defined version of the $\mathfrak{C}$-conditional expectation of $X$.
\end{proof}

This derivation is slightly different from the one in \cite{Gyenis17}. The corrections on \cite{Gyenis17} are given in Appendix~\ref{app:CorLong}.

As we now have a $\mathfrak{C}$-conditional expectation, we can look at the measure space $(S,\mathfrak{C})$. Let $\psi'\in(0,\pi)$ be arbitrary, then from $\mathfrak{C}$ we can take a latitude $C=[0,2\pi)\times\{\psi'\}$ and a measurable arc $A=[\phi_1,\phi_2]\times\{\psi'\}\subset C$. Let $\P'$ be the probability measure taking $1$ on $C$ and $0$ on $C^c$, then the conditional probability $\bar{\P}$ of points being on $A$ given there is a point on $C$ is
\begin{align}
\bar{\P}[A]&=\int_S\P[A|\mathfrak{C}]d\P'=\frac{1}{2\pi}\int_S\int_0^{2\pi} \1_A(\phi',\psi)d\phi' d\P'(\phi,\psi)\label{eq:BorelLongConProb}\\
&=\frac{1}{2\pi}\left(\int_{S\setminus C}\int_{\phi_1}^{\phi_2}0d\phi' d\P'+\int_{C}\int_{\phi_1}^{\phi_2}1d\phi' d\P' \right)\\
&=\frac{\phi_2-\phi_1}{2\pi}.
\end{align}
Thus the conditional probability distribution on $C$ is uniform, resulting in all latitudes having uniform conditional probability measure. Since the unit sphere itself has a uniform probability measure, this result is as expected.

\subsection{Conditional probability on meridians}\label{sec:BorelMer}
A great circle cannot only be described as a latitude, but also as a meridian. Therefore it is interesting whether describing great circles as meridians yield to the same conditional distribution. Let
\begin{equation}
\mathfrak{M}=\sigma(\{A\times[0,\pi]\mid A\in\B([0,2\pi))\})
\end{equation}
be the $\sigma$-algebra of measurable subsets of meridians. We can now calculate the conditional expectation $\E[X|\mathfrak{M}]$.
\begin{proposition}
Let $X$ be $\B$-measurable. The conditional expectation of $\E[X|\mathfrak{M}]$ is given by
\begin{equation}
\E[X|\mathfrak{M}](\phi,\psi)=\frac{1}{2}\int_0^\pi X(\phi,\psi')\sin\psi'd\psi'
\end{equation}
with $(\phi,\psi)\in S$.
\end{proposition}
\begin{proof}
The proof is largely taken from \cite{Gyenis17}. Let $A\in\B([0,2\pi))$ be measurable and consider $M=A\times[0,\pi]\in\mathfrak{M}$. Coordinate transformation between polar coordinates and the uniform measure on the circle $\P$ and further rewrites yield
\begin{align}
\int_M \E[X|\mathfrak{M}]d\P&=\frac{1}{4\pi}\int_0^\pi\int_A\E[X|\mathfrak{M}](\phi,\psi)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_0^\pi\int_A\left(\frac{1}{2}\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'\right)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_A\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'd\phi\\
&=\int_M Xd\P.
\end{align}

Note that $\mathfrak{M}$ is generated by sets like $M$, thus $\E[X|\mathfrak{M}]$ is a well-defined version of the $\mathfrak{M}$-conditional expectation of $X$.
\end{proof}

Note that this representation is different from equations (81) and (112) of \cite{Gyenis17}, where equation 81 is
\begin{equation}
\E[X|\mathfrak{M}](\phi,\psi)=\frac{1}{2}\int_0^{2\pi}X(\phi,\psi')|\sin\psi'|d\psi'
\end{equation}
and equation 112 is
\begin{equation}
\E[X|\mathfrak{M}](\phi,\psi)=\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'.
\end{equation}
The corrections on \cite{Gyenis17} are given in Appendix~\ref{app:CorMer}. We can verify that our version is the correct one.\\
Take first the meridian $M=\{\phi',\phi'+\pi\}\times[0,\pi]\in\mathfrak{M}$ with $\phi'\in[0,\pi)$. Let $\psi_1^*,\psi_2^*\in\R$ be arbitrary with $\psi_2^*-2\pi\leq\psi_1^*\leq\psi_2^*$, define the angles $\psi_1=\psi_1^*\mod2\pi$ and $\psi_2=\psi_2^*\mod2\pi$ and define arc $A\subseteq M$ as
\begin{equation}
A=\begin{cases}
\{\phi'\}\times[\psi_1,\psi_2],&\psi_1,\psi_2\leq\pi,\\
\{\phi'\}\times[\psi_1,\pi]\cup\{\phi'+\pi\}\times[\psi_2-\pi,\pi],&\psi_1\leq \pi, \psi_2>\pi,\\
\{\phi'+\pi\}\times[\psi_1-\pi,\psi_2-\pi],&\psi_1,\psi_2>\pi,\\
\{\phi'\}\times[0,\psi_1]\cup\{\phi'+\pi\}\times[0,\psi_2-\pi],&\psi_2\leq\pi,\psi_1>\pi.
\end{cases}
\end{equation}
This definition is exhaustive, yet it provides all possible arcs on a meridian while restricting ourselves to the domain $[0,2\pi)\times[0,\pi]$. Now, analogous to the latitudes, let $\P'$ be the uniform measure taking $1$ on meridian $M$ and $0$ on $M^c$. The conditional probability of $\hat{\P}$ of points being on $A$ with $\psi_1,\psi_2\leq\pi$ given there is a point on $M$ is given by
\begin{align}
\hat{\P}[A]&=\int_S\P[A|\mathfrak{M}]d\P'=\frac{1}{2}\int_S\int_0^\pi\1_A(\phi,\psi')\sin\psi'd\psi'd\P'(\phi,\psi)\\
&=\frac{1}{2}\left(\int_{S\setminus M}\int_{\phi_1}^{\phi_2}0d\psi'd\P'+\int_{M}\1_{\{\phi'\}\times[0,\pi]}\int_{\phi_1}^{\phi_2}\sin\psi'd\psi'd\P'\right)\\
&=\frac{1}{4}\int_{\phi_1}^{\phi_2}\sin\psi'd\psi'd\P'=\frac{\cos\psi_1-\cos\psi_2}{4}
\end{align}
since $\int_{\{\phi'\}\times[0,\pi]}d\P'=\frac{1}{2}$. On the other possible arcs of $M$ the probability $\hat{\P}[A]$ with $\psi_1,\psi_2$ as in the definition of $A$ becomes
\begin{equation}\label{eq:BorelMerConProb}
\hat{\P}[A]=\begin{cases}
\frac{1}{4}\left(\cos\psi_1-\cos\psi_2\right),&\psi_1,\psi_2\leq\pi,\\
\frac{1}{4}\left(2+\cos\psi_1-\cos\psi_2\right),&\psi_1\leq \pi, \psi_2>\pi,\\
\frac{1}{4}\left(\cos\psi_2-\cos\psi_1\right),&\psi_1,\psi_2>\pi,\\
\frac{1}{4}\left(2-\cos\psi_1+\cos\psi_2\right),&\psi_2\leq\pi,\psi_1>\pi.
\end{cases}
\end{equation}
Now one can immediately check that $\hat{\P}$ is well-defined on $M$ as
\begin{equation}
\hat{\P}[M]=\int_S\P[A|\mathfrak{M}]d\P'=\frac{1}{2}\int_M\int_0^\pi\sin\psi'd\psi'd\P'=-\frac{1}{2}\left(\cos\pi-\cos0\right)=1.
\end{equation}

Clearly this conditional distribution on meridians is not uniform. However, one should expect they are, as a meridian is just a rotation of a latitude and the points on the sphere are spread uniformly. An explanation of this difference is given in Section~\ref{sec:BorelExplained}.

\subsection{Combining latitudes and meridians}\label{sec:BorelCombining}
Another question one could ask is the following: if I define $\Sigma=\sigma(\mathfrak{C},\mathfrak{M})$ as the smallest $\sigma$-algebra containing both measurable subsets of meridians and latitudes, what is then the conditional probability distribution on a great circle given $\Sigma$? The answer is that in this approach the distributions in sections~\ref{sec:BorelLong} and~\ref{sec:BorelMer} can be recovered as limiting distributions of a sequence of traditional conditional probabilities defined in Section~\ref{sec:BorelNaive}.

First we'll further analyse the new $\sigma$-algebra $\Sigma$. Consider an arbitrary rectangle $(a,b)\times(c,d)\subset[0,2\pi)\times[0,\pi]$. Since by definition $(a,b)\times[0,\pi]\in\mathfrak{M}$ and $[0,2\pi)\times(c,d)\in\mathfrak{C}$, we have
\begin{equation}
(a,b)\times(c,d)=\left((a,b)\times[0,\pi]\right)\cap\left([0,2\pi)\times(c,d)\right)\in\Sigma.
\end{equation}
Thus all Borel-measurable sets on our sphere are contained in $\Sigma$. Furthermore, as $\mathfrak{C}$ and $\mathfrak{M}$ contain only Borel-measurable sets, $\Sigma=\sigma(\mathfrak{C},\mathfrak{M})$ can only have Borel-measurable sets. Therefore $\Sigma=\B$ is the $\sigma$-algebra of Borel-measurable sets on the sphere.

Now take the following approach. Pick $x\in(0,\pi)$. Define the two rectangles $C_\epsilon=[0,2\pi)\times[x-\epsilon,x+\epsilon]$ and $R_\epsilon=[a,b]\times[x-\epsilon,x+\epsilon]\subseteq C_\epsilon$ for all ${\epsilon\in(0,\min\{\pi-x,x\})}$, then what is the limiting conditional probability of $R_0$ given $C_0$? The conditional probability of $R_0$ given $C_0$ can be modeled as the limit of the sequence $\P[R_\epsilon|C_\epsilon]$ with $\epsilon\to0$, which will take on the uniform distribution on $C_0$ as calculated in Equation~\ref{eq:BorelLongConProb}.
\begin{proposition}\label{prop:BorelLongBayes}
Define the two rectangles $C_\epsilon=[0,2\pi)\times[x-\epsilon,x+\epsilon]$ and $R_\epsilon=[a,b]\times[x-\epsilon,x+\epsilon]\subset C_\epsilon$ for all $\epsilon\in(0,\min\{\pi-x,x\})$ with $x\in(0,\pi)$. The limiting conditional probability of $R_0$ given $C_0$ equals
\begin{equation}
\P[R_0|C_0]=\lim_{\epsilon\downarrow0}\P[R_\epsilon|C_\epsilon]=\frac{b-a}{2\pi}.
\end{equation}
\end{proposition}
\begin{proof}
Let $\epsilon\in(0,\min\{\pi-x,x\})$. Let $C_\epsilon=[0,2\pi)\times[x-\epsilon,x+\epsilon]\in\mathfrak{C}$. The probability $\P[R_\epsilon|C_\epsilon]$ equals
\begin{equation}
\P[R_\epsilon|C_\epsilon]=\frac{\P[R_\epsilon\cap C_\epsilon]}{\P[C_\epsilon]}=\frac{\int_{a}^{b}\int_{x-\epsilon}^{x+\epsilon}\sin\psi d\psi d\phi}{\int_{0}^{2\pi}\int_{x-\epsilon}^{x+\epsilon}\sin\psi d\psi d\phi}=\frac{b-a}{2\pi}.
\end{equation}
Thus $\P[R_\epsilon|C_\epsilon]=\frac{b-a}{2\pi}$ holds for all possible $\epsilon$, resulting in the limiting value
\begin{equation}
\P[R_0|C_0]=\lim_{\epsilon\downarrow0}\P[R_\epsilon|C_\epsilon]=\frac{b-a}{2\pi}.
\end{equation}
\end{proof}

A same proposition can be found for the meridians.

\begin{proposition}\label{prop:BorelMerBayes}
Let $x\in(0,2\pi)$ and $\epsilon\in(0,\min\{2\pi-x,x\})$. Consider $M_\epsilon=[x-\epsilon,x+\epsilon]\times[0,\pi]$ and $R_\epsilon=[x-\epsilon,x+\epsilon]\times[a,b]\subseteq M_\epsilon$, then the limiting conditional probability of $R_0$ given $M_0$ is
\begin{equation}
\P[R_0|M_0]=\lim_{\epsilon\downarrow0}\P[R_\epsilon|M_\epsilon]=\frac{1}{2}(\cos a-\cos b)
\end{equation}
\end{proposition}
\begin{proof}
The conditional probability $\P[R_\epsilon|M_\epsilon]$ equals
\begin{equation}
\P[R_\epsilon|M_\epsilon]=\frac{\P[R_\epsilon\cap M_\epsilon]}{\P[M_\epsilon]}=\frac{\int_{x-\epsilon}^{x+\epsilon}\int_a^b\sin\psi d\psi d\phi}{\int_{x-\epsilon}^{x+\epsilon}\int_0^{\pi}\sin\psi d\psi d\phi}=\frac{1}{2}\left(\cos a-\cos b\right).
\end{equation}
The limit of $\epsilon\downarrow0$ remains $\P[R_0|M_0]=\frac{1}{2}\left(\cos a-\cos b\right)$.
\end{proof}

Note that the probability in Proposition~\ref{prop:BorelMerBayes} has $\frac{1}{2}$ as normalization factor instead of $\frac{1}{4}$, as $M_\epsilon$ converges to only a half meridian. By symmetry the distribution on the whole meridian follows the absolute cosine function.

The conditional probabilities on $\mathfrak{C}$ and $\mathfrak{M}$ therefore can be approached by traditional conditional probabilities on $\B$. This gives rise to the following conjecture.
\begin{conjecture}\label{con:BorelConjecture}
Let $(X,\F,\P)$ be a probability space. Let $F\in\mathcal{F}$ have zero measure and let $E\subseteq F$ be measurable. Let $\G\subset\F$ be a sub-$\sigma$-algebra containing $E$ and $F$ with $\G\neq\F$. Let $\{F_n\}_{n\in\N}\subset\G$ be a sequence converging to $F$ and $\{E_n\}_{n\in\N}\subset\G$ be a sequence converging to $E$ with for all $n\in\N$ the sets $E_n$ and $F_n$ have positive measure, $E_n\subseteq F_n$ holds and the limit $\lim_{n\to\infty}\P[E_n|F_n]$ exists. Then
\begin{equation}
\P[E|F]:=\lim_{n\to\infty}\P[E_n|F_n]=\int_X\E[\1_F|\mathcal{G}]d\P'(E)
\end{equation}
where $\P'$ is the probability measure taking $1$ on $F$ and $\E[\cdot|\G]$ is a version of the conditional expectation on $\G$.
\end{conjecture}

If Conjecture~\ref{con:BorelConjecture} is true, then all conditional probabilities on null sets can be computed by limiting traditional conditional probabilities. It seems to hold as for the Borel-Kolmogorov paradox when considering latitudes we can take $\F=\B$ and $\G=\mathfrak{C}$ and when considering meridians we can take $\F=\B$ and $\G=\mathfrak{M}$. It however is false as the following proposition will point out.

\begin{proposition}\label{prop:BorelConFalse}
Conjecture~\ref{con:BorelConjecture} is false.
\end{proposition}
\begin{proof}
In sections \ref{sec:BorelLong} and \ref{sec:BorelMer} we have seen that the parametrization using latitudes gave the uniform distribution and the parametrization using meridians gave a cosine. However, in Section~\ref{sec:BorelLong} the set $[0,2\pi)\times\left\{\frac{\pi}{2}\right\}$ is considered, the red horizontal line in Figure~\ref{fig:BorelVis}. Section~\ref{sec:BorelMer} treats the set $\{0,\pi\}\times[0,\pi]$, like the blue vertical lines in Figure~\ref{fig:BorelVis}. Those sets differer, however we can find a set which has two different probabilities using a rotation.

Call $f$ the transformation from polar to Euclidean coordinates from Equation~\ref{eq:BorelPolar}. Let $O$ be a rotation such that
\begin{equation}
(f^{-1}\circ O\circ f)\left([0,\pi]\times\left\{\frac{\pi}{2}\right\}\right)=\{\pi\}\times[0,\pi],
\end{equation}
thus where the largest half latitude is rotated to a half meridian. Recall that $\mathfrak{C}$ is the $\sigma$-algebra of latitudes. We now rotate those latitudes to get the following $\sigma$-algebra:
\begin{equation}
\mathfrak{C}'=\left\{F'\in\B\mid F'=\left(f^{-1}\circ O^{-1}\circ f\right)(F),F\in\mathfrak{C}\right\}.
\end{equation}
Now $F=\{\pi\}\times[0,\pi]$ is an element of both $\mathfrak{C}'$ and $\mathfrak{M}$. We will prove that different methods of converging to $F$ yield to different $\P[E|F]$ for a measurable $E$. We will take the set $E=\{\pi\}\times\left[\frac{1}{4}\pi,\frac{3}{4}\pi\right]$ as our example.

First take a look at $\mathfrak{C}'$. Let $\epsilon\in\left(0,\frac{\pi}{2}\right)$ be arbitrary and consider the sets $C_\epsilon=[0,\pi]\times\left[\frac{\pi}{2}-\epsilon,\frac{\pi}{2}+\epsilon\right]$ and $R_\epsilon=\left[\frac{1}{4}\pi,\frac{3}{4}\pi\right]\times\left[\frac{\pi}{2}-\epsilon,\frac{\pi}{2}+\epsilon\right]$. Let $\{R'_{n^{-1}}\}_{n\in\N}$ be with $R'_{n^{-1}}=(f^{-1}\circ O^{-1}\circ f)(R_{n^{-1}})$ and let the sequence $\{C'_{n^{-1}}\}_{n\in\N}$ be with $C'_{n^{-1}}=(f^{-1}\circ O^{-1}\circ f)(C_{n^{-1}})$. Since $R_{n^{-1}},C_{n^{-1}}\in\mathfrak{C}$ holds for all $n\in\N$, we get $\{R'_{n^{-1}}\}_{n\in\N},\{C'_{n^{-1}}\}_{n\in\N}\subset\mathfrak{C'}$. Furthermore, $f^{-1}\circ O^{-1}\circ f$ has determinant $1$ since $O$ is a rotation and $f$ is almost surely a bijection on $F$, resulting to
\begin{align}
\P[R'_\epsilon|C'_\epsilon]&=\frac{\P[R'_\epsilon]}{\P[C'_\epsilon]}=\frac{\iint_{R'_\epsilon}\sin\psi d\psi d\phi}{\iint_{C'_\epsilon} \sin\psi d\psi d\phi}=\frac{\iint_{R_\epsilon}g(\sin\psi)\cdot1 d\psi d\phi}{\iint_{C_\epsilon}g(\sin\psi)\cdot1d\psi d\phi}\\
&=\frac{\int_{\frac{1}{4}\pi}^{\frac{3}{4}\pi}\int_{\frac{\pi}{2}-\epsilon}^{\frac{\pi}{2}+\epsilon}g(\sin\psi)\cdot1 d\psi d\phi}{\int_{0}^{\pi}\int_{\frac{\pi}{2}-\epsilon}^{\frac{\pi}{2}+\epsilon}g(\sin\psi)\cdot1d\psi d\phi}=\frac{\frac{3}{4}\pi-\frac{1}{4}\pi}{\pi}=\frac{1}{2}
\end{align}
where $g(x)$ is the result of the transformation $f^{-1}\circ O^{-1}\circ f$ on the integrand. The function $g$ is not written out explicitly, as the integral can be split up as a multiplication of integrals and the resulting integrals in the numerator en denominator with $g$ in the integrand are equal, thus cancel out. The result is $\P[R'_0|C'_0]=\frac{1}{2}$, as $\P[R'_\epsilon|C'_\epsilon]$ is constant in $\epsilon$. The sequence $\{R'_{n^{-1}}\}_{n\in\N}$ converges to $E$ and the sequence $\{C'_{n^{-1}}\}_{n\in\N}$ converges to $F$, thus we have $\P[E|F]=\frac{1}{2}$ as well.

For $\mathfrak{M}$ we will use Proposition~\ref{prop:BorelMerBayes}. Let $x=\pi$ and $\epsilon\in(0,\pi)$, then we have $M_\epsilon\in\mathfrak{M}$ and $R_\epsilon=[\pi-\epsilon,\pi+\epsilon]\times\left[\frac{1}{4}\pi,\frac{3}{4}\pi\right]\subset M_\epsilon$. Furthermore, the sequence $\{M_{n^{-1}}\}_{n\in\N}$ converges to $F$ and $\{R_{n^{-1}}\}_{n\in\N}$ converges to $E$, thus Proposition~\ref{prop:BorelMerBayes} states that
\begin{equation}
\P[E|F]=\frac{1}{2}\left(\cos\left(\frac{1}{4}\pi\right)-\cos\left(\frac{3}{4}\pi\right)\right)=\frac{\sqrt{2}}{2}.
\end{equation}

We now have both $\P[E|F]=\frac{1}{2}$ and $\P[E|F]=\frac{1}{2}\sqrt{2}$, which are clearly not equal. Conjecture~\ref{con:BorelConjecture} is therefore false.
\end{proof}

This demonstrates that when having an $F\in\F$ with zero measure, one cannot uniquely define $\P[E|F]$. A conditional expectation needs to be accompanied by a sub-$\sigma$-algebra, otherwise contradictory results can arise.

The idea underlying Conjecture~\ref{con:BorelConjecture} is however useful to guess a conditional expectation. The measure-theoretic definition of conditional expectation is only a list of properties, not a constructive definition. In the case of a $\mathfrak{C}$-conditional expectation, looking at Proposition~\ref{prop:BorelLongBayes} and its proof we observe that the resulting probability is $\frac{b-a}{2\pi}$, thus uniform. Furthermore, $\phi$ is the only variable that is integrated, thus one can guess $\phi$ is the only needed variable. Therefore one can suspect that
\begin{equation}
\E\left[\1_{[a,b]\times\left\{\frac{\pi}{2}\right\}}\middle|\mathfrak{C}\right](\phi,\psi)=\frac{1}{2\pi}\int_0^{2\pi}\1_{[a,b]\times\left\{\frac{\pi}{2}\right\}}(\phi',\psi)d\phi'=\frac{b-a}{2\pi}\1_{\left\{\frac{\pi}{2}\right\}}(\psi)
\end{equation}
holds and the guess can be generalized to
\begin{equation}
\E[X|\mathfrak{C}](\phi,\psi)=\frac{1}{2\pi}\int_0^{2\pi}X(\phi',\psi)d\phi'.
\end{equation}
This is in fact the same $\mathfrak{C}$-conditional expectation of $X$ by Proposition~\ref{prop:BorelLongBayes}. The same steps can be taken to guess $\E[X|\mathfrak{M}]$ using Proposition~\ref{prop:BorelMerBayes}.

Thus we can conclude that it is wise to look at a converging sequence of traditional conditional probabilities for a potential conditional expectation when conditioning on zero sets. However, it must be noted that the resulting conditional probability is not uniquely defined, must be checked whether it complies to the actual definition of conditional probability and depends on the chosen sub-$\sigma$-algebra.

\section{The Borel-Kolmogorov paradox explained}\label{sec:BorelExplained}
Following the arguments of \cite{Gyenis17} we will argue that the difference in conditional distribution is no paradox, but a misinterpretation of conditional probability.

A first explanation is quite intuitive. All meridians intersect at the north and south pole of the sphere, whereas latitudes do not intersect. Therefore the $\sigma$-algebras $\mathfrak{M}$ and $\mathfrak{C}$ are vastly different. If points are spread uniformly on the meridians, the distribution of mass on the whole sphere will not be uniform and the density of mass will be highest at the poles. This is simulated by \cite{Weisstein} and his simulations support this statement.

Let $\bar{\P}$ be the conditional probability measure on a latitude, defined by Equation~\ref{eq:BorelLongConProb} and let $\hat{\P}$ be the conditional probability measure on a meridian, defined by Equation~\ref{eq:BorelMerConProb}. If the spaces $(S, \mathfrak{C}, \bar{\P})$ and $(S, \mathfrak{M}, \hat{\P})$ are isomorphic with a measurable bijection, then $\hat{\P}$ and $\bar{\P}$ must have equal distribution and the results of sections~\ref{sec:BorelLong} and \ref{sec:BorelMer} must be paradoxical and contradictory. If there is a measurable bijection from a meridian to a latitude, their conditional probability distributions must agree. This is not the case here.
\begin{theorem}[\cite{Gyenis17}]
Let $f\colon S\to S$ be a measurable bijection with measurable inverse. There is no Boolean algebra isomorphism $h_f\colon\mathfrak{C}\to\mathfrak{M}$ that is determined by $f$.
\end{theorem}
\begin{proof}
The proof can be found in \cite{Gyenis17}, but we will repeat it here. Suppose such isomorphism $h_f$ exists. All latitudes $C$ in $\mathfrak{C}$ are the only atoms of $\mathfrak{C}$, therefore $h_f(C)$ are the only atoms of $\mathfrak{M}$ as well, making $h_f(C)$ meridians. We will now prove that $h_f(C)$ cannot be a meridian.

Let $m_0=\{(0,0), (0,\pi)\}$ be the set of north and south poles. Let $c_0\in\mathfrak{C}$ be such that $h_f(c_0)=m_0$. Then $c_0$ consists of two elements as well, thus we can choose latitude $C$ not on the north or south poles such that $C\cap c_0=\emptyset$. Note that $h_f(C)$ is a meridian, thus $m_0\subset h_f(C)$ as all meridians pass through the north and south poles. Since $h_f$ is a Boolean algebra isomorphism, we have $h_f(C\cap c_0)=h_f(C)\cap h_f(c_0)$. This all can be combined in the following line:
\begin{equation}
\emptyset=h_f(\emptyset)=h_f(C\cap c_0)=h_f(C)\cap h_f(c_0)=h_f(C)\cap m_0=m_0.
\end{equation}
This is clearly a contradiction.

Let $C$ be the north or south pole of the sphere. Since $h_f$ is a bijection, $h_f(C)$ is only a single point as well. Thus $h_f(C)$ cannot be an atom of $\mathfrak{M}$. Therefore there is a contradiction here as well.

Thus no single atom of $\mathfrak{C}$ result into an atom of $\mathfrak{M}$ by $h_f$ and therefore function $h_f$ cannot exist.
\end{proof}

Therefore every measurable bijection between $(S,\mathfrak{C})$ and $(S,\mathfrak{M})$ has no Boolean algebra isomorphism $h_f\colon\mathfrak{C}\to\mathfrak{M}$ such that latitudes in $\mathfrak{C}$ are mapped to meridians on $\mathfrak{M}$. This theorem holds on all subalgebras of $\mathfrak{C}$ and $\mathfrak{M}$ as well, as is proven by \cite{Gyenis17}.

Now it should be clear why the conditional distribution on the latitudes and meridians of the sphere differ, since we are dealing with two entirely different structured spaces. Therefore, the question `why is the conditional distribution on the latitudes different from the conditional distribution on the meridians' is easy to answer; difference in spaces. To verify that two different methods of conditioning on the same set yield to the correct conditional probability, one must ask the following question: `if one models a distribution on the sample space using conditional expectation on a sub-$\sigma$-algebra, does the resulting conditional probability distribution extend to the original distribution on the whole sample space?' As earlier mentioned, Weisstein \cite{Weisstein} demonstrated that the answer to that question is yes. The uniform distribution on latitudes and the cosine on meridians both extend to the uniform distribution on the whole sphere. Thus when dealing with different conditional distributions on measure zero sets, one should rather ask a question like the last one, as the answer to that question must always be yes. Further reading and a more in-depth analysis can be found in \cite{Gyenis17}.


\section{Conclusion}
After analysing the Borel-Kolmogorov paradox, the first and most important conclusion we can take is that when conditioning, especially on a set $F$ of measure zero, one needs to give the accompanying sub-$\sigma$-algebra of the event that is conditioned on. Otherwise the Borel-Kolmogorov phenomenon appears, where one can give different sub-$\sigma$-algebras containing $F$ such that the resulting conditional probability distribution is different.

Secondly, we must accept that conditional probability on sets of measure zero are not uniquely defined. That uniqueness only appears when conditioning on sets of positive measure, as then the classical rule $\P[E|F]=\frac{\P[E\cap F]}{\P[F]}$ gives a version of the conditional probability. If a sub-$\sigma$-algebra can be used to model the entire sample space, for example the sub-$\sigma$-algebra of latitudes with the sphere as sample space, then the conditional probability must be able to extend to the original probability distribution. For example providing the uniform distribution on all latitudes gives back the uniform distribution on the sphere, as well as applying the cosine distribution on the meridians results into the uniform distribution on the sphere.

Thirdly, a conditional distribution on a zero set must be calculated using the measure-theoretic conditional expectation. Conjecture~\ref{con:BorelConjecture} can help with finding the correct conditional expectation, however Proposition~\ref{prop:BorelConFalse} proves that the resulting conditional probability does not need to be unique. Therefore Conjecture~\ref{con:BorelConjecture} cannot be used to calculate a conditional probability and then call it a day, you need to check whether the distribution you found actually satisfies all conditions of conditional probability.

At last, as Kolmogorov \cite{Kolmogorov33} already stated in his work, there is nothing paradoxical going on in the Borel-Kolmogorov paradox. The space of the sphere with the latitudes as $\sigma$-algebra is not homeomorphic with the space of the sphere and the meridians as $\sigma$-algebra. The not-uniqueness of the conditional probability on a zero set is thus as expected and we must give the sub-$\sigma$-algebra when conditioning on sets of measure zero.


\chapter{Safe probability}\label{chap:SafeProp}
Take a look at the following game of dice, devised by Grünwald \cite{Grunwald13}. A player and game master are present and the game master casts a fair die. He peeks at the value of the die and must either state that the value is 4 or lower or state that the value is 3 or higher. The game master is not allowed to lie. Upon hearing the game master's advice, what is the probability the die has landed on a 3?\\
Suppose an alternative game is considered, where the game master can only say that the value is either from 1 to and including 4 or the value is 5 or 6. The question of the probability of the die having 3 becomes easy to answer. When 5 or 6 is revealed, the probability is 0 and when 1 to 4 is revealed, traditional conditional probability gives us that the probability is 0.25.\\
However, when either `4 or lower' or `3 or higher' is stated, the game master can choose his statement when the die lands on a 3 or 4. His strategy on choosing his statement heavily influences the probability of the die landing on 3 upon hearing the game master's statement, making that probability not unique.

What can we say about the probability of the die landing on 3? Traditional conditional probability will not suffice here. We want to condition on sets with positive measure and the versions of conditional expectation then almost surely coincides with traditional conditional probability, thus using measure-theoretic conditional probability will not give more insight than conditional probability.

If we loose our notion of conditional probability, we can resort to \emph{safe probability}, introduced by Grünwald in 2018 \cite{Grunwald18}. Suppose there are two random variables, $U$ and $V$, and we want to use the distribution $U|V$. Suppose however that there is no method of finding this distribution, only you that you know that it is a member of a model of distributions $\Pmod$. One can then create a new conditional distribution, not necessarily one from our model nor necessarily reflecting the true distribution at all, which gives a probability of $U$ given $V$ that on average performs equal for all distributions in our model $\Pmod$ in predicting $U$ given $V$. For example, one notion of safe probability is that we have a distribution $\Psafe$ such that $\E_{\P}[U]=\E_{\P}[\E_{\Psafe}[U|V]]$ holds for all $\P\in\Pmod$. Under this $\Psafe$ the expectation $\E_{\Psafe}[U|V]$ is an unbiased estimator of $U$ for all distributions $\P\in\Pmod$. Thus no matter which strategy the game master has, distribution $\Psafe$ allows the player to estimate $U$ on average correctly.

There are more notions on safety, further restricting the freedom $\Psafe$ has. We will first introduce some notions of safe probability and how all notions are related to each other. The example of the dice game is then continued to illustrate how safe probability can be applied.
\section{Definitions}
Before we will define safe probability, we need to introduce some notation.

\begin{definition}[Range and support]
Let $S\colon\X\to\Y$ be a random variable. The \emph{range} of $S$ is
\begin{equation}
\range(S)=\{s\in\Y\mid s=S(x),x\in\X\}.
\end{equation}
Let $\P$ be a probability distribution on $\X$. The \emph{support} of $S$ under $\P$ is
\begin{equation}
\supp_{\P}(S)=\{s\in \Y\mid\P[S=s]>0\}.
\end{equation}
\end{definition}

Let $U$ and $U'$ be two random variables and suppose there is a function $f$ such that $f(U)=U'$ holds, then we will write $U\stackrel{f}{\rightsquigarrow}U'$. If a function $f$ exists such that $U\stackrel{f}{\rightsquigarrow}V$, we will write $U\rightsquigarrow V$. Random variable $V$ is then called a \emph{coarsening} of $U$ or equivalently we state that $U$ determines $V$.

Let $\Pmod$ be a set of probability measures, then $\Pmod$ is called a \emph{model}.

Lastly, if we keep $U$ and $V$ as random variables, the distribution of $U$ under $\P$ is written as $\P[U]$. The distribution of $U$ given $V$ under $\P$ is written as $\P[U|V]$.

We are now able to define conditional probability. The following definitions and propositions are all taken from \cite{Grunwald18}.

\begin{definition}[Safe probabilities]\label{def:SafeProp}
Let $\Omega$ be a sample space and let $\Pmod$ be a set of distributions on $\Omega$. Let $U$ be a real-valued random variable with finite expectation and let $V$ be a random variable on $\Omega$. Let $\Psafe$ be a probability distribution on $\Omega$. Then $\Psafe$ is \emph{safe} with respect to $\Pmod$ for
\begin{itemize}
    \item $\langle U\rangle|\langle V\rangle$ if $\E_{\P}[U]=\E_{\P}[\E_{\Psafe}[U|V]]$ holds for all $\P\in\Pmod$. This $\Psafe$ is called \emph{unbiased} for $U|V$.
    \item $\langle U\rangle|[V]$ if $\E_{\P}[U]=\E_{\Psafe}[U|V=v]$ holds for all $v\in\supp_{\Psafe}(V)$. This $\Psafe$ is called \emph{marginally valid} for $U|V$.
\end{itemize}
\end{definition}
Since in almost all situations we only consider a single model $\Pmod$, we will from now on omit the statement `with respect to $\Pmod$'.
\begin{definition}[Stronger safety]\label{def:SafeStrongProp}
Let $\Omega,\Pmod,U,V$ and $\Psafe$ as above. Then $\Psafe$ is \emph{safe} for
\begin{itemize}
    \item $U|\langle V\rangle$ if for all real-valued random variables $U'$ on $\Omega$ with $U\rightsquigarrow U'$ distribution $\Psafe$ is safe for $\langle U'\rangle|\langle V\rangle$.
    \item $U|[V]$ if for all real-valued random variables $U'$ on $\Omega$ with $U\rightsquigarrow U'$ distribution $\Psafe$ is safe for $\langle U'\rangle|[V]$.
\end{itemize}
\end{definition}

Both definitions introduce a lot of new notation. In practice, $\langle V\rangle$ implies that $\E_{\Psafe}[U|V]$ must be an unbiased estimator for $U$ for all $\P\in\Pmod$.\\
When $[V]$ is considered, the expected value of $U|V=v$ under $\Psafe$ must equal the expected value of $U$ under $\P$ for all $v\in\supp_{\P}(V)$ and $\P\in\Pmod$. In other words, the outcome of $V$ will not influence the conditional expectation value of $U$ under $\Psafe$ and this expectation will equal the expectation of $U$ under all $\P\in\Pmod$. Note that when $\E_{\P}[U]$ does not remain constant for all $\P\in\Pmod$, safety for $[V]$ is immediately impossible.\\
Lastly, the difference between $\langle U\rangle$ and $U$ is that for $\langle U\rangle$ the requirement for safety does only have to be met for $U$. When $U$ is considered, the requirement must be met for all coarsenings $U'$ of $U$.

When we say that $\Psafe$ is safe for $U|[V]$, we actually mean that `for all $\P\in\Pmod$ we have $\supp_{\P}(V)\subseteq\supp_{\Psafe}(V)$, $\E_{\P}[U]$ is well-defined and finite, $\E_{\Psafe}[U|V=v]$ is well-defined for all $v\in\supp_{\Psafe}(V)$, both expectations are equal and this holds for all $U'$ with $U\rightsquigarrow U'$'. This statement is quite lengthy, therefore we will abbreviate it by stating that $\Psafe$ is safe for $U|[V]$. Such abbreviations are performed for other notions of safety as well.

Definition~\ref{def:SafeStrongProp} is not easy to work with as constructing a $\Psafe$ that is safe for $U|[V]$ requires checking $\Psafe$ against all coarsenings $U'$ of $U$. The following proposition from \cite{Grunwald18} gives us tools to actually create a safe $\Psafe$ for $U|\langle V\rangle$, $U|[V]$ and a few extra notions.

\begin{proposition}\label{prop:SafeProperties}
Let $\Omega$, $\Pmod$, $U$, $V$ and $\Psafe$ be as above. Then
\begin{enumerate}
    \item $\Psafe$ is safe for $U|\langle V\rangle$ if and only if for all $\P\in\Pmod$ there is a distribution $\P'$ on $\Omega$ with $\P'[U=u,V=v]=\Psafe[U=u|V=v]\P[V=v]$ for all $(u,v)\in\range((U,V))$ such that $\P'[U]=\P[U]$.
    \item $\Psafe$ is safe for $\langle U\rangle|V$ if and only if
        \begin{equation}
    \E_{\P}[U|V]=\E_{\Psafe}[U|V]
    \end{equation}
    holds with probability $1$ for all $\P\in\Pmod$. This $\Psafe$ is called \emph{squared error-optimal} for $U|V$.
    \item $\Psafe$ is safe for $U|V$ if and only if
        \begin{equation}
    \P[U|V]=\Psafe[U|V]
    \end{equation}
    holds with probability $1$ for all $\P\in\Pmod$. This $\Psafe$ is called \emph{valid} for $U|V$.
    \item $\Psafe$ is safe for $U|[V]$ if and only if $\P[U]=\Psafe[U|V=v]$ for all $\P\in\Pmod$ and $v\in\supp_{\Psafe}(V)$.
\end{enumerate}
\end{proposition}
\begin{proof}
The proof is in \cite{Grunwald16}, section A.2.
\end{proof}

It is easy to see that safety for $\langle U\rangle|[V]$ implies safety for $\langle U\rangle|\langle V\rangle$. If we have $\E_{\Psafe}[U|V=v]=\E_{\P}[U]$ for all $v\in\supp_{\Psafe}(V)$, then
\begin{equation}
\E_{\P}[\E_{\Psafe}[U|V]]=\E_{\P}[\E_{\P}[U]]=\E_{\P}[U]
\end{equation}
must hold. All implications are stated in the following proposition.

\begin{figure}
\centering{
\input{figures/SafeDiagram.tikz}
}
\caption{A diagram of implications between safe probabilities. If $\Psafe$ is safe for X, then it is safe for all statements that can be reached from X as well.}
\label{fig:SafeDiagram}
\end{figure}

Note that safety for $U|V$ will almost never occur. When a $\Psafe$ is safe for $U|V$, then all probability measures in $\Pmod$ have an almost surely equal distribution on $U|V$. In that case we should not bother ourselves with safe probability and just use one distribution from $\Pmod$ as our probability. Furthermore, if $\Pmod=\{\P\}$ consists of only a single probability measure, then $\P$ is automatically safe for $U|V$. Therefore it is wise to make the model $\Pmod$ large enough when considering safe probabilities, otherwise the safe distribution is prone to overfitting.

\begin{proposition}\label{prop:SafeImply}
Let $U$, $V$ and $\Psafe$ be as above.
\begin{enumerate}
\item If $\Psafe$ is safe for $U|V$, it is safe for $\langle U\rangle|V$ and $U|[V]$.
\item If $\Psafe$ is safe for $U|[V]$, it is safe for $U|\langle V\rangle$ and $\langle U\rangle|[V]$.
\item If $\Psafe$ is safe for $\langle U\rangle|V$, $U|\langle V\rangle$ or $\langle U\rangle|[V]$, it is safe for $\langle U\rangle|\langle V\rangle$.
\end{enumerate}
All relations are visualised in Figure~\ref{fig:SafeDiagram}.
\end{proposition}
\begin{proof}
All implications are found and proven in \cite{Grunwald18}.
\end{proof}

\section{The dice game}\label{sec:SafeDice}
Recall the dice game from the introduction of this chapter, introduced by \cite{Grunwald13}. The game master casts a fair die and observes a value. This value is either in the set $\{1,2,3,4\}$ or in the set $\{3,4,5,6\}$. The game master must reveal one of the two sets to the player and cannot lie. The player must then guess the probability that the die has landed on 3.

It is insufficient to take $\Omega=\{1,2,3,4,5,6\}$ as our sample space. Let $U$ be a random variable denoting the die's value and let $V$ be the statement of the game master, then $\P[U=3|V=\{1,2,3,4\}]$ cannot be calculated. When the die has rolled to a $3$, the game master can choose between $\{1,2,3,4\}$ and $\{3,4,5,6\}$ and this must be taken into account. The smallest $\sigma$-algebra containing both $\{1,2,3,4\}$ and $\{3,4,5,6\}$ is
\begin{equation}
\G=\sigma(\{1,2\},\{3,4\},\{5,6\}),
\end{equation}
thus $\{3\}$ is no member of $\G$ as $\G$ cannot have sets of odd size. When adding $\{3\}$ to $\G$, we get $\G'=\sigma(\G\cup\{3\})=2^\Omega$, the power set of $\Omega$. Therefore we cannot condition in $\Omega$ to compute $U=3$ given $V=\{1,2,3,4\}$.

Therefore we need to extend our sample space. We will use the extension introduced in \cite{Grunwald13}. Take $\X=\{1,2,3,4,5,6\}$ and $\Y=\{\{1,2,3,4\},\{3,4,5,6\}\}$ and let our sample space be $\Omega=\X\times\Y$. Let $U$ be an $\X$-valued random variable and let $V$ be a $\Y$-valued random variable. In this space conditioning on $\{1,2,3,4\}$ is valid, as $\G=\sigma(\{3\}\times Y)$ is a strict sub-$\sigma$-algebra of $2^\Omega$ for all subsets $Y\subseteq\Y$.
\subsection{Using traditional conditional probability}
First we will try using traditional conditional probability. Let $\P$ be a probability measure with the uniform distribution on $U$. To model the strategy of the game master, we need $p,q\in[0,1]$ such that
\begin{align}
\P[V=\{3,4,5,6\}\mid U=3]&=p,&\P[V=\{3,4,5,6\}\mid U=4]&=q.
\end{align}

We can now for example calculate the probability of the die landing on $6$ after the game master reveals $\{3,4,5,6\}$. This calculation can be found in equation~2 of \cite{Grunwald13} and it states that
\begin{equation}\label{eq:SafeDiceCon6}
\P[U=6\mid V=\{3,4,5,6\}]=\frac{1}{p+q+2}.
\end{equation}
Thus the conditional probability of rolling $6$ ranges from $\frac{1}{4}$ to $\frac{1}{2}$ depending on the game master's strategy. This should not suffice as answer, as the player does not know the game master's strategy.

\subsection{Using safe probabilities}
Can we create a distribution $\Psafe$ such that the probability of $U=3$ given $V$ is independent of $p$ and $q$ and behaves on average like the dice? The answer is yes and we need to use safe probability. Consider as model
\begin{equation}
\Pmod=\left\{\P\middle|\forall u\in\X:\P[U=u]=\frac{1}{6},\forall v\in\Y:\P[U\in v\mid V=v]=1\right\}.
\end{equation}
We do not know the strategy of the game master when $3$ or $4$ is rolled. However, we do know that the die is fair and that the game master is not able to lie, thus we put this information in our model.

For shorthand purposes, we write $y_1=\{1,2,3,4\}$ and $y_2=\{3,4,5,6\}$ such that $\Y=\{y_1,y_2\}$ from now on.

\begin{proposition}\label{prop:SafeDice}
Let $\X$, $\Y$, $\Omega$, $U$, $V$ and $\Pmod$ be as before. Let
\begin{equation}
\tilde{\mathcal{P}}=\left\{\P\middle|
\begin{array}{l}
\P[U=1\mid V=y_1]=\P[U=2\mid V=y_1],\\
\P[U=3\mid V=y_1]=\frac{1}{2}-5\P[U=1\mid V=y_1],\\
\P[U=5\mid V=y_2]=\P[U=6\mid V=y_2],\\
\P[U=3\mid V=y_2]=3\P[U=5\mid V=y_2]+\frac{1}{2},\\
\P[U=1\mid V=y_1],\P[U=5\mid V=y_1]\in\left[0,\frac{1}{10}\right]
\end{array}
\right\}
\end{equation}
be a set of probability distributions on $\Omega$. Let $\Psafe$ be a probability distribution on $\Omega$ with $\Psafe[U=1|V=y_1]=\Psafe[U=2|V=y_1]$, $\Psafe[U=5|V=y_2]=\Psafe[U=6|V=y_2]$ and $\Psafe[U\in y|V=y]=1$ for all $y\in\Y$. The following are equivalent:
\begin{enumerate}
    \item $\Psafe$ is a member of $\tilde{\mathcal{P}}$.
    \item $\Psafe$ is safe for $\langle U\rangle|[V]$.
    \item $\Psafe$ is safe for $\langle U\rangle|\langle V\rangle$.
\end{enumerate}

Distribution $\Psafe$ is not safe for $U|[V]$.
\end{proposition}
\begin{proof}
Here we will abbreviate $\Psafe[U=u|V=v]$ to $\Psafe[u|v]$ for all $(u,v)\in\Omega$. We will start with the implication from 1 to 2. Take an arbitrary $\Psafe\in\tilde{\mathcal{P}}$. We then have
\begin{equation}
\E_{\P}[U]=\sum_{u=1}^6 u\P[U=u]=\frac{7}{2}.
\end{equation}
Consider $y_1$ first. Writing $\Psafe[u|y_1]$ in terms of $\Psafe[1|y_1]$ for all $u\in\{2,3,4\}$ gives
\begin{align}
\E_{\Psafe}[U|V=y_1]&=\Psafe[1|y_1]+2\Psafe[1|y_1]+3\left(\frac{1}{2}-5\Psafe[1|y_1]\right)+4\left(3\Psafe[1|y_1]+\frac{1}{2}\right)\\
&=\frac{7}{2}+15\Psafe[1|y_1]-15\Psafe[1|y_1]=\frac{7}{2}.
\end{align}
The same calculation holds for $V=y_2$ as well. Therefore we have the equality $\E_{\P}[U]=\E_{\Psafe}[U|V=v]$ for all $\P\in\Pmod$ and $v\in\supp_{\Psafe}(V)$, letting $\Psafe$ fulfil the requirement of safety for $\langle U\rangle|[V]$.

The implication from 2 to 3 follows from Proposition~\ref{prop:SafeImply}.

Lastly, we will prove the implication from 3 to 1. Let $\Psafe$ be safe for $\langle U\rangle|\langle V\rangle$. Let $\P\in\Pmod$ be arbitrary and take $p,q\in[0,1]$ with $\P[V=y_2|U=3]=p$ and $\P[V=y_2|U=4]=q$.\\
Definition~\ref{def:SafeProp} states that we need $\E_{\P}[\E_{\Psafe}[U|V]]=\E_{\P}[U]$. The right-hand side is already computed as $\E_{\P}[U]=\frac{7}{2}$. For $\E_{\P}[\E_{\Psafe}[U|V]]$, we first focus on $V=y_1$. Conditioning on $U$ yields
\begin{equation}
\P[V=y_1]=\sum_{u=1}^6\P[V=y_1|U=u]\P[U=u]=\frac{1}{6}\sum_{u=1}^6\P[V=y_1|U=u].
\end{equation}
The game master cannot lie, thus $\P[V=y_1|U=1]=\P[V=y_1|U=2]=1$ and $\P[V=y_1|U=5]=\P[V=y_1|U=6]=0$ immediately hold. We further have $\P[V=y_2|U=3]=p$ and $\P[V=y_2|U=4]=q$, thus the value of $\P[V=y_1]$ is
\begin{equation}
\P[V=y_1]=\frac{1}{6}\sum_{u=1}^6\P[V=y_1|U=u]=\frac{2}{3}-\frac{p+q}{6}.
\end{equation}
Since $\Y$ only has two elements, $\P[V=y_2]=\frac{1}{3}+\frac{p+q}{6}$ follows. We can now write out $\E_{\P}[\E_{\Psafe}[U|V]]$ to
\begin{align}
\E_{\P}[\E_{\Psafe}[U|V]]&=\E_{\Psafe}[U|V=y_1]\P[V=y_1]+\E_{\Psafe}[U|V=y_2]\P[V=y_2]\\
&=\left(\Psafe[1|y_1]+2\Psafe[2|y_1]+3\Psafe[3|y_1]+4\Psafe[4|y_1]\right)\left(\frac{2}{3}-\frac{p+q}{6}\right)\nonumber\\
&\phantom{=}+\left(3\Psafe[3|y_2]+4\Psafe[4|y_2]+5\Psafe[5|y_2]+6\Psafe[6|y_2]\right)\left(\frac{1}{3}+\frac{p+q}{6}\right).
\end{align}
This collapes to
\begin{align}
\E_{\P}[\E_{\Psafe}[U|V]]&=\frac{p+q}{6}\left(\sum_{u=3}^{6}u\Psafe[u|y_2]-\sum_{u=1}^4u\Psafe[u|y_1]\right)+\sum_{u=1}^4\frac{2u}{3}\Psafe[u|y_1]\nonumber\\
&\phantom{=}+\sum_{u=3}^6\frac{u}{3}\Psafe[u|y_2].
\end{align}
As we have $\E_{\P}[U]=\frac{7}{2}$, we need to get rid of the dependence on $p+q$. This results into the condition
\begin{equation}
\sum_{u=3}^{6}u\Psafe[u|y_2]=\sum_{u=1}^4u\Psafe[u|y_1].
\end{equation}
When this condition is satisfied, we need
\begin{equation}
\sum_{u=1}^4\frac{2u}{3}\Psafe[u|y_1]+\sum_{u=3}^6\frac{u}{3}\Psafe[u|y_2]=\frac{7}{2}
\end{equation}
for $\Psafe$ to be safe for $\langle U\rangle|\langle V\rangle$. We impose one further assumption on our safe probability to reduce the amount of possible safe probabilities. Suppose the die shows $5$ or $6$, the game master has to tell $y_2$. Thus it is logical to assume that $\Psafe[5|y_2]=\Psafe[6|y_2]$ and $\Psafe[1|y_1]=\Psafe[2|y_1]$, as when an $y_2$ is revealed there is no reason why $5$ is preferred more than $6$. As the game master cannot lie,
\begin{equation}
\Psafe[5|y_1]=\Psafe[6|y_1]=\Psafe[1|y_2]=\Psafe[2|y_2]=0
\end{equation}
can be assumed. Luckily, these assumptions are made mandatory by the proposition. This all results into the following system of equations:
\begin{equation}\label{eq:SafeDiceSystem}
\begin{pmatrix}
\frac{2}{3}&\frac{4}{3}&2&\frac{8}{3}&1&\frac{4}{3}&\frac{5}{3}&2\\
-1&-2&-3&-4&3&4&5&6\\
1&-1&0&0&0&0&0&0\\
0&0&0&0&0&0&1&-1\\
1&1&1&1&0&0&0&0\\
0&0&0&0&1&1&1&1
\end{pmatrix}\begin{pmatrix}
\Psafe[1|y_1]\\\Psafe[2|y_1]\\\Psafe[3|y_1]\\\Psafe[4|y_1]\\
\Psafe[3|y_2]\\\Psafe[4|y_2]\\\Psafe[5|y_2]\\\Psafe[6|y_2]
\end{pmatrix}=\begin{pmatrix}
\frac{7}{2}\\0\\0\\0\\1\\1
\end{pmatrix}.
\end{equation}
The set $\tilde{\mathcal{P}}$ is the set of all probability distributions that are solution to this system. As $\Psafe\in\tilde{\mathcal{P}}$ needs to be a valid probability distribution, we need to have $\Psafe[u|v]\in[0,1]$ for all $(u,v)\in\Omega$. Solving \eqref{eq:SafeDiceSystem} gives
\begin{align}
\Psafe[3|y_1]&=3\Psafe[1|y_1]+\frac{1}{2},\\
\Psafe[4|y_1]&=5\Psafe[1|y_1]-\frac{1}{2},
\end{align}
thus $\Psafe[3|y_1]\in[0,1]$ holds when $\Psafe[1|y_1]\in\left[0,\frac{1}{6}\right]$ and $\Psafe[4|y_1]\in[0,1]$ holds when $\Psafe[1|y_1]\in\left[0,\frac{1}{10}\right]$. Without loss of generality $\Psafe[5|y_2]\in\left[0,\frac{1}{10}\right]$ must hold as well. This proves the last requirement $\Psafe[1|y_2],\Psafe[5|y_2]\in\left[0,\frac{1}{10}\right]$ from $\tilde{\mathcal{P}}$ and completes our proof that if a distribution $\Psafe$ is safe for $\langle U\rangle|\langle V\rangle$ with the requirements in the proposition, it is a member of $\tilde{\mathcal{P}}$.

Lastly we turn to safety for $U|[V]$. Pick probability measure $\Psafe$ on $\Omega$ arbitrarily with $\Psafe[1|y_1]=\Psafe[2|y_1]$, $\Psafe[5|y_2]=\Psafe[6|y_2]$ and $\Psafe[U\in y|V=y]=1$ for all $y\in\Y$. Proposition~\ref{prop:SafeProperties} implies $\Psafe[u|y_1]=\frac{1}{6}$ for all $u\in\X$, which is a clear contradiction to $\Psafe[U\in y|V=y]=1$. Thus there is no probability measure $\Psafe$ with the requirements in the proposition that is safe for $U|[V]$.
\end{proof}

The original question of the dice game did not concern an overall safe probability distribution for the whole die, but rather the probability of rolling $3$. Proposition~\ref{prop:DiscDiceSafe} states that a distribution $\Psafe$ is safe for $\langle\DieInd\rangle|\langle V\rangle$ as well as $\DieInd|[V]$ if and only if $\Psafe[U=3|V=v]=\frac{1}{6}$ holds for all $v\in\Y$. It is therefore safe to ignore the statement of the game master when guessing the probability of the die rolling to a $3$ and safe to state that the probability of rolling to a $3$ remains $\frac{1}{6}$.

A safe distribution for a statement of $\langle\1_{\{U=1\}}\rangle|\langle V\rangle$ cannot exist, as the following proposition will prove.

\begin{proposition}
There is no probability distribution $\Psafe$ on $\Omega$ that is safe for $\langle\1_{\{U=u\}}\rangle|\langle V\rangle$ with $u\in\{1,2,5,6\}$ when $\Psafe[U\in v|V=v]=1$ is required for all $v\in\Y$.
\end{proposition}
\begin{proof}
Pick $u=1$ and suppose $\Psafe$ is safe for $\langle\1_{\{U=1\}}\rangle|\langle V\rangle$. Let $p,q\in[0,1]$ be as in the proof of Proposition~\ref{prop:SafeDice}. As $V=y_1$ is the only possible option when rolling $1$, we have
\begin{align}
\E_{\P}[\E_{\Psafe}[\1_{\{U=1\}}|V]]&=\Psafe[U=1|V=y_1]\P[V=y_1]\\
&=\Psafe[U=1|V=y_1]\left(\frac{2}{3}-\frac{p+q}{6}\right).
\end{align}
We need $\E_{\P}[\E_{\Psafe}[\1_{\{U=1\}}|V]]=\P[U=1]=\frac{1}{6}$, but there is no possible value for $\Psafe[U=1|V=y_1]$ such that
\begin{equation}
\Psafe[U=1|V=y_1]\left(\frac{2}{3}-\frac{p+q}{6}\right)=\frac{1}{6}
\end{equation}
holds for all $p,q\in[0,1]$. This only happens when $p+q=1$, so there is a contradiction. Therefore $\Psafe$ cannot be safe for $\langle\1_{\{U=1\}}\rangle|\langle V\rangle$.

This proof applies to all $v\in\{1,2,5,6\}$, where in the case of $v\in\{5,6\}$ we need to condition on $V=y_2$ instead of $V=y_1$.
\end{proof}

\subsection{Conclusion}
To conclude, in the example of the dice game, we cannot compute a unique conditional distribution for $U|V$. However, we were able to create a set of probability distributions that are all unbiased estimators for $\E_{\P}[U]$ with $\P\in\Pmod$. When only considering the probability of the die rolling to a specific value, we can create a safe distribution for the values 3 or 4, which will be done in Proposition~\ref{prop:DiscDiceSafe}. A safe distribution for the values $1$, $2$, $5$ and $6$ does not exist.

\chapter{Discrete conditional paradoxes}\label{chap:DiscPara}
Consider the following three problems:
\begin{enumerate}
    \item[Monty Hall:] There are three doors. Two doors have to a goat behind and one door a car. The player chooses initially a door, say he chooses door $a$. The game master then opens either door $b$ or $c$, however he never opens the door with the car. After opening a door, the player is asked if he should switch. He ultimately wins the contents behind the door he has chosen. Should the player switch to the other door?
    \item[Boy or girl:] Suppose you encounter a stranger in a bar and you two get in conversation. Suddenly, the stranger states that he has two children and at least one of them is a girl. What is the probability he has two daughters?
    \item[Dice game:] This game is already introduced in Section~\ref{sec:SafeDice}, but here is a repeat. A die is rolled. Only the game master is able to observe the die's value. He then tells you whether the value of the die is in the set $\{1,2,3,4\}$ or in the set $\{3,4,5,6\}$. What is the probability you rolled $3$ given the game master's statement?
\end{enumerate}

All three problems essentially have the same structure. Firstly, there is a finite set of outcomes in each problem. Monty Hall's problem has three doors, the boy or girl problem has three different family compositions and there are six values on a die. Secondly, the game master must reveal some information, where it is essential that there is at least one outcome that remains possible no matter what the game master reveals. For that outcome the game master is free to choose his revelation. In Monty Hall's problem, if door $a$ has the car, the game master can choose to either open door $a$ or door $b$. If for the boy or girl problem the stranger has one son and one daughter, he is free to choose whether he tells at least one child is a boy or at least one is a girl. If the die rolled 3 in the dice game, the game master is free to choose to reveal $\{1,2,3,4\}$ or $\{3,4,5,6\}$.\\
Since there is at least one outcome remaining possible no matter which revelation the game master makes, traditional conditional probability does not suffice. This is seen for example in the dice game in Section~\ref{sec:SafeDice}, where Equation~\ref{eq:SafeDiceCon6} states that the probability of rolling $6$ given $\{3,4,5,6\}$ dilates from $\frac{1}{4}$ to $\frac{1}{2}$. Measure-theoretic conditional probability is not sufficient as well, as the smallest sub-$\sigma$-algebra of $\{1,2,3,4,5,6\}$ containing the subset $\{\{1,2,3,4\},\{3,4,5,6\}\}$ does not contain the set $\{3\}$. When a larger sub-$\sigma$-algebra is chosen that does contain $\{3\}$, a version of the measure-theoretic conditional expectation cannot be found as the problem does not state the probability of for example revealing $\{1,2,3,4\}$.

All three problems can be analysed using the same methods; they are essentially equal. The game master has a choice and therefore is able to employ a strategy. To counteract such strategies we need to first model all possible probability distributions. We can then create a pragmatic probability distribution using safe probability where the decision maker can act on. This distribution performs for predicting the distribution on the outcome space on average as well as all probability distributions in the model.

In this chapter we first introduce Theorem~\ref{thm:DiscMainThm} to create safe probability distributions in the setting of the three introduced problems. Then each problem is treated individually where Theorem~\ref{thm:DiscMainThm} is applied and we will discuss the results. The dice game will be discussed in Section~\ref{sec:DiscDice}, the Monty Hall problem will be discussed in Section~\ref{sec:DiscMonty} and the boy or girl problem will be treated in Section~\ref{sec:DiscChildren}. We will conclude with section~\ref{sec:DiscConcl} by stating that there is nothing paradoxical going on with the three problems. The paradoxical statements are just misapplications of conditional probability.

\section{The main theorem}\label{sec:DiscMain}
In the general setting we consider a set $\X$ of all possible outcomes called the \emph{outcome space} and a set $\Y$ of observations the player can make called the \emph{observation space}. Our sample space then becomes $\Omega=\X\times\Y$. Let $U$ be an $\X$-valued random variable denoting the outcome of the game, e.g.~the door containing the car in Monty Hall or the family composition in the boy or girl problem. Let $V$ be a $\Y$-valued random variable denoting the statement of the game master, e.g.~the opening of door $b$ in the Monty Hall problem. Using safe probability we will create a safe distribution for $\1_{\{U=u'\}}|[V]$ for a particular $u'\in\X$.

\begin{theorem}\label{thm:DiscMainThm}
Let $\X$ be countable and $\Y$ be finite. Let $U$ be an $\X$-valued random variable and $V$ be a $\Y$-valued random variable. Let $\{p_u\}_{u\in\X}\subset[0,1]$ with $\sum_{u\in\X}p_u=1$. Let
\begin{equation}\label{eq:DiscMainMod}
\Pmod\subseteq\{\P\mid\forall u\in\X:\P[U=u]=p_u\}
\end{equation}
be our model of probability distributions on $\X\times\Y$ where $|\Y|$ distributions $\P_1,\ldots,\P_{|\Y|}\in\Pmod$ impose $|\Y|$ linearly independent vectors $(\P_i[V=v])_{v\in\Y}$ with $i\in\{1,\ldots,|\Y|\}$. Let $u\in\X$ be arbitrary and let $\Psafe$ be a distribution on $\X\times\Y$, then the following are equivalent:
\begin{enumerate}
    \item For all $v\in\Y$ we have $\Psafe[U=u|V=v]=p_{u}$.
    \item $\Psafe$ is safe for $\GeneralInd|[V]$.
    \item $\Psafe$ is safe for $\langle\GeneralInd\rangle|\langle V\rangle$.
\end{enumerate}
\end{theorem}
\begin{proof}
The implication from 1 to 2 is satisfied by Proposition~\ref{prop:SafeProperties}. Let $v\in\Y$ and $\P\in\Pmod$ be arbitrary, then we have
\begin{equation}
\Psafe[U=u|V=v]=p_{u}=\P[U=u].
\end{equation}
Proposition~\ref{prop:SafeProperties} states that $\Psafe$ is safe for $\GeneralInd|[V]$.

The implication from 2 to 3 is by Proposition~\ref{prop:SafeImply}.

Consider lastly the implication from 3 to 1. Safety for $\langle\GeneralInd\rangle|\langle V\rangle$ implies
\begin{equation}
\E_{\P}[\GeneralInd]=\E_{\P}[\E_{\Psafe}[\GeneralInd|V]]
\end{equation}
for all $\P\in\Pmod$. We will construct a sufficient $\Psafe$ from this requirement and prove that $\Psafe[U=u|V=v]=p_{u}$ for all $v\in\Y$ is necessary.\\
Let $\P\in\Pmod$ be arbitrary. Take a first look at $\E_{\P}[\GeneralInd]$, then
\begin{equation}
\E_{\P}[\GeneralInd]=\P[U=u]=p_{u}.
\end{equation}
Let $\Psafe$ be a arbitrary distribution on $\X\times\Y$ that is safe for $\langle\GeneralInd\rangle|\langle V\rangle$ and let $v\in\Y$ be arbitrary, then we can write out
\begin{equation}
\E_{\Psafe}[\GeneralInd|V=v]=\Psafe[U=u|V=v].
\end{equation}
The expectation $\E_{\P}[\E_{\Psafe}[\GeneralInd|V]]$ can now be expanded to
\begin{align}
\E_{\P}[\E_{\Psafe}[\GeneralInd|V]]&=\sum_{v\in\supp_{\P}(V)}\E_{\Psafe}[\GeneralInd|V=v]\P[V=v]\\
&=\sum_{v\in\supp_{\P}(V)}\Psafe[U=u|V=v]\P[V=v].
\end{align}
We now need to abbreviate our notation. From now on we write $\Psafe[u|v]$ instead of $\Psafe[U=u|V=v]$ for all $(u,v)\in\X\times\Y$. For safety for $\langle\GeneralInd\rangle|\langle V\rangle$ we require
\begin{equation}
p_{u}=\E_{\P}[\E_{\Psafe}[\GeneralInd|V]]=\sum_{v\in\supp_{\P}(V)}\Psafe[u|v]\P[V=v].\label{eq:DiscSafeCondition}
\end{equation}
We will also abbreviate the index of the sum to $v\in\Y$ instead of $v\in\supp_{\P}(V)$, as $\P[V=v]=0$ will be zero for every $v\in\Y\setminus\supp_{\P}(V)$ and this will have no impact on the summation.\\
Note that equation~\ref{eq:DiscSafeCondition} is a linear combination of the vectors $(\Psafe[u|v])_{v\in\Y}$ and $(\P[V=v])_{v\in\Y}$ and the values of the latter vector are known, thus the equation $\sum_{v\in\Y}\Psafe[u|v]\P[V=v]=p_{u}$ is linear. Since $\Pmod$ has $|\Y|$ distributions $\P_1,\ldots,\P_{|\Y|}$ that have linearly independent vectors $(\P_i[V=v])_{v\in\Y}$ with $i\in\{1,\ldots,|\Y|\}$, we can create a system of $|\Y|$ linearly independent equations
\begin{equation}
\sum_{v\in\Y}\Psafe[u|v]\P_i[V=v]=p_{u},\qquad i\in\{1,\ldots,|\Y|\}.
\end{equation}
Since we have $|\Y|$ linearly independent equations with $|\Y|$ unknowns $\Psafe[u|v]$ with $v\in\Y$, there is a unique solution. The coefficients $\P[V=v]$ with sum up to~$1$, thus $\Psafe[u|v]=p_{u}$ is the unique solution to this system.\\
Let now $\P\in\Pmod$ again be arbitrary, then the requirement in Equation~\ref{eq:DiscSafeCondition} still holds as
\begin{equation}
\E_{\P}[\E_{\Psafe}[\GeneralInd|V]]=\sum_{v\in\supp_{\P}(V)}\Psafe[u|v]\P[V=v]=p_{u}\sum_{v\in\supp_{\P}(V)}\P[V=v]=p_{u}.
\end{equation}
Therefore $\Psafe[u|v]=p_{u}$ for all $v\in\Y$ is required for $\Psafe$ when $\Psafe$ is safe for $\langle\GeneralInd\rangle|\langle V\rangle$, thus 3 implies 1.
\end{proof}

It looks like Theorem~\ref{thm:DiscMainThm} can only applied in a few circumstances. However, the restriction on model $\Pmod$ is in many cases quickly satisfied, as in most cases $\Pmod$ is not a strict subset of the set in Equation~\ref{eq:DiscMainMod}. If $\Pmod$ is equal to the set in Equation~\ref{eq:DiscMainMod}, then the distributions $\P_i$ with $\P_i[V=v_j]=\delta_{ij}$ for all $i,j\in\{1,\ldots,|\Y|\}$, where $\delta$ is the Kronecker delta, form a sequence of $|\Y|$ linearly independent unit vectors imposed from $|\Y|$ distributions of $\Pmod$. In other cases when excluding the unit vectors, if for all $v\in\Y$ there is a $\P\in\Pmod$ with $v\in\supp_{\P}(V)$, then finding $|\Y|$ different distributions that impose $|\Y|$ linearly independent marginal probabilities on $V$ is not a hard task; otherwise the model is simply too small. This leads to the following lemma that is helpful when checking whether the model is large enough to apply Theorem~\ref{thm:DiscMainThm}.
\begin{lemma}\label{lem:DiscMainReq}
Let $\X$, $\Y$, $U$, $V$, $\{p_u\}_{u\in\X}$ be as in Theorem~\ref{thm:DiscMainThm}.  Let
\begin{equation}
\Pmod\subseteq\{\P\mid\forall u\in\X:\P[U=u]=p_u\}.
\end{equation}
It is necessary that $\bigcup_{\P\in\Pmod}\supp_{\P}(V)=\Y$ holds for $\Pmod$ to fulfil the requirements of Theorem~\ref{thm:DiscMainThm}.
\end{lemma}
\begin{proof}
Suppose $\bigcup_{\P\in\Pmod}\supp_{\P}(V)\neq\Y$, then there is a $v'\in\Y$ with $\P[V=v']=0$ for all $\P\in\Pmod$. Take this $v'$ and let $\P_1,\ldots,\P_{|\Y|}\in\Pmod$ be an arbitrary distribution of sequences. Then the vectors $(\P_1[V=v])_{v\in\Y},\ldots,(\P_{|\Y|}[V=v])_{v\in\Y}$ are not linearly independent as the entry of $v'$ is always zero.
\end{proof}

Furthermore, Theorem~\ref{thm:DiscMainThm} implies that a safe probability for $\GeneralInd|[V]$ can be created for all $u\in\X$. However, earlier in the introduction of section~\ref{sec:DiscMain} we specifically discussed a special $u'\in\X$ that is supported by $U|V=v$ for all $v\in\supp_{\P}(V)$ and $\P\in\Pmod$.\\
Take for example the dice game, there either $3$ or $4$ can be picked as $u'$ as those values are still possible after the game master's statement that the value of the die is either in $\{1,2,3,4\}$ or in $\{3,4,5,6\}$. Using Theorem~\ref{thm:DiscMainThm} we can still create a safe distribution $\Psafe$ for $\1_{\{U=1\}}|[V]$, however this implies
\begin{equation}
\Psafe[U=1|V=\{3,4,5,6\}]=\frac{1}{6}.
\end{equation}
Thus we need to assume that the game master lies with a probability $\frac{1}{6}$ when he tells the die has rolled three or higher, while in the problem we specifically assume the game master is not able to lie. This solution or distribution should therefore not be regarded as admissible, while it is still mathematically valid. However, when calculating a safe distribution $\Psafe$ for $\DieInd|[V]$, as is done in Proposition~\ref{prop:DiscDiceSafe}, we obtain $\Psafe[U=3|V=v]=\frac{1}{6}$ for all $v\in\Y$. Since for all $v\in\Y$ a $\P\in\Pmod$ exists with $\P[U=3|V=v]>0$, putting a positive probability on $U=3$ given any value of $V$ is a wise thing to do.\\
Therefore, while Theorem~\ref{thm:DiscMainThm} can be applied in many cases, it is wise to only apply it on $\GeneralInd|V$ when firstly $\P[U=u]>0$ holds for all $\P\in\Pmod$ and secondly when this $u$ is a member of the set
\begin{equation}\label{eq:DiscMainUReq}
\bigcap_{\P\in\Pmod}\bigcap_{v\in\supp_{\P}(V)}\range(U|V=v).
\end{equation}
Explaining the second requirement, since the event $\{U=u\}$ has positive and equal probability for all $\P\in\Pmod$ and since we have $\bigcup_{\P\in\Pmod}\supp_{\P}(V)=\Y$ by Lemma~\ref{lem:DiscMainReq}, for each $v\in\Y$ a $\P\in\Pmod$ must exist with $\P[U=u|V=v]>0$ when $u$ is a member of the set in Equation~\ref{eq:DiscMainUReq}, making it sensible to put $\Psafe[U=u|V=v]=\P[U=u]>0$.

The requirement $\Y=\bigcup_{\P\in\Pmod}\supp_{\P}(V)$ is however restrictive. The following proposition drops this requirement, but the equivalence in Theorem~\ref{thm:DiscMainThm} disappears as well leaving only the implications from 1 to 2 and from 2 to 3.

\begin{proposition}\label{prop:DiscSafeMargGen}
Let $\X$, $\Y$, $U$, $V$ and $\{p_{u}\}_{u\in\X}$ be as before. Let
\begin{equation}
\Pmod\subseteq\{\P|\forall u\in\X:\P[U=u]=p_u\}
\end{equation}
be non-empty and let $\Psafe$ be a distribution on $\X\times\Y$. Let $u\in\X$, then $\Psafe$ is safe for $\GeneralInd|[V]$ if $\Psafe[U=u|V=v]=p_{u}$ holds for all $v\in\Y$. This $\Psafe$ is safe for $\langle\GeneralInd\rangle|\langle V\rangle$ as well.
\end{proposition}
\begin{proof}
Let $\P\in\Pmod$ and $v\in\Y$ be arbitrary, then we have
\begin{equation}
\Psafe[U=u|V=v]=p_{u}=\P[U=u].
\end{equation}
Proposition~\ref{prop:SafeProperties} states that this $\Psafe$ is safe for $\GeneralInd|[V]$.

Safety for $\langle\GeneralInd\rangle|\langle V\rangle$ now immediately follows for $\Psafe$ by Proposition~\ref{prop:SafeImply}.
\end{proof}

Note that from the proof of Proposition~\ref{prop:DiscSafeMargGen} one can deduce that the elaborate set-up in Theorem~\ref{thm:DiscMainThm} is not necessarily needed and that Proposition~\ref{prop:DiscSafeMargGen} is nothing more than a restricted restatement of the fourth property of Proposition~\ref{prop:SafeProperties}. However, in the case of Proposition~\ref{prop:DiscSafeMargGen}, you do not know whether distributions exist that are safe for $\langle \GeneralInd\rangle|\langle V\rangle$ but not for $\GeneralInd|[V]$. In the case of Theorem~\ref{thm:DiscMainThm}, safety for $\langle \GeneralInd\rangle|\langle V\rangle$ and $\GeneralInd|[V]$ are equivalent. Finding a safe distribution for $\GeneralInd|[V]$ is easy when the distribution on $U$ in $\Pmod$ is known, however for knowing that only these distributions are unbiased estimators for $U$ we need Theorem~\ref{thm:DiscMainThm}.

Theorem~\ref{thm:DiscMainThm} only considers single $u\in\X$. We can construct safe probability distributions for $\GeneralGenInd|V$ for larger $\X'\subset\X$ as well in the following corollary.
\begin{corollary}\label{cor:DiscSafeGeneral}
Let $\X$, $\Y$, $U$, $V$, $\{p_u\}_{u\in\Y}$ and $\Pmod$ be as in Theorem~\ref{thm:DiscMainThm}. Let $\X'\subseteq\X$ be non-empty. Let $\Psafe$ be a distribution on $\X\times\Y$ with
\begin{equation}
\Psafe[U=u|V=v]=p_{u}
\end{equation}
for all $v\in\Y$ and $u\in\X'$, then the following are equivalent:
\begin{enumerate}
    \item For all $u\in\X'$ and $v\in\Y$ we have $\Psafe[U=u|V=v]=p_{u}$.
    \item $\Psafe$ is safe for $\GeneralGenInd|[V]$.
    \item $\Psafe$ is safe for $\langle\GeneralGenInd\rangle|\langle V\rangle$.
\end{enumerate}
\end{corollary}
\begin{proof}
The proof is in essence a repetition of the proof of Theorem~\ref{thm:DiscMainThm} where $\GeneralGenInd=\max_{u\in\X'}\GeneralInd$ must be used.
\end{proof}

\subsection{Accuracy}
Suppose you are in a situation where you need to guess $\GeneralGenInd$ for an $\X'\subset\X$. One example is the Monty Hall game where on live television you need to guess whether your door $a$ has the car behind, thus whether $\1_{\{U=a\}}=1$ holds given the opened door $V$. To probability of guessing correctly is called the \emph{accuracy}.

\begin{definition}[Accuracy]\label{def:DiscAccuracy}
Let $\mathcal{Z}$ be countable. Let $X$, $Y$ be $\mathcal{Z}$-valued random variables. Let $\Pmod$ be a set of probability measures on $\mathcal{Z}$ and let $\P'$ be an arbitrary probability measure on $\mathcal{Z}$ denoting the distribution of $X$. The \emph{accuracy of $X$ for guessing $Y$ distributed by $\P$} is defined by
\begin{equation}
\acc_Y^{\P}(X)=\sum_{k\in\mathcal{Z}}\P'[X=k]\P[Y=k].
\end{equation}
When $\acc_Y^{\P}(X)$ is constant for all $\P\in\Pmod$, this value is called the \emph{accuracy of $X$ for guessing $Y$} and is denoted by $\acc_Y(X)$. If from context it is clear we want to guess $Y$, we write \emph{the accuracy of $X$} and denote this by $\acc(X)$.
\end{definition}

The next theorem optimizes the accuracy of $\GeneralGenInd^*$ for guessing $\GeneralGenInd$ against all $\P\in\Pmod$.
\begin{theorem}\label{thm:DiscAccOpt}
Let $\X$, $\Y$, $U$, $V$, $\{p_u\}_{u\in\Y}$ and $\Pmod$ be as in Proposition~\ref{prop:DiscSafeMargGen} and let $\X'\subseteq\X$ be non-empty. Let $\GeneralGenInd^*$ be a random variable on $\{0,1\}$ with probability measure $\P'$ such that $\GeneralGenInd^*$ is distributed like
\begin{equation}\
\GeneralGenInd^*=\begin{cases}
1,&\sum_{u\in\X'}p_u>\frac{1}{2},\\
0,&\sum_{u\in\X'}p_u<\frac{1}{2},\\
\mathrm{Ber}(p),&\sum_{u\in\X'}p_u=\frac{1}{2},p\in[0,1]
\end{cases}
\end{equation}
where $\mathrm{Ber}(p)$ is the Bernoulli distribution with parameter $p\in[0,1]$. This distribution guesses $\GeneralGenInd$ correctly with probability
\begin{equation}
\acc^{\P}\left(\GeneralGenInd^*\right)=\max\left\{\sum_{u\in\X'}p_u,1-\sum_{u\in\X'}p_u\right\}.
\end{equation}

Any other binary random variable $W$ has a lower accuracy than $\GeneralGenInd^*$.
\end{theorem}
\begin{proof}
Let $\P\in\Pmod$ be arbitrary and let $W$ be an arbitrary $\{0,1\}$-valued random variable. Let $\P'$ be the probability measure of $W$. We will first prove that $\acc^{\P}(W)$ is constant for all $\P\in\Pmod$ and then prove that $W=\GeneralGenInd^*$ maximizes the accuracy of $W$.

By Definition~\ref{def:DiscAccuracy} and the fact that $\P\in\Pmod$ we have
\begin{align}
\acc_{\P}(W)&=\P'[W=0]\sum_{u\in\X\setminus\X'}\P[U=u]+\P'[W=1]\sum_{u\in\X'}\P[U=u]\\
&=\P'[W=0]\left(1-\sum_{u\in\X'}p_u\right)+\P'[W=1]\sum_{u\in\X'}p_u,
\end{align}
thus $\acc^{\P}(W)$ is constant for all $\P\in\Pmod$. Abbreviate the terms $\sum_{u\in\X'}p_u=x$ and $\P'[W=1]=p$. We now need to maximize
\begin{equation}
f(p):=px+(1-p)(1-x)=p(2x-1)+1-x.
\end{equation}
Note that both $x,p\in[0,1]$. Consider the following different values of $x$.
\begin{itemize}
    \item[$x>\frac{1}{2}$:] When $x>\frac{1}{2}$ holds, the function $f$ is strictly increasing in $p$. The maximum is then achieved at $p=1$, which is $p$. Thus always guessing $W=1$ maximizes its accuracy.
    \item[$x<\frac{1}{2}$:] When $x<\frac{1}{2}$ holds, the function $f$ is strictly decreasing in $p$. The maximum is then achieved at $p=0$, which is $1-x$. Thus always guessing $W=0$ maximizes its accuracy.
    \item[$x=\frac{1}{2}$:] When $x=\frac{1}{2}$ holds, the function $f$ is constant. Its value is $f(p)=\frac{1}{2}=x$ for all $p\in[0,1]$. Therefore if $W$ is made the Bernoulli distribution with parameter $p$, the accuracy of $W$ is maximized.
\end{itemize}
Therefore $W$ has maximal accuracy when $W=\GeneralGenInd^*$ and the accuracy of $\GeneralGenInd^*$ is
\begin{equation}
\acc\left(\GeneralGenInd^*\right)=\max\left\{\sum_{u\in\X'}p_u,1-\sum_{u\in\X'}p_u\right\}.
\end{equation}
\end{proof}

Suppose the player applies a safe distribution $\Psafe$ on his guess $\GeneralGenInd^*$ for $\GeneralGenInd$, what will the accuracy be? The following proposition answers this question.

\begin{proposition}\label{prop:DiscAccSafe}
Let $\X$, $\X'$, $\Y$, $U$, $V$, $\{p_u\}_{u\in\Y}$ and $\Pmod$ be as in Corollary~\ref{cor:DiscSafeGeneral}. Let $\Psafe$ be a safe distribution for $\GeneralGenInd|\langle V\rangle$. Let $\GeneralGenIndSafe$ be a $\{0,1\}$-valued random variable and employ distribution $\P'$ on $\GeneralGenIndSafe$ such that
\begin{equation}
\P'\left[\GeneralGenIndSafe=1\right]=\Psafe[\GeneralGenInd=1|V=v]=\sum_{u\in\X'}p_{u}
\end{equation}
holds for all $v\in\Y$. The accuracy of $\GeneralGenIndSafe$ is
\begin{equation}
\acc^{\P}\left(\GeneralGenIndSafe\right)=\left(\sum_{u\in\X'}p_{u}\right)^2+\left(1-\sum_{u\in\X'}p_{u}\right)^2
\end{equation}
for all $\P\in\Pmod$.
\end{proposition}
\begin{proof}
Let $\P\in\Pmod$ be arbitrary. By Corollary~\ref{cor:DiscSafeGeneral} and Definition~\ref{def:DiscAccuracy} we have
\begin{align}
\acc^{\P}\left(\GeneralGenIndSafe\right)&=\sum_{k=0}^1\P'\left[\GeneralGenIndSafe=k\right]\P[\GeneralGenInd=k]\\
&=\P[\GeneralGenInd=0]\left(1-\sum_{u\in\X'}p_u\right)+\P[\GeneralGenInd=1]\sum_{u\in\X'}p_u.
\end{align}
Since $\P[\GeneralGenInd=1]=\sum_{u\in\X'}p_u$ holds for all $\P\in\Pmod$, we can conclue that
\begin{equation}
\acc\left(\GeneralGenIndSafe\right)=\left(\sum_{u\in\X'}p_{u}\right)^2+\left(1-\sum_{u\in\X'}p_{u}\right)^2.
\end{equation}
\end{proof}

Now we know when a safe distribution $\Psafe$ for $\GeneralGenInd|[V]$ results into being the most accurate guess for $\GeneralGenInd$ as well.

\begin{corollary}
Let $\X$, $\X'$, $\Y$, $U$, $V$, $\{p_u\}_{u\in\Y}$ and $\Pmod$ be as in Corollary~\ref{cor:DiscSafeGeneral}. Let $\GeneralGenInd^*$ be as in Theorem~\ref{thm:DiscAccOpt}. Let $\Psafe$ be safe for $\GeneralGenInd|[V]$ and let $\GeneralGenIndSafe$ be as in Proposition~\ref{prop:DiscAccSafe}. The accuracy of $\GeneralGenIndSafe$ for guessing $\GeneralGenInd$ is only optimal when
\begin{equation}
\sum_{u\in\X'}p_u\in\left\{0,\frac{1}{2},1\right\}.
\end{equation}
\end{corollary}
\begin{proof}
Let $\P\in\Pmod$ be arbitrary. According to Proposition~\ref{prop:DiscAccSafe} the accuracy of $\GeneralGenIndSafe$ is
\begin{equation}
\acc\left(\GeneralGenIndSafe\right)=\left(\sum_{u\in\X'}p_{u}\right)^2+\left(1-\sum_{u\in\X'}p_{u}\right)^2.
\end{equation}
Theorem~\ref{thm:DiscAccOpt} states that the optimal accuracy for predicting $\GeneralGenInd$ is
\begin{equation}
\acc\left(\GeneralGenInd^*\right)=\max\left\{\sum_{u\in\X'}p_u,1-\sum_{u\in\X'}p_u\right\}.
\end{equation}

Note that $\GeneralGenIndSafe$ and $\GeneralGenInd^*$ are only equal when $\sum_{u\in\X'}p_u\in\{0,1\}$ holds or when $\sum_{u\in\X'}p_u=\frac{1}{2}$ and $\GeneralGenInd^*=\mathrm{Ber}\left(\frac{1}{2}\right)$ hold. The first case is quite trivial as when $\sum_{u\in\X'}p_u\in\{0,1\}$ we have $\acc\left(\GeneralGenIndSafe\right)=1$ as result. When $\sum_{u\in\X'}p_u=\frac{1}{2}$ holds, we have $\P[\GeneralGenIndSafe=1]=\frac{1}{2}$, resulting into $\GeneralGenIndSafe=\mathrm{Ber}\left(\frac{1}{2}\right)$. Since $\GeneralGenInd^*=\mathrm{Ber}\left(\frac{1}{2}\right)$ gives optimal accuracy for $\sum_{u\in\X'}p_u=\frac{1}{2}$ by Theorem~\ref{thm:DiscAccOpt}, the accuracy of $\GeneralGenIndSafe$ is in this case optimal with $\acc\left(\GeneralGenIndSafe\right)=\frac{1}{2}$.
\end{proof}

Knowing this, one needs to keep in mind that safe probability gives in almost no case a prediction for $\GeneralGenInd|V$ with optimal accuracy. Safe probability merely simulates the distribution of $\GeneralGenInd$, where in the setting of Theorem~\ref{thm:DiscMainThm} the revelation of the game master is in no cases of influence. A safe probability distribution must therefore in this setting not directly be used to make predictions for $\GeneralGenInd$, but it can be used to create a different random variable that does predict $\GeneralGenInd$ with optimal accuracy. In the Monty Hall game, you should not switch with probability $\frac{2}{3}$, as then the probability of winning the car is $\frac{5}{9}$. If the player observes that the safe distribution in the Monty Hall game puts a probability of $\frac{1}{3}$ door $a$, then never choosing door $a$ yields a probability of winning the car of $\frac{2}{3}$.

\section{Dice game}\label{sec:DiscDice}
Now the main theorems are stated, we can apply them to some well-known paradoxes. We will first continue our example of the dice game, started in Section~\ref{sec:SafeDice}. Recall that a die is cast with values in $\X=\{1,2,3,4,5,6\}$ and the game master is, after observing the die's value, only able to reveal a member of the set $\Y=\{\{1,2,3,4\},\{3,4,5,6\}\}=:\{y_1,y_2\}$. Our sample space is $\Omega=\X\times\Y$ with its power set $\Sigma=2^\Omega$ as $\sigma$-algebra. Let $U$ be an $\X$-valued random variable and $V$ be a $\Y$-valued random variable. The game master is not allowed to lie and the die is fair, thus any probability distribution for this game must be a member of \begin{equation}
\Pmod=\left\{\P\middle|\forall u\in\X:\P[U=u]=\frac{1}{6},\forall y\in\Y:\P[U\in y|Y=y]=1\right\}.
\end{equation}

\subsection{Safe probability}
Proposition~\ref{prop:SafeDice} gives a set $\tilde{\mathcal{P}}$ of distributions that are all safe for $\langle U\rangle|[V]$. However, in the original problem we are not interested in the whole distribution of $U$ given $V$, but only want to know the probability of rolling $3$ after the game master's statement. Therefore finding safe distributions for $\DieInd|[V]$ is sufficient.

\begin{proposition}\label{prop:DiscDiceSafe}
Let $\X$, $\Y$, $\Omega$, $U$, $V$ and $\Pmod$ be as in the dice game. Let $\Psafe$ be a distribution on $\Omega$. Let $u\in\X$, then the following statements are equivalent:
\begin{enumerate}
\item For all $v\in\Y$ we have $\P[U=u|V=v]=\frac{1}{6}$.
\item $\Psafe$ is safe for $\langle \1_{\{U=u\}}\rangle|\langle V\rangle$.
\item $\Psafe$ is safe for $\1_{\{U=u\}}|[V]$.
\end{enumerate}
\end{proposition}
\begin{proof}
This proposition is a direct application of Theorem~\ref{thm:DiscMainThm}. The set $\X$ is of size $6$ and therefore countable and the set $\Y$ is of size $2$, which makes it finite. $U$ is a random variable on $\X$ and $V$ is a random variable on $\Y$. In this dice game we have $p_u=\frac{1}{6}$ as $\P\in\Pmod$ implies $\P[U=u]=\frac{1}{6}$ for all $u\in\X$. Lastly, the set $\Pmod$ is a subset of $\{\P|\forall u\in\X:\P[U=u]=p_u\}$.

Take $\P_1,\P_2\in\Pmod$ with $\P_i[V=y_i|U=3]=\P_i[V=y_i|U=4]=1$. Let $i\in\{1,2\}$ be arbitrary, then
\begin{align}
\P_i[V=y_i]&=\sum_{u=1}^6\P_i[V=y_i|U=u]\P_i[U=u]\\
&=\frac{1}{6}\left(4\cdot1+2\cdot0\right)=\frac{2}{3}.
\end{align}
Thus the vectors $\left(\P_i[V=v]\right)_{v\in\Y}$ become $\left(\frac{1}{3},\frac{2}{3}\right)$ and $\left(\frac{2}{3},\frac{1}{3}\right)$, which are linearly independent.

Now we can directly apply Theorem~\ref{thm:DiscMainThm} to conclude the proof.
\end{proof}
\subsection{Accuracy}
Suppose you want to bet on this game and want to know whether you must put money on rolling $3$. As the following proposition will state, after observing the game master's statement, always stating that the die has not rolled to a $3$ has a $\frac{5}{6}$ chance of being correct independently on the game master's strategy.

\begin{proposition}
Let $\X$, $\Y$, $U$, $V$ and $\Pmod$ be as in the dice game. Consider the random variable $\DieInd^*=0$, then this $\DieInd^*$ has accuracy
\begin{equation}
\acc^{\P}\left(\DieInd^*\right)=\frac{5}{6}
\end{equation}
for all $\P\in\Pmod$. There is no random variable $W$ with $\acc^{\P}(W)>\acc^{\P}(\DieInd^*)$ for a $\P\in\Pmod$.
\end{proposition}
\begin{proof}
We want to apply Theorem~\ref{thm:DiscAccOpt}. Firstly in this case $\X'=\{3\}\subset\X$ is non-empty. Secondly, we have $\sum_{u\in\X'}p_u=p_3=\frac{1}{6}<\frac{1}{2}$. Then Theorem~\ref{thm:DiscAccOpt} states that $\DieInd^*=0$ optimizes the accuracy for guessing $\DieInd$ with
\begin{equation}
\acc\left(\DieInd^*\right)=\max\left\{\frac{1}{6},\frac{5}{6}\right\}=\frac{5}{6}.
\end{equation}
\end{proof}

We now observe that using the information given by the game master does not lead to a higher probability of guessing $\DieInd$ correct. When always stating that the die has not rolled $3$ you maximize the probability of having a correct guess, which is $5$ out of $6$. The information revealed by the game master can therefore be ignored as it will not yield to a higher probability of guessing correctly.

\subsection{Final remarks}
We have seen that the conditional probability of rolling $3$ given the game master's statement is not uniquely defined; it dilates from $0$ to $\frac{1}{4}$ depending on the to the player unknown strategy of the game master. However, when playing like the probability of rolling $3$ is always $\frac{1}{6}$ disregarding the game master's statement, you are able to simulate the probability distribution of $\DieInd$. Furthermore, when always stating that the die has not rolled $3$, you are correct in five out of six games, which is the maximal fraction of games you can be correct on.

The paradoxical nature of this problem is that most people treat this problem with sample space $\Omega=\{1,2,3,4,5,6\}$ and condition on for example the information $\{3,4,5,6\}$. However, as we have seen in Section~\ref{sec:SafeDice}, we also need to condition on the possibility $\{1,2,3,4\}$ and those two sets cannot form a partition of $\Omega$. Therefore the sample space $\Omega$ is insufficient for solving this problem, resulting in the paradoxical statements.

This underlines the argument that when conditioning the accompanying sub-$\sigma$-algebra must always be given, as then it is easily seen that conditioning on a sub-$\sigma$-algebra containing $\sigma(\{\{1,2,3,4\},\{3,4,5,6\}\})\subset2^\Omega$ is not possible.

\section{Monty Hall}\label{sec:DiscMonty}
One of the most classic paradoxes in probability theory is Monty Hall's three door problem. The problem is already explained in the introduction of this chapter, but we will repeat it here.\footnote{Geschiedenis en bronnenonderzoek toevoegen.}

There are three doors called $a$, $b$ and $c$.  Two doors have a goat behind and one door has a car. The player chooses one door, where after the game master opens one of the remaining doors. However, the game master does not open the door with the car. The player is then asked whether he wants to switch to the remaining door. The player wins the contents of the door he ultimately chooses.

A logical question, and the centre of this paradox, is whether the player should switch after the revealing of a door. There are multiple viewpoints for this question. Throughout the whole analysis we assume that initially the car is placed behind a door with equal probability. Without loss of generality we now assume the player has chosen door $a$, as the choice of the first door has become independent of the question whether the player should switch.

The most naive viewpoint is the following. Say door $c$ is opened by the game master, then either door $a$ or door $b$ has a car with equal probability $\frac{1}{2}$. Since there is a 50-50 chance, switching does not increase your chances. This viewpoint is widely disputed.

Another viewpoint using classic conditional probability states the following. In the beginning there is a $\frac{1}{3}$-chance that door $a$ has a car. After choosing door $a$, suppose door $c$ is revealed. There was a chance of $\frac{2}{3}$ that the car was behind door $b$ or $c$, leaving a $\frac{2}{3}$-chance that the car is behind door $b$. Therefore switching is advised, doubling your chances of winning the car. However, this viewpoint is disputed as well.

Grünwald and Halpern \cite{Grunwald03} pointed out that the probability of the car being behind door $a$ is anything between $0$ and $\frac{1}{2}$. When the car is behind door $a$, the game master is free to choose between doors $b$ and $c$. The game master's strategy influences the probability of the car being behind the other door.

Let us start with a formal statement of the problem. Let $\X=\{a,b,c\}$ be our outcome space and $\Y=\{b,c\}$ be our observation space. Let $\Omega=\X\times\Y$ be our sample space with $\Sigma=2^\Omega$ as $\sigma$-algebra. Let $U$ be an $\X$-valued random variable denoting the placement of the car and let $V$ be a $\Y$-valued random variable on $\Y$ denoting the opening of a door.\\
Note that the doors are called $a$, $b$ and $c$ and not $1$, $2$ and $3$, as in the latter case the expectation $\E[U]$ suddenly can be given a meaning. There is no such thing as the `average door', as the labels on the doors are nothing more than names, not aspects indicating any value. Therefore we choose to put $\X=\{a,b,c\}$ to not give rise to any confusion.\\
We assume that the car is initially distributed with equal probability and the game master cannot open a door with a car. A probability distribution must therefore be a member of the set
\begin{equation}
\Pmod=\left\{\P\middle|\forall u\in\X:\P[U=u]=\frac{1}{3},\forall v\in\Y:\P[V=v|U=v]=0\right\}.
\end{equation}

In this section we first give a repetition the arguments of \cite{Grunwald13} as they view the Monty Hall problem in the viewpoint of traditional probability. Then we apply safe probability and Theorem~\ref{thm:DiscMainThm} to the Monty Hall problem, with as result that it is safe to state that the car is behind door $a$ with probability~$\frac{1}{3}$.


\subsection{Traditional conditional probability}
Let us start with the viewpoint that result in a probability of $\frac{2}{3}$ of obtaining the car after switching. This viewpoint takes $\Omega'=\{a,b,c\}$ as sample space. Let $U'$ be an $\Omega'$-valued random variable denoting the cars location and $V'$ be a $\{b,c\}$-valued random variable denoting the game master's statement. Equip $\Omega'$ with the uniform distribution $\P$ such that $\P[U'=u]=\frac{1}{3}$ and $\P[V'=y|U'=y]=0$ with $y\in\{b,c\}$. Assume lastly that $\P[V'=b|U'=a]=\frac{1}{2}$, thus that the game master chooses a door with equal probability when he has free choice. Traditional conditional probability then gives
\begin{equation}
\P[U'=a|V'=b]=\frac{\P[V'=b|U'=a]\P[U'=a]}{\sum_{u\in\Omega}\P[V'=b|U'=u]\P[U'=u]}=\frac{\frac{1}{2}\cdot\frac{1}{3}}{\frac{1}{3}\left(\frac{1}{2}+0+1\right)}=\frac{1}{3}.
\end{equation}
It is clear that the argument above needs to make assumptions on $\P$ in order to be able to perform the calculation, as following the problem's statement we cannot put a direct value on $\P[V'=b,U'=a]$. We thus need to make a reasonable assumption on $\P[V'=b|U'=a]$ for this argument.\\
Let now $p\in[0,1]$ be such that $\P[V'=b|U'=a]=p$, then we have
\begin{equation}\label{eq:DiscMonty}
\P[U'=a|V'=b]=\frac{\P[V'=b|U'=a]\P[U'=a]}{\sum_{u\in\Omega}\P[V'=b|U'=u]\P[U'=u]}=\frac{p}{p+1}.
\end{equation}
Thus the probability of door $a$ having the car after opening door $b$ dilates from $0$ to $\frac{1}{2}$, as \cite{Grunwald13} also pointed out in their work.\\
To create a probability of $\frac{1}{2}$ of our initial door having the car, we need to pick $p=1$. We thus need to assume that when we initially chose the correct door, the game master \emph{always} opens door $b$. This also debunks the argument that there are two doors left, thus there is a probability of $\frac{1}{2}$ that door $a$ has the car. The information that door $b$ has no car heavily influences the conditional probability.


However, more is wrong with the above reasoning. Suppose door $a$ has the car. The game master is now able to choose between doors $b$ and $c$. If door $b$ is opened, the resulting set of possibilities is $\{a,b\}$ and when door $c$ is opened, the resulting set of possibilities is $\{a,c\}$. We need to condition on both the possibilities $\{a,b\}$ and $\{a,c\}$, however both sets do not form a partition. Furthermore, the smallest $\sigma$-algebra containing both sets is
\begin{equation}
\G=\sigma(\{a,b\},\{a,c\})=2^{\{a,b,c\}}=2^{\Omega'},
\end{equation}
our original $\sigma$-algebra. As stated by \cite{Williams91} and also easy to prove, we have $\P[F|\G]=\P[F]$ for all $F\in\G$ when $\G=\F$ is our original $\sigma$-algebra. We are therefore simply not able to use measure-theoretic conditional probability, let alone traditional conditional probability.

Can we say nothing sensible at all about the probability of winning the car after switching? Consider the $\Omega$, $U$ and $V$ from the formal statement in the introduction of this section. Now we have $\Omega=\X\times\Y$, thus $\Omega$ is split up in outcome space $\X=\{a,b,c\}$ and observation space $\Y=\{b,c\}$. Now we can condition on all possibilities of open doors, as $\sigma(\{a\}\times\Y)$ is a strict sub-$\sigma$-algebra of $2^\Omega$.\\
The resulting conditional probabilities come in the form of Equation~\ref{eq:DiscMonty}. Let $\P\in\Pmod$, thus such that $\P$ is uniform distributed on $U$ and a door with a car cannot be opened. Then a $p\in[0,1]$ exists such that $\P[V=b|U=a]=p$. This results into
\begin{align}
\P[U=a|V=b]=\frac{\P[V=b|U=a]\P[U=a]}{\sum_{u\in\Omega}\P[V=b|U=u]\P[U=u]}=\frac{p}{p+1},\\
\P[U=a|V=c]=\frac{\P[V=c|U=a]\P[U=a]}{\sum_{u\in\Omega}\P[V=c|U=u]\P[U=u]}=\frac{1-p}{2-p}.
\end{align}
In both cases the probability of door $a$ having the car dilates from $0$ to $\frac{1}{2}$. Therefore, you are advised to switch no matter the strategy of the game master. However, we cannot pin down a single probability of door $a$ having the car given the game master's statement. We can put a prior on $\P[V=b|U=a]$, however we do need more information to apply such prior wisely.

This subsection shows another example of why when doing conditional probability one always needs to provide the accompanying sub-$\sigma$-algebra. Unlike the Borel-Kolmogorov paradox in Chapter~\ref{chap:BorelKolmogorov}, here the measure-theoretic conditional expectation defines a unique probability measure. However, most use $\Omega=\{a,b,c\}$ as sample space and without taking the accompanying $\sigma$-algebra in account when conditioning, one should find no reason not to do this. Only when correctly applying conditional probability and noticing that both $\{a,b\}$ and $\{a,c\}$ are possible options, one finds out that $\Omega=\{a,b,c\}$ is insufficient as sample space and paradoxical results will rise.

\subsection{Safe probability}
Using safe probability it is possible to find a pragmatic distribution $\Psafe$ on $\Omega$ that is safe against all $\P\in\Pmod$. We only need to apply Theorem~\ref{thm:DiscMainThm}.

\begin{proposition}\label{prop:DiscMontySafe}
Let $\X$, $\Y$, $\Omega$, $U$, $V$ and $\Pmod$ be as in the Monty Hall problem. Let $\Psafe$ be a distribution on $\Omega$. Let $u\in\X$, then the following statements are equivalent:
\begin{enumerate}
\item For all $v\in\Y$ we have $\P[U=u|V=v]=\frac{1}{3}$.
\item $\Psafe$ is safe for $\langle \1_{\{U=u\}}\rangle|\langle V\rangle$.
\item $\Psafe$ is safe for $\1_{\{U=u\}}|[V]$.
\end{enumerate}
\end{proposition}
\begin{proof}
This proposition is a direct application of Theorem~\ref{thm:DiscMainThm} and its proof is equal to the proof of Proposition~\ref{prop:DiscDiceSafe}. The set $\X$ is of size $3$ and therefore countable and the set $\Y$ is of size $2$, which makes it finite. $U$ is a random variable on $\X$ and $V$ is a random variable on $\Y$. In this Monty Hall problem we have $p_u=\frac{1}{3}$ as $\P\in\Pmod$ implies $\P[U=u]=\frac{1}{3}$ for all $u\in\X$. Lastly, the set $\Pmod$ is a subset of $\{\P|\forall u\in\X:\P[U=u]=p_u\}$.

Take $\P_b,\P_c\in\Pmod$ with $\P_i[V=i|U=a]=1$. Let $i\in\{b,c\}$ be arbitrary, then
\begin{align}
\P_i[V=i]&=\sum_{u=1}^3\P_i[V=i|U=u]\P_i[U=u]\\
&=\frac{1}{3}\left(2\cdot1+0\right)=\frac{2}{3}.
\end{align}
Thus the vectors $\left(\P_i[V=v]\right)_{v\in\Y}$ become $\left(\frac{1}{3},\frac{2}{3}\right)$ and $\left(\frac{2}{3},\frac{1}{3}\right)$, which are linearly independent.

Now we can directly apply Theorem~\ref{thm:DiscMainThm} to conclude the proof.
\end{proof}

Thus the distribution stating that door $a$ has probability $\frac{1}{3}$ of having the car is safe for $\MontyInd|[V]$, thus is marginally valid. In this case safety for $\MontyInd|[V]$ implies a few other results using \emph{pivotal safety}. Before we can introduce pivotal safety, we first need to introduce the \emph{pivot}. This definition is taken from \cite{Grunwald18}.

\begin{definition}[Discrete pivot]\label{def:DiscPivot}
Let $U$ and $V$ be random variables on $\Omega$. Assume $\Omega$ is countable. A random variable $U'$ is a \emph{discrete pivot} for $U|V$ if
\begin{enumerate}
    \item $(U,V)\rightsquigarrow U'$ holds, thus there is a function $f$ with $U'=f(U,V)$.
    \item For each fixed $v\in\range(V)$ the function
    \begin{equation}
    f_v\colon\range(U|V=v)\to\range(U'),\qquad u\mapsto f(u,v)
    \end{equation}
    is injective.
    \item All $\P\in\Pmod$ agree on $U'$, thus for all $\P_1,\P_2\in\Pmod$ we have $\P_1(U')=\P_2(U')$.
\end{enumerate}
Pivot $U'$ is \emph{simple} if $f_v$ is a bijection for all $v\in\range(V)$.
\end{definition}

Since we will not consider continuous pivots in this thesis, we just state that $U'$ is a \emph{pivot} for $U|V$. Now we can define pivotal safety, where the definition is taken from \cite{Grunwald18}.

\begin{definition}[Pivotal safety]\label{def:DiscPivotSafe}
Let $U$ and $V$ be as in Definition~\ref{def:DiscPivot} and let $\Psafe$ be an arbitrary distribution on $\Omega$. If $V$ has full support under $\Psafe$, thus $\supp_{\Psafe}(V)=\range(V)$, and $U'$ is a discrete pivot such that $\Psafe$ is safe for $U'|[V]$, then $\Psafe$ is \emph{pivotally safe for $U|V$ with pivot $U'$}.
\end{definition}

Now we can prove that $\MontyInd$ is a pivot for $U|V$ and immediately that $\Psafe$ from Proposition~\ref{prop:DiscMontySafe}.

\begin{proposition}\label{prop:DiscMontyPivSafe}
Let $\X$, $\Y$, $\Omega$, $U$, $V$, $\Pmod$ and $\Psafe$ be as in Theorem~\ref{prop:DiscMontySafe}. The random variable $\MontyInd$ is a pivot for $U|V$ and $\Psafe$ is pivotally safe for $U|V$ with pivot $\MontyInd$.
\end{proposition}
\begin{proof}
The proof is a mere check of Definitions~\ref{def:DiscPivot} and \ref{def:DiscPivotSafe}.

We will start with $\MontyInd$ being a pivot for $U|V$ by checking the three requirements of Definition~\ref{def:DiscPivot}.
\begin{enumerate}
    \item Take as function $f(U,V)=\MontyInd$. This is a well-defined function, therefore $(U,V)\rightsquigarrow U'$.
    \item Note that $\range(V)=\{b,c\}$. Take $v=b$, then $\range(U|V=b)=\{a,c\}$. We have $f_b(a)=f(a,b)=1$ and $f_b(c)=f(c,b)=0$, thus $f_b$ is a bijection. Taking $v=c$ we get $f_c(a)=1$ and $f_c(b)=0$, thus $f_c$ is a bijection as well.
    \item Take $\P_1,\P_2\in\Pmod$ arbitrarily. Note that $\range(\MontyInd)=\{0,1\}$. Therefore
    \begin{align}
        \P_1[U\neq a]&=\P_1[U\in\{2,3\}]=\frac{2}{3}=\P_2[U\in\{2,3\}]=\P_2[U\neq a],\\
        \P_1[U=a]&=\P_1[U=1]=\frac{1}{3}=\P_2[U=1]=\P_2[U=a],
    \end{align}
    thus we have $\P_1[\MontyInd]=\P_2[\MontyInd]$.
\end{enumerate}
Since we have proven that $f_v$ is a bijection for all $v\in \range(V)$, the pivot $\MontyInd$ is a simple pivot for $U|V$.

Theorem~\ref{thm:DiscMainThm} states that $\Psafe$ with $\Psafe[U=a|V=v]=\frac{1}{3}$ for all $v\in\Y$ is safe for $\MontyInd|[V]$. This $\Psafe$ has full support under $V$, as for all $v\in\Y$ the statement $\Psafe[U=a|V=v]>0$ implies $\Psafe[V=v]>0$ by definition of traditional conditional probability. Therefore by Definition~\ref{def:DiscPivotSafe} probability measure $\Psafe$ is pivotally safe for $U|V$ with pivot $\MontyInd$.
\end{proof}

Pivotal safety comes with a nice result. Following \cite{Grunwald18}, introduce the random variable $\tilde{p}(U|V)$ by
\begin{equation}
\tilde{p}(U|V)(\omega)=\Psafe[U=U(\omega)|V=V(\omega)]
\end{equation}
for all $\omega\in\Omega$. This random variable maps $\omega\in\Omega$ to the probability of a single outcome $\Psafe[U=U(\omega)|V=V(\omega)]$. Now \cite{Grunwald18} stated the following theorem on pivotal safety.
\begin{theorem}\label{thm:PivotSafe}
Let $\Omega$ be countable and $U,V$ be random variables on $\Omega$. Let $\Psafe$ be an arbitrary distribution on $\Omega$. Suppose that for all $v\in\range(V)$ there are no two $u_1,u_2\in\range(U|V=v)$ with $\Psafe[U=u_1|V=v]=\Psafe[U=u_2|V=v]$. The following statements are equivalent:
\begin{enumerate}
    \item $\Psafe$ is pivotally safe for $\tilde{p}(U|V)|[V]$.
    \item $\Psafe$ is pivotally safe for $U|V$ with simple pivot $U'=\tilde{p}(U|V)|[V]$.
    \item $\Psafe$ is pivotally safe for $U|V$ for some simple pivot $U''$.
\end{enumerate}
\end{theorem}
\begin{proof}
The proof can be found in \cite{Grunwald16}.
\end{proof}

Let in our case $\Psafe$ be as in Proposition~\ref{prop:DiscMontyPivSafe}. We only defined $\Psafe[U=a|V=v]$ for all $v\in\Y$, not $\Psafe[U=u|V=v]$ with $(u,v)\in\{b,c\}\times\Y$. Thus to actually write down the outcomes of $\tilde{p}(U|V)$ with their respective probabilities, we need to put $p_u^v\in\left[0,\frac{2}{3}\right]\setminus\left\{\frac{1}{3}\right\}$ with $p_b^v+p_c^v=\frac{2}{3}$ for all $(u,v)\in\{b,c\}\times\Y$ and apply
\begin{equation}
\Psafe[U=u|V=v]=p_u^v.
\end{equation}
Now $\Psafe[U=a|V=v]$ is fixed to be $\frac{1}{3}$. No $u\in\{b,c\}$ has $\Psafe[U=u|V=v]=\frac{1}{3}$ as well, since we excluded $\frac{1}{3}$ from the set $p_u^v$ could be picked from. Since $\Psafe$ is pivotally safe for $U|V$ with pivot $\MontyInd$, Theorem~\ref{thm:PivotSafe} states that $\Psafe$ is pivotally safe for $U|V$ with simple pivot $\tilde{p}(U|V)$ as well.

Summarizing, since $\Psafe$ is pivotally safe for $\MontyInd$ with simple pivot $\MontyInd$ and since we can create $\tilde{p}(U|V)$ to be a simple pivot for $U|V$ as well, we immediately obtain pivotal safety for $U|V$ with simple pivot $\tilde{p}(U|V)$. We also obtain an infinite family of simple pivots $\tilde{p}(U|V)$, as for example $\Psafe[U=b|V=c]$ is not uniquely defined, for which $\Psafe$ is safe for $\tilde{p}(U|V)$. We have seen that it is safe to assume that the car is behind door $a$ with a probability of $\frac{1}{3}$ and by some special properties of the Monty Hall game, specifically that $\MontyInd$ is a simple pivot for $U|V$, we see that $\Psafe$ is safe for $U'|[V]$ with $U'=\tilde{p}(U|V)$ being another simple pivot constructed by $\Psafe$.


\subsection{Accuracy}
Suppose you are playing the Monty Hall game, which strategy optimizes winning the car? The answer is that you must always switch for a probability of $2$ out of $3$ to win the car, as can be found by applying Theorem~\ref{thm:DiscAccOpt}.

\begin{proposition}
Let $\X$, $\Y$, $U$, $V$ and $\Pmod$ be as in the dice game. Consider the random variable $\MontyInd^*=0$, then this $\MontyInd^*$ has accuracy
\begin{equation}
\acc^{\P}\left(\MontyInd^*\right)=\frac{2}{3}
\end{equation}
for all $\P\in\Pmod$. There is no random variable $W$ with $\acc^{\P}(W)>\acc^{\P}(\MontyInd^*)$ for a $\P\in\Pmod$.
\end{proposition}
\begin{proof}
We want to apply Theorem~\ref{thm:DiscAccOpt}. Firstly in this case $\X'=\{a\}\subset\X$ is non-empty. Secondly, we have $\sum_{u\in\X'}p_u=p_a=\frac{1}{3}<\frac{1}{2}$. Then Theorem~\ref{thm:DiscAccOpt} states that $\MontyInd^*=0$ optimizes the accuracy for guessing $\MontyInd$ with
\begin{equation}
\acc\left(\MontyInd^*\right)=\max\left\{\frac{1}{3},\frac{2}{3}\right\}=\frac{2}{3}.
\end{equation}
\end{proof}

As in the dice game, the information of the game master does not lead to a higher probability of winning the car. When always switching doors, the probability of winning the car becomes $\frac{2}{3}$ and this is the optimal probability you can have.

\subsection{Final remarks}
The Monty Hall problem applied Theorems~\ref{thm:DiscMainThm} and \ref{thm:DiscAccOpt} in the same manner as the dice game. In the setting of Monty Hall, it is safe to say that the car has probability $\frac{1}{3}$ to be behind the originally chosen door. Therefore, always switching yields to an optimal probability of $\frac{2}{3}$ for winning the car.

Since in the case of Monty Hall the random variable $\MontyInd$ is a simple pivot for $U|V$, we can pick $\Psafe$ to construct a family of other simple pivots $U'$ that are safe for $U'|[V]$. This is not possible in the dice game, as there $f_v$ as in Definition~\ref{def:DiscPivot} with $v\in\{\{1,2,3,4\},\{3,4,5,6\}\}$ is not injective.

When pivots are not considered, the Monty Hall problem is essentially equal to the dice game. Here the paradoxical nature of the Monty Hall problem stems from the fact that most use $\Omega=\{a,b,c\}$ when first trying to solve the problem. However, in that case we cannot create a partition to apply traditional conditional probability or a $\sigma$-algebra for measure-theoretic conditional probability. This problem can only be spotted when performing conditional probability correctly, thus with stating the accompanying sub-$\sigma$-algebra. Then it is clear that $\Omega$ must be extended with an observation space $\Y=\{b,c\}$.

Therefore the Monty Hall problem is yet another argument as in why the $\sigma$-algebra must be given when performing conditional probability.

\section{Boy or girl problem}\label{sec:DiscChildren}
The last problem we will observe in this chapter is the boy or girl problem.

\subsection{Traditional conditional probability}

\subsection{Safe probability}

\subsection{Accuracy}

\subsection{Final remarks}

\section{Conclusion}\label{sec:DiscConcl}

\chapter{The two envelopes problem}\label{chap:TwoEnvelope}

\bibliographystyle{alpha}
\bibliography{../Referenties/Referenties}

\appendix
\chapter{Corrections to [GHSR17]}
\section{Corrections to appendix 1}\label{app:CorLong}
\pagenumbering{Alph}
In appendix 1, equation 94 of the proof of the conditional expectation on latitudes in \cite{Gyenis17}, they claim that the volume of the unit sphere is $2\pi$, whereas they should have claimed that the surface area of the unit sphere is $4\pi$. Luckily this constant is not actually used in the computations, thus their mistake has no impact on the rest of their article.

\section{Corrections to appendix 2}\label{app:CorMer}
I suggest the following corrections on appendix 2, the proof of conditional expectation on meridians:
\begin{enumerate}
\item In the verification \cite{Gyenis17} claims that $\int_0^\pi\sin\theta d\theta=1$ holds between equations (107) and (108) and between equations (115) and (116), while that integral actually has value $2$.
\item The conditional distribution of \cite{Gyenis17} is verified on a half meridian arc on page 2614, while this distribution must be verified on a full circle in order to be compared with the latitudes. Verification on a full meridian, e.g.~computation of $q(C)$ in formula (85), quickly reveals that the conditional distribution of \cite{Gyenis17} integrates to $2$.
\item The random variable $X$ is integrated on the domain $[0,2\pi)$, while $X$ is only defined on $[0,2\pi)\times[0,\pi]\in\mathfrak{M}$. The set $A\times[0,2\pi)$ is not an element of $\mathfrak{M}$.
\item Between equations (105) and (106) of \cite{Gyenis17}, they implicitly claim that the identity $X(\phi,\theta)=X(\phi,\theta+\pi)$ holds for all $\F$-measurable $X$, where now notation of \cite{Gyenis17} is used. This is most certainly false without any more restrictions on $X$.
\item Before equation (85) a measure $q_\mathcal{M}$ is defined on a whole meridian. Since the integral is taken from $\psi=0$ to $\psi=2\pi$, the integral of $q_\mathcal{M}$ over $S$ becomes $1$. However, as pointed out earlier, $\psi>\pi$ is not in our domain. Thus the integral must be split up in two arcs with $q_{\mathcal{M}}$ taking value $\frac{1}{2}$ on each arc.
\item The normalization constant is used in equation (103) of \cite{Gyenis17} is $2\pi$, where it must be $4\pi$. This does however not impact further calculations, like the same mistake made from equation (109) to (110).
\end{enumerate}

\newpage

\printindex

\end{document}