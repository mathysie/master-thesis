%!TEX program = xelatex
\documentclass[a4paper]{report}
\usepackage[ngerman, english]{babel}
\usepackage{comment}
\usepackage{csquotes}
\usepackage{amsmath,amsfonts,amssymb,amsthm,bbm,mathrsfs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{showidx}
\usepackage{pgf,tikz,pgfplots}
\usetikzlibrary{arrows, positioning}
\pgfplotsset{compat=1.16}
\makeindex

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem*{theoremmain*}{Theorem \ref{thm:DiscMainThm}}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{property}[theorem]{Property}
\newtheorem{properties}[theorem]{Properties}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}

\numberwithin{equation}{chapter}

\hypersetup{%
  colorlinks = true
}

%Afkortingen voor wiskundige symbolen
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\C}{\mathbb{C}}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\1}{\mathbbm{1}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\G}{\mathcal{G}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\range}{range}

\newcommand{\Pmod}{\mathcal{P}^*}
\newcommand{\Psafe}{\tilde{\P}}
\newcommand{\Uonesafe}{\tilde{\1}_U}
\newcommand{\Usafe}{\tilde{U}}

\newcommand{\EnvIndSafe}{\1_{\{B=2A\}}}
\newcommand{\DieInd}{\1_{\{U=3\}}}
\newcommand{\DieIndSafe}{\tilde{\1}_{\{Y=3\}}}
\newcommand{\ChildInd}{\1_{\{U=bg\}}}
\newcommand{\ChildIndSafe}{\tilde{\1}_{\{U=bg\}}}
\newcommand{\ChildTwoInd}{\1_{\{U=bb\}}}
\newcommand{\ChildTwoIndSafe}{\tilde{\1}_{\{U=bb\}}}
\newcommand{\GeneralInd}{\1_{\{U=u'\}}}
\newcommand{\GeneralGenInd}{\1_{\{U\in\X'\}}}
\newcommand{\GeneralGenIndSafe}{\tilde{\1}_{\{U\in\X'\}}}

\title{Thesis}
\author{Mathijs Kolkhuis Tanke}
\date{\today}

\begin{document}
\begin{titlepage}
\maketitle
\end{titlepage}

\pagenumbering{roman}

\begin{abstract}
This is my thesis.
\newpage
This is a long abstract.
\end{abstract}


\pagenumbering{gobble}
\tableofcontents

\chapter{Introduction}
\pagenumbering{arabic}
Probability theory is one of the most important and most researched fields in mathematics. It is entirely based on three axioms first stated by Andrey Kolmogorov \cite{Kolmogorov33}, namely that the probability of an event is non-negative, the probability measure is a unit measure and the probability measure is countably additive on disjoint sets. These three axioms however do not prevent the existence of certain paradoxes, like the Borel-Kolmogorov paradox, Monty Hall's problem and the Sleeping Beauty problem. Some paradoxes arise from wrongly using probability theory, others are a misinterpretation of results.

This thesis focuses mainly paradoxes arising from conditional probability, such as the Borel-Kolmogorov paradox, Monty Hall's problem and the two envelope problem. We study will these problems and address their paradoxical nature. Ultimately it will be shown that all paradoxes arise by incorrectly applying conditional probability.

Take a look at the Borel-Kolmogorov paradox first. A sphere is equipped with the uniform probability measure. Suppose a point exists on a great circle of the sphere, but you do not know where that point is. Is there a probability distribution on this great circle for that point? If one models this problem using longitudes, the conditional distribution on the great circle is uniform. However, if one models this problem using meridians, the conditional distribution is a cosine.

Borel \cite{Borel09} and Kolmogorov \cite{Kolmogorov33} both addressed this problem and Kolmogorov gave the following answer:
\foreignblockquote{ngerman}[Andrey Kolmogoroff, \cite{Kolmogorov33}]{Dieser Umstand zeigt, daß der Begriff der bedingten Wahrscheinlichkeit in bezug auf eine isoliert gegebene Hypothese, deren Wahrscheinlichkeit gleich Null ist, unzulässig ist: nur dann erhält man auf einem Meridiankreis eine Wahrscheinlichkeitsverteilung für [Breite] Theta, wenn dieser Meridiankreis als Element der Zerlegung der ganzen Kugelfläche in Meridiankreise mit den gegebenen Polen betrachtet wird.}
In summary, Kolmogorov states that the concept of conditional probability on a great circle is inadmissible, since the event that the point lies on a great circle of the sphere has zero measure. Furthermore, the sole reason of a cosine distribution on a great circle arising when considering meridians is that the meridian circle serves as an element of the decomposition of the whole spherical surface in meridian circles with the poles considered.

Despite Kolmogorov's explanation this problem is still upon debate. Recently Gyenis, Hofer-Szabó and Rédei \cite{Gyenis17} studied this problem and provided more insight and an in my opinion satisfying discussion to the problem, eventually drawing the same conclusion as Kolmogorov already did. This problem is more elaborately discussed in chapter~\ref{chap:BorelKolmogorov} where we will expand the analysis of \cite{Gyenis17} and not only consider longitudes and meridians, but their combination as well. Furthermore, we will look whether a conditional probability on a zero set can be uniquely approached by traditional conditional probability on sets of decreasing measure as stated in conjecture~\ref{con:BorelConjecture}. This conjecture turns out to be false, again affirming that conditional probabilities on sets of measure zero are not uniquely defined.

Another paradox arising from conditional probability is Monty Hall's problem. In Monty Hall's problem, a player is facing three doors called $a$, $b$ and $c$. One door opens to a car and the other two have goats. Suppose the player initially chooses door $a$. The game master then opens either door $b$ or $c$, but always a door with a goat. The player now faces two doors and is asked whether he wants to switch. One possible solution is that the player faces two doors without any preference for either door, thus the probability is 50\%. Another possible solution is that first the player had a 33\% chance of correctly guessing the door with the car. If for example door $c$ is opened, door $b$ remains with a conditional probability of 67\% of having the car. Which solution is correct?

Let $\X=\{a,b,c\}$ be the set of doors and $U$ the random variable denoting the location of the car. Conditional probability then states
\begin{equation*}\label{eq:IntMonty}
\P[U=b\mid U\in\{a,b\}]=\frac{\P[U=b,U\in\{a,b\}]}{\P[U\in\{a,b\}]}=\frac{\frac{1}{3}}{\frac{2}{3}}=\frac{1}{2},
\end{equation*}
supporting the claim that a probability of 50\% is the correct answer. However, the space conditioned on door $c$ is opened is $\{a,b\}$ and the space conditioned on door $b$ is opened is $\{a,c\}$. If door $a$ has the car, the game master can open either door and the set of events we must condition on is $\{\{a,b\},\{a,c\}\}$. These two events do not form a partition, preventing us from using traditional conditional probability.

Thus in addition to Kolmogorov's statement, we must not only be wary for conditioning on events with measure 0, we cannot condition on arbitrary events at all. I propose that when using conditional probability, one must provide a pair of a sub-$\sigma$-algebra and the event from that $\sigma$-algebra to condition on. Regarding Monty Hall's problem there is no $\sigma$-algebra on $\{a,b,c\}$ containing the set $\{\{a,b\},\{a,c\}\}$. In the case of the Borel-Kolmogorov paradox providing sub-$\sigma$-algebras immediately make clear why the conditional distribution on a great circle is not unique, as both calculations are supported by different sub-$\sigma$-algebras.

The crux of the Monty Hall problem is that the initial distribution of the car is unknown and the player does not know with which probability door $b$ is opened given the car is behind door $a$. Thus there is a set of different probability distributions where one is the correct distribution, but we do not know which one is. Using the theory of safe probability we can obtain a strategy that give equal results for all distributions in a specified model. This theory is introduced by Grünwald \cite{Grunwald18} and here in chapter~\ref{chap:SafeProp}. Safe probability is then applied in chapter~\ref{chap:DiscPara} to problems as the Monty Hall problem and to the two envelope problem in chapter~\ref{chap:TwoEnvelope}.

In chapter~\ref{chap:DiscPara} the results of the analysis of Monty Hall's problem are generalized to a theorem, which can be used to provide safe distributions for other problems like the boy or girl problem.
\begin{theoremmain*}
Let $\X$ and $\Y$ be countable. Let $U$ be a random variable on $\X$ and $V$ be a random variable on $\Y$. Let $p_u\in[0,1]$ with $\sum_{u\in\X}p_u=1$ and let 
\begin{equation*}
\Pmod\subseteq\{\P\mid\forall u\in\X:\P[U=u]=p_u,\E_{\P}[U]<\infty\}
\end{equation*}
be our model of probability distributions on $\X\times\Y$. Suppose there is a
\begin{equation*}
u'\in\bigcap_{\P\in\Pmod}\bigcap_{v\in\supp_{\P}(V)}\range(U|V=v).
\end{equation*}
Let $\Psafe$ be a distribution on $\X\times\Y$. If $\Y=\bigcup_{\P\in\Pmod}\supp_{\P}(V)$, then the following are equivalent:
\begin{enumerate}
    \item For all $v\in\Y$ we have $\Psafe[U=u'|V=v]=p_{u'}$.
    \item $\Psafe$ is safe for $\GeneralInd|[V]$.
    \item $\Psafe$ is safe for $\langle\GeneralInd\rangle|\langle V\rangle$.
\end{enumerate}
\end{theoremmain*}

This theorem and its notation will be explained, proven and applied in chapter~\ref{chap:DiscPara}, but it essentially states that in the case of the Monty Hall problem if one assumes the car is initially distributed evenly between the doors, the probability of the car being behind the originally chosen door $a$ can be assumed to be $\frac{1}{3}$. Using this assumption one must always switch to the other door as the probability of the car being behind that door can be assumed to be $\frac{2}{3}$, resulting in a 67\% chance of winning the car. Note that this assumption is with probability 0 the correct distribution. However, this assumption always yields to a 67\% chance of winning the car independent on the probability of opening door $b$ when the car is behind door $a$.

Another paradox we will treat is the two envelope problem. There are two envelopes, where one is filled with value $a$ and the other with value $b$. The only thing the player knows is that either $a=2b$ or $b=2a$. An envelope is given to the player, with the player receiving either envelope with equal probability. Call this envelope $A$ and suppose the player observes value $a$. Should he switch to envelope $B$ or keep the contents of $A$?

At first glance, he should. Either $b=2a$ or $b=\frac{a}{2}$ holds with equal probability, thus \[\E[B|A=a]=\frac{1}{2}\cdot2a+\frac{1}{2}\cdot\frac{a}{2}=\frac{5}{4}a.\]
However, the player has received an envelope at random and only knows the contents of that particular envelope. So he could also have been given envelope $B$ with value $b$, for which $\E[A|B=b]=\frac{5}{4}b$ holds. Therefore no matter what envelope the player receives, he should switch. This solution must clearly be wrong and it is. When conditioning on $A=a$, then either $b=2a$ or $a=2b$ holds with probability $1$ in the conditioned probability space. This renders the previous calculation of $\E[B|A=a]$ to be incorrect. However, we do not know which event takes place.

Now let $x=\min\{a,b\}$ be the lowest value, then either $a=x$ or $a=2x$ holds with equal probability. For both envelopes we now have
\[
\E[A|B=b]=\E[B|A=a]=\frac{1}{2}x+\frac{1}{2}\cdot2x=\frac{3}{2}x.
\]
Both envelopes thus have on average the same contents, as is expected when the envelopes are handed out uniformly. Unfortunately, we do not know the value of $x$. Furthermore, if $x$ is picked from an unbounded set and we assume that given an observation the other envelope must hold twice as much or half the value with equal probability, then $x$ must be taken from a uniform probability distribution on the unbounded set. Such distribution does not exist. We conclude that the value of $\E[B|A=a]$ does depend on a prior distribution on the chosen value $x$.

There are two ways to address this problem. Using safe probability, like in the case of Monty Hall, we will device a strategy where the player wins $\frac{3}{2}\E[X]$ on average, where $X$ is any prior for the lowest value in the envelopes on the positive number line with finite expectation. This strategy is in fact that the player must flip a fair coin and switch when it lands heads. Another method is using a switching strategy introduced by McDonell and Abbott~\cite{McDonnell09,Abbott10,McDonnell11}. Let $f\colon (0,\infty)\to[0,1]$ be a function that takes an observation $a$ and equips it with a probability. The player must switch with probability $f(a)$ when observing value $a$. When $f$ is a decreasing function and strictly decreases on an interval where prior $X$ has positive measure, then switching using $f$ will always return on average a higher value than $\frac{3}{2}\E[X]$. If $f$ is a threshold switch, for example $f=\1_{(0,a^*]}$, then for some priors $X$ the strategy $f$ is actually the best strategy available. However, when playing the game, the player does not know how much strategy $f$ outperforms switching using safe probability. For any means prior $X$ gives $f$ an arbitrarily small advantage over safe probability, making it an unnecessarily complicated method to marginally increase the average value won.

The two envelope problem will be discussed further in chapter~\ref{chap:TwoEnvelope}.


\begin{comment}
A third still unsolved problem is the Sleeping Beauty problem. In this problem Sleeping Beauty is willing to do an experiment. At Sunday she goes to sleep. A fair coin is tossed with the following results: Sleeping Beauty wakes on Monday with heads and wakes on Monday and Tuesday with tails. If she is awake, she must state her credence of the coin giving heads. After the question, she goes to sleep using an amnesia-inducing drug which makes her forget that she has waken before. At Wednesday she wakes and the experiment is over.

A first argument of the probability distribution of the coin is given by the halfers. When Sleeping Beauty wakes, she has no clue what day it is. She knows that the coin is fair and has no reason guess otherwise, thus she must say the the coin is still fair.\\
Another argument is given by the thirders. Namely that there are three possible events, namely awake on Monday and coin has heads, awake on Monday and coin has tails and as last awake on Tuesday and coin has tails. She does not know which event she attends, thus the events happen with uniform probability. There is one event with the coin giving heads, thus the probability of the coin having heads is 33\$.

I propose a solution that the thirders use a different model than the halfers. The question concerns the probability distribution of the coin, which remains 50\%. The thirders answer the question `what is the probability of guessing correctly if I guess that the coin landed heads', which is 33\%. This solution will be discussed with more depth in chapter~\ref{chap:SleepingBeauty}.
\end{comment}

Every problem has a different reason for getting paradoxical results. There is one recurring theme with all paradoxes, namely when analysing these problems most of the times the underlying $\sigma$-algebra is not taken into account. This yields to various conflicting results, as conditioning on events that cannot be conditioned on to not recognizing that multiple probability distributions are possible. The theme of is thesis this therefore that when doing probability theory, the underlying probability space and $\sigma$-algebra must never be ignored and not providing a sub-$\sigma$-algebra with a conditional distribution must become a bad habit instead of an accepted practice.

\chapter{The Borel-Kolmogorov paradox}\label{chap:BorelKolmogorov}
The first paradox we will study in more detail is the Borel-Kolmogorov paradox. It is first studied by Borel \cite{Borel09} and Kolmogorov \cite{Kolmogorov33} and has sparked debate over the centuries afterwards\footnote{Bronnen lezen en invoeren.}. We will take a look at the following version of the paradox.

Suppose a random variable has a uniform probability distribution on the unit sphere. If one conditions on a great circle, what will the resulting conditional distribution be? If the great circle is viewed as a longitude, the conditional distribution will be uniform as well. However, if the great circle is modelled as a meridian, the conditional distribution has a cosine as density function. We have two different conditional distributions on the same set, thus which one is correct?

The basic solution to the paradox is to use measure-theoretic conditional probability, so that one is given not just a set like a great circle to condition on but rather a set and an accompanying sub-$\sigma$-algebra containing this set. The definition of measure-theoretic conditional probability now provides an answer that is unique and meaningful up to a set of zero measure. When conditional probability is defined traditionally based on density functions, it is defined on every point even though each such point has measure zero. One may now ask the following question: can we generally extend the measure-theoretic conditional probability to be defined on all points once a sub-$\sigma$-algebra is provided, by treating the conditional probability at that point as a limit of traditional conditional probabilities where one conditions on smaller and smaller sets? In section~\ref{sec:BorelCombining} we provide conjecture~\ref{con:BorelConjecture} that would imply this, but we will then show this conjecture is wrong. If we condition on both a given meridian and a suitably rotated longitude, then two phenomena occur:
\begin{enumerate}
\item the only sub-$\sigma$-algebra that we can provide is the full Borel $\sigma$-algebra and conditioning on the full $\sigma$-algebra leads to no effect and
\item the required limit is undefined as the limit of smaller and smaller meridians takes on a different value than the limit of smaller and smaller rotated longitudes.
\end{enumerate}

Kolmogorov \cite{Kolmogorov33} pointed out that the great circle has zero measure and according to him conditioning on a great circle must not be allowed. Our analysis will concern conditioning on zero sets using measure-theoretic conditional expectations as such conditioning is defined there. The article of Gyenis, Hofer-Szabó and Rédei \cite{Gyenis17} is followed to compute the conditional distributions on the great circle, viewing that circle both as longitude and as meridian. In their article the same conclusion as in \cite{Kolmogorov33} is drawn but with a more elaborate explanation. We will take it one step further to also consider the $\sigma$-algebra of both the meridians and longitudes and to consider conditional distributions on rotated longitudes making them coincide with meridians. We will then conclude that once again conditioning on measure zero sets must not be admissible, as it will lead to contradicting results.

After drawing these conclusions, we will rename the Borel-Kolmogorov paradox to the \emph{Borel-Kolmogorov phenomenon}, as it is more an example on why you should not condition on zero sets than it is a paradox.

\section{Conditional expectation}
First we need to define conditional expectations. We will use the definitions given by David Williams \cite{Williams91}.

Let $\Omega$ be an arbitrary sample space and let $\mathcal{F}$ be a $\sigma$-algebra on $\Omega$. The indicator function $\1_A$ on a set $A$ is defined as
\begin{equation}
\1_A(x)=\begin{cases}1,&x\in A,\\0,&x\not\in A.\end{cases}
\end{equation}
Let $\B(A)$ be the $\sigma$-algebra of all Borel-measurable sets on $A\subseteq\R^n$. We will start with the definition of a random variable.

\begin{definition}[Random variable]
Let $\X$ be a measurable space. A function $X\colon\Omega\to\X$ is an \emph{$\X$-valued random variable} if it is $\F$-measurable, thus if $X^{-1}(A)\in\F$ holds for all $A\in\G$ where $\G$ is a $\sigma$-algebra on $\X$.
\end{definition}

We can now define the conditional expectation as definition 9.2 of \cite{Williams91}.

\begin{definition}[Conditional expectation]\label{def:conexp}
Let $X$ be an $\F$-measurable random variable with finite $\E[|X|]$. Let $\G$ be a sub-$\sigma$-algebra of $\F$. There exists a random variable $Y$ such that 
\begin{enumerate}
\item $Y$ is $\G$-measurable,
\item $\E[|Y|]$ is finite,
\item for every $G\in\G$ we have 
\begin{equation}
\int_G Yd\P=\int_G Xd\P.
\end{equation}
\end{enumerate}
$Y$ is called a \emph{version of the conditional expectation} $\E[X|\G]$ of $X$ given $\G$, written as $Y=\E[X|\G]$ almost surely. Moreover, if $\tilde{Y}$ is another random variable with these properties, then $\tilde{Y}=Y$ equal almost surely. Since two versions of $\E[X|\G]$ coincide almost surely, $Y$ is also called \emph{the} conditional expectation $\E[X|\G]$.
\end{definition}

Note that when $\G=\sigma(Z)$ is the smallest $\sigma$-algebra generated by a set $Z\in\F$, we also write $\E[X|\G]=\E[X|Z]$. This generalizes to $\E[X|\G]=\E[X|Z_1,Z_2,\ldots]$ when $\G=\sigma(Z_1,Z_2,\ldots)$.

Definition~\ref{def:conexp} extends very nicely to the definition of conditional probability. Note that $\E[\1_F]=\P[F]$ holds for all $F\in\F$ and this carries over to conditional probabilities.

\begin{definition}[Conditional probability]
If $\G$ is a sub-$\sigma$-algebra of $\F$ and $F\in\F$, then the \emph{conditional probability} $\P[F|\G]$ is a version of $\E[\1_F|\G]$.
\end{definition}

\subsection{Comparison to traditional definitions}
The traditional definition of traditional density-based conditional expectation and probability is the following: when $X$ and $Y$ are two random variables on $\R$, $f_{X,Y}$ is the joint density function of $X$ and $Y$ and $f_Y$ is the density function of $X$, the conditional expectation $\E[X|Y=y]$ is defined as
\begin{equation}
\E[X|Y=y]=\int_\R x\frac{f_{X,Y}(x,y)}{f_Y(y)}dx.
\end{equation}
Definition~\ref{def:conexp} agrees on this traditional usage although it can be modified on a set of measure zero, as we will see now. Take $\G=\sigma(\{Y=y\})$. The smallest $\sigma$-algebra generated by all $\omega\in\R$ such that $Y(\omega)=y$ and define
\begin{equation}
g(y)=\int_\R x\frac{f_{X,Y}(x,y)}{f_Y(y)}dx,
\end{equation}
then $g(Y)$ is a version of $\E[X|Y]$. The proof and a more general statement can be found in section~9.6 of \cite{Williams91}.

The traditional conditional expectation can also be extended to traditional conditional probability.
\begin{definition}[Traditional conditional probability]
Let $(\Omega,\F, \P)$ be a probability space. Let $F\in\F$ be with positive measure and let $E\in\F$ be measurable. The \emph{traditional conditional probability} of $E$ given $F$ is given by
\begin{equation}
\P[E|F]=\frac{\P[E\cap F]}{\P[F]}.
\end{equation}

If $F$ has measure zero, the traditional conditional probability of $E$ given $F$ is undefined.
\end{definition}

\section{Formal description of the probability space}
\begin{figure}
\centering{
\input{figures/BorelTransform.tikz}
}
\caption{A visualization of coordinate transformation~\eqref{eq:BorelPolar} from Euclidean to spherical coordinates. The rectangle is the set $S$. The horizontal lines in the rectangle like $C$ parametrize the longitudes. The vertical lines in the rectangle like $M$ parametrize the meridians. Note that the two blue vertical lines of $M$ are distance $\pi$ apart, this is needed to fully parametrize a meridian.}
\label{fig:BorelVis}
\end{figure}

To analyse the Borel-Kolmogorov paradox, we need to formalize our probability space. The following construction is visualized in figure~\ref{fig:BorelVis}. Let $S$ be the unit sphere. For easier calculations, we equip $S$ with polar coordinates. Define $S=[0,2\pi)\times[0,\pi]$, then $S$ is described in the Euclidean space with the function
\begin{equation}\label{eq:BorelPolar}
S\to\R^3:(\phi,\psi)\mapsto(\cos\phi\sin\psi,\sin\phi\sin\psi,\cos\psi).
\end{equation}
Let $\B=\B(S)$ be the Borel-$\sigma$-algebra on $S$. The uniform distribution on $S$ is defined as
\begin{equation}
\P[B]=\frac{1}{4\pi}\iint_B\sin\psi d\psi d\phi
\end{equation}
for a $B\in\B$. The triple $(S,\B,\P)$ form a probability space.

The set of longitudes is described by 
\begin{equation}
\mathcal{C}=\{[0,2\pi)\times\{\psi\}\mid\psi\in[0,\pi]\}
\end{equation}
and will be horizontal lines in the rectangle of figure~\ref{fig:BorelVis}. The set of meridians is described by
\begin{equation}
\mathcal{M}=\{\{\phi,\phi+\pi\}\times[0,\pi]\mid\phi\in[0,\pi)\}
\end{equation}
and will be the vertical lines in the rectangle of figure~\ref{fig:BorelVis}.

Note that the function in \eqref{eq:BorelPolar} is not a bijection. The image of $S$ is not one-to-one on the north and south pole, but this is a null set and will not cause any problems in our case. The reason why \eqref{eq:BorelPolar} is not made a formal bijection is that it eases notation.

\section{Conditional distributions}
We will now explore various ways to describe the conditional distribution on a great circle.
\subsection{Traditional conditional probability}\label{sec:BorelNaive}
The first method is the naive method using traditional conditional probability. The probability space here is $(S,\B,\P)$. Let $F\in\B$ be a great circle.

Note that $\P[F]=0$, as a great circle always has zero measure. To be precise, let $f\colon S\to\R^3$ be the coordinate transformation of \eqref{eq:BorelPolar}. All great circles $F'$ have a rotation $O\colon\R^3\to\R^3$ such that $(f^{-1}\circ O\circ f)(F')=[0,2\pi)\times\left\{\frac{\pi}{2}\right\}$. Rotations are orthogonal and have determinant $1$, thus
\begin{align}
\P[F]&=\frac{1}{4\pi}\iint_F\sin\psi d\psi d\phi\\
&=\frac{1}{4\pi}\int_0^{2\pi}\int_{\frac{1}{2}\pi}^{\frac{1}{2}\pi}\sin\psi \det(f^{-1}\circ O\circ f)d\psi d\phi\\
&=\frac{1}{4\pi}\int_0^{2\pi}\int_{\frac{1}{2}\pi}^{\frac{1}{2}\pi}\sin\psi d\psi d\phi=0.
\end{align}
The inverse $f^{-1}$ is well-defined here since $f$ is a bijection locally around the circle $[0,2\pi)\times\left\{\frac{\pi}{2}\right\}$. Let $E\subset F$ be a measurable subset of $F$, then the traditional conditional probability of $E$ given $F$ equals
\begin{equation}
\P[E|F]=\frac{\P[E\cap F]}{\P[F]},
\end{equation}
which is not defined as $\P[F]=0$.

Thus the traditional interpretation of conditional probability will not give an answer. Furthermore, a single great circle does not yield enough information to compute any conditional probability. In terms of definition~\ref{def:conexp} the sub-$\sigma$-algebra considered here is $\G=\{\emptyset,S,B,S\setminus B\}$, which turns out to be too small.

\subsection{Conditional probability on longitudes}\label{sec:BorelLong}
Since four-element sub-$\sigma$-algebras are too small, we need to condition on larger sub-$\sigma$-algebras. One option is the $\sigma$-algebra of longitudes. Let
\begin{equation}
\mathfrak{C}=\sigma\left(\left\{[0,2\pi)\times A\mid A\in\B([0,\pi])\right\}\right)
\end{equation}
be the $\sigma$-algebra of all measurable subsets of longitudes. We can then calculate the conditional expectation of $\E[X|\mathfrak{C}]$.

\begin{proposition}
Let $X$ be $\B$-measurable. The conditional expectation of $\E[X|\mathfrak{C}]$ is given by
\begin{equation}
\E[X|\mathfrak{C}](\phi,\psi)=\frac{1}{2\pi}\int_0^{2\pi}X(\phi',\psi)d\phi'
\end{equation}
with $(\phi,\psi)\in S$.
\end{proposition}
\begin{proof}
The proof is taken from \cite{Gyenis17}. Take $A\in\B([0,2\pi))$ and consider $C=[0,2\pi)\times A\in\mathfrak{C}$. Since $\P$ is the uniform measure on the surface of the unit sphere, we have 
\begin{equation}
\int_C Xd\P=\frac{1}{4\pi}\int_A\int_0^{2\pi}X(\phi,\psi)\sin\psi d\phi d\psi
\end{equation}
by the standard spherical to Euclidean coordinate transformation. We can now apply the same coordinate transformation on the integral of $\E[X|\mathfrak{C}]$:
\begin{equation}
\int_C\E[X|\mathfrak{C}]d\P=\frac{1}{4\pi}\int_A\int_0^{2\pi}\E[X|\mathfrak{C}](\phi,\psi)\sin\psi d\phi d\psi.
\end{equation}
Filling in $\E[X|\mathfrak{C}]$ and rewriting yields
\begin{align}
\int_C\E[X|\mathfrak{C}]d\P&=\frac{1}{4\pi}\int_A\int_0^{2\pi}\E[X|\mathfrak{C}](\phi,\psi)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_A\int_0^{2\pi}\frac{1}{2\pi}\left(\int_0^{2\pi}X(\phi',\psi)d\phi'\right)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_A\left(\int_0^{2\pi}\frac{1}{2\pi}d\phi\right)\int_0^{2\pi} X(\phi',\psi)\sin\psi d\phi' d\psi\\
&=\frac{1}{4\pi}\int_A\int_0^{2\pi}X(\phi,\psi)\sin\psi d\phi d\psi=\int_C Xd\P.
\end{align}
Note that $\mathfrak{C}$ is generated by sets like $C$, thus $\E[X|\mathfrak{C}]$ is a well-defined version of the $\mathfrak{C}$-conditional expectation of $X$.
\end{proof}

This derivation is slightly different from the one in \cite{Gyenis17}. The corrections on \cite{Gyenis17} are given in appendix~\ref{app:CorLong}.

As we now have a $\mathfrak{C}$-conditional expectation, we can look at the measure space $(S,\mathfrak{C})$. Let $\psi'\in(0,\pi)$ be arbitrary, then from $\mathfrak{C}$ we can take a longitude $C=[0,2\pi)\times\{\psi'\}$ and a measurable arc $A=[\phi_1,\phi_2]\times\{\psi'\}\subset C$. Let $\P'$ be the probability measure taking $1$ on $C$ and $0$ on $C^c$, then the conditional probability $\bar{\P}$ of points being on $A$ given there is a point on $C$ is
\begin{align}
\bar{\P}[A]&=\int_S\P[A|\mathfrak{C}]d\P'=\frac{1}{2\pi}\int_S\int_0^{2\pi} \1_A(\phi',\psi)d\phi' d\P'(\phi,\psi)\label{eq:BorelLongConProb}\\
&=\frac{1}{2\pi}\left(\int_{S\setminus C}\int_{\phi_1}^{\phi_2}0d\phi' d\P'+\int_{C}\int_{\phi_1}^{\phi_2}1d\phi' d\P' \right)\\
&=\frac{\phi_2-\phi_1}{2\pi}.
\end{align}
Thus the conditional probability distribution on $C$ is uniform, resulting in all longitudes having uniform conditional probability measure.

\subsection{Conditional probability on meridians}\label{sec:BorelMer}
A great circle can not only be described as a longitude, but also as a meridian. Therefore it is interesting whether describing great circles as meridians yield to the same conditional distribution. Let 
\begin{equation}
\mathfrak{M}=\sigma(\{A\times[0,\pi]\mid A\in\B([0,2\pi))\})
\end{equation}
be the $\sigma$-algebra of measurable subsets of meridians. We can now calculate the conditional expectation $\E[X|\mathfrak{M}]$.
\begin{proposition}
Let $X$ be $\B$-measurable. The conditional expectation of $\E[X|\mathfrak{M}]$ is given by
\begin{equation}
\E[X|\mathfrak{M}](\phi,\psi)=\frac{1}{2}\int_0^\pi X(\phi,\psi')\sin\psi'd\psi'
\end{equation}
with $(\phi,\psi)\in S$.
\end{proposition}
\begin{proof}
The proof is largely taken from \cite{Gyenis17}. Let $A\in\B([0,2\pi))$ be measurable and consider $M=A\times[0,\pi]\in\mathfrak{M}$. Coordinate transformation between polar coordinates and the uniform measure on the circle $\P$ and further rewrites yield
\begin{align}
\int_M \E[X|\mathfrak{M}]d\P&=\frac{1}{4\pi}\int_0^\pi\int_A\E[X|\mathfrak{M}](\phi,\psi)\sin\psi d\phi d\psi\\
&=\frac{1}{4\pi}\int_0^\pi\int_A\left(\frac{1}{2}\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'\right)\sin\psi d\phi d\psi\\
&=\frac{1}{8\pi}\left(\int_0^\pi\sin\psi d\psi\right)\int_A\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'd\phi\\
&=\frac{1}{4\pi}\int_A\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'd\phi\\
&=\int_M Xd\P.
\end{align}

Note that $\mathfrak{M}$ is generated by sets like $M$, thus $\E[X|\mathfrak{M}]$ is a well-defined version of the $\mathfrak{M}$-conditional expectation of $X$.
\end{proof}

Note that this representation is different from equations (81) and (112) of \cite{Gyenis17}, where equation 81 is
\begin{equation}
\E[X|\mathfrak{M}](\phi,\psi)=\frac{1}{2}\int_0^{2\pi}X(\phi,\psi')|\sin\psi'|d\psi'
\end{equation}
and equation 112 is
\begin{equation}
\E[X|\mathfrak{M}](\phi,\psi)=\int_0^{\pi}X(\phi,\psi')\sin\psi'd\psi'.
\end{equation}
The corrections on \cite{Gyenis17} are given in appendix~\ref{app:CorMer}. We can verify that our version is the correct one.\\
Take first the meridian $M=\{\phi',\phi'+\pi\}\times[0,\pi]\in\mathfrak{M}$ with $\phi'\in[0,\pi)$. Let $\psi_1^*,\psi_2^*\in\R$ be arbitrary with $\psi_2^*-2\pi\leq\psi_1^*\leq\psi_2^*$, define the angles $\psi_1=\psi_1^*\mod2\pi$ and $\psi_2=\psi_2^*\mod2\pi$ and define arc $A\subseteq M$ as
\begin{equation}
A=\begin{cases}
\{\phi'\}\times[\psi_1,\psi_2],&\psi_1,\psi_2\leq\pi,\\
\{\phi'\}\times[\psi_1,\pi]\cup\{\phi'+\pi\}\times[\psi_2-\pi,\pi],&\psi_1\leq \pi, \psi_2>\pi,\\
\{\phi'+\pi\}\times[\psi_1-\pi,\psi_2-\pi],&\psi_1,\psi_2>\pi,\\
\{\phi'\}\times[0,\psi_1]\cup\{\phi'+\pi\}\times[0,\psi_2-\pi],&\psi_2\leq\pi,\psi_1>\pi.
\end{cases}
\end{equation}
This definition is exhaustive, yet it provides all possible arcs on a meridian while restricting ourselves to the domain $[0,2\pi)\times[0,\pi]$. Now, analogous to the longitudes, let $\P'$ be the uniform measure taking $1$ on meridian $M$ and $0$ on $M^c$. The conditional probability of $\hat{\P}$ of points being on $A$ with $\psi_1,\psi_2\leq\pi$ given there is a point on $M$ is given by
\begin{align}
\hat{\P}[A]&=\int_S\P[A|\mathfrak{M}]d\P'=\frac{1}{2}\int_S\int_0^\pi\1_A(\phi,\psi')\sin\psi'd\psi'd\P'(\phi,\psi)\\
&=\frac{1}{2}\left(\int_{S\setminus M}\int_{\phi_1}^{\phi_2}0d\psi'd\P'+\int_{M}\1_{\{\phi'\}\times[0,\pi]}\int_{\phi_1}^{\phi_2}\sin\psi'd\psi'd\P'\right)\\
&=\frac{1}{4}\int_{\phi_1}^{\phi_2}\sin\psi'd\psi'd\P'=\frac{\cos\psi_1-\cos\psi_2}{4}
\end{align}
since $\int_{\{\phi'\}\times[0,\pi]}d\P'=\frac{1}{2}$. On the other possible arcs of $M$ the probability $\hat{\P}[A]$ with $\psi_1,\psi_2$ as in the definition of $A$ becomes
\begin{equation}\label{eq:BorelMerConProb}
\hat{\P}[A]=\begin{cases}
\frac{1}{4}\left(\cos\psi_1-\cos\psi_2\right),&\psi_1,\psi_2\leq\pi,\\
\frac{1}{4}\left(2+\cos\psi_1-\cos\psi_2\right),&\psi_1\leq \pi, \psi_2>\pi,\\
\frac{1}{4}\left(\cos\psi_2-\cos\psi_1\right),&\psi_1,\psi_2>\pi,\\
\frac{1}{4}\left(2-\cos\psi_1+\cos\psi_2\right),&\psi_2\leq\pi,\psi_1>\pi.
\end{cases}
\end{equation}
Now one can immediately check that $\hat{\P}$ is well-defined on $M$ as
\begin{equation}
\hat{\P}[M]=\int_S\P[A|\mathfrak{M}]d\P'=\frac{1}{2}\int_M\int_0^\pi\sin\psi'd\psi'd\P'=-\frac{1}{2}\left(\cos\pi-\cos0\right)=1.
\end{equation}

Clearly this conditional distribution on meridians is not uniform. However, one should expect they are, as a meridian is just a rotation of a longitude and the points on the sphere are spread uniformly. An explanation of this difference is given in section~\ref{sec:BorelExplained}.

\subsection{Combining longitudes and meridians}\label{sec:BorelCombining}
Another question one could ask is the following: if I define $\Sigma=\sigma(\mathfrak{C},\mathfrak{M})$ as the smallest $\sigma$-algebra containing both measurable subsets of meridians and longitudes, what is then the conditional probability distribution on a great circle given $\Sigma$? The answer is that in this approach the distributions in sections~\ref{sec:BorelLong} and~\ref{sec:BorelMer} can be recovered as limiting distributions of a sequence of traditional conditional probabilities defined in section~\ref{sec:BorelNaive}.

First we'll further analyse the new $\sigma$-algebra $\Sigma$. Consider an arbitrary rectangle $(a,b)\times(c,d)\subset[0,2\pi)\times[0,\pi]$. Since by definition $(a,b)\times[0,\pi]\in\mathfrak{M}$ and $[0,2\pi)\times(c,d)\in\mathfrak{C}$, we have
\begin{equation}
(a,b)\times(c,d)=\left((a,b)\times[0,\pi]\right)\cap\left([0,2\pi)\times(c,d)\right)\in\Sigma.
\end{equation}
Thus all Borel-measurable sets on our sphere are contained in $\Sigma$. Furthermore, as $\mathfrak{C}$ and $\mathfrak{M}$ contain only Borel-measurable sets, $\Sigma=\sigma(\mathfrak{C},\mathfrak{M})$ can only have Borel-measurable sets. Therefore $\Sigma=\B$ is the $\sigma$-algebra of Borel-measurable sets on the sphere.

Now take the following approach. Pick $x\in(0,\pi)$. Define the two rectangles $C_\epsilon=[0,2\pi)\times[x-\epsilon,x+\epsilon]$ and $R_\epsilon=[a,b]\times[x-\epsilon,x+\epsilon]\subseteq C_\epsilon$ for all ${\epsilon\in(0,\min\{\pi-x,x\})}$, then what is the limiting conditional probability of $R_0$ given $C_0$? The conditional probability of $R_0$ given $C_0$ can be modeled as the limit of the sequence $\P[R_\epsilon|C_\epsilon]$ with $\epsilon\to0$, which will take on the uniform distribution on $C_0$ as calculated in equation~\ref{eq:BorelLongConProb}.
\begin{proposition}\label{prop:BorelLongBayes}
Define the two rectangles $C_\epsilon=[0,2\pi)\times[x-\epsilon,x+\epsilon]$ and $R_\epsilon=[a,b]\times[x-\epsilon,x+\epsilon]\subset C_\epsilon$ for all $\epsilon\in(0,\min\{\pi-x,x\})$ with $x\in(0,\pi)$. The limiting conditional probability of $R_0$ given $C_0$ equals
\begin{equation}
\P[R_0|C_0]=\lim_{\epsilon\downarrow0}\P[R_\epsilon|C_\epsilon]=\frac{b-a}{2\pi}.
\end{equation}
\end{proposition}
\begin{proof}
Let $\epsilon\in(0,\min\{\pi-x,x\})$. Let $C_\epsilon=[0,2\pi)\times[x-\epsilon,x+\epsilon]\in\mathfrak{C}$. The probability $\P[R_\epsilon|C_\epsilon]$ equals
\begin{equation}
\P[R_\epsilon|C_\epsilon]=\frac{\P[R_\epsilon\cap C_\epsilon]}{\P[C_\epsilon]}=\frac{\int_{a}^{b}\int_{x-\epsilon}^{x+\epsilon}\sin\psi d\psi d\phi}{\int_{0}^{2\pi}\int_{x-\epsilon}^{x+\epsilon}\sin\psi d\psi d\phi}=\frac{b-a}{2\pi}.
\end{equation}
Thus $\P[R_\epsilon|C_\epsilon]=\frac{b-a}{2\pi}$ holds for all possible $\epsilon$, resulting in the limiting value
\begin{equation}
\P[R_0|C_0]=\lim_{\epsilon\downarrow0}\P[R_\epsilon|C_\epsilon]=\frac{b-a}{2\pi}.
\end{equation}
\end{proof}

A same proposition can be found for the meridians.

\begin{proposition}\label{prop:BorelMerBayes}
Let $x\in(0,2\pi)$ and $\epsilon\in(0,\min\{2\pi-x,x\})$. Consider $M_\epsilon=[x-\epsilon,x+\epsilon]\times[0,\pi]$ and $R_\epsilon=[x-\epsilon,x+\epsilon]\times[a,b]\subseteq M_\epsilon$, then the limiting conditional probability of $R_0$ given $M_0$ is
\begin{equation}
\P[R_0|M_0]=\lim_{\epsilon\downarrow0}\P[R_\epsilon|M_\epsilon]=\frac{1}{2}(\cos a-\cos b)
\end{equation}
\end{proposition}
\begin{proof}
The conditional probability $\P[R_\epsilon|M_\epsilon]$ equals
\begin{equation}
\P[R_\epsilon|M_\epsilon]=\frac{\P[R_\epsilon\cap M_\epsilon]}{\P[M_\epsilon]}=\frac{\int_{x-\epsilon}^{x+\epsilon}\int_a^b\sin\psi d\psi d\phi}{\int_{x-\epsilon}^{x+\epsilon}\int_0^{\pi}\sin\psi d\psi d\phi}=\frac{1}{2}\left(\cos a-\cos b\right).
\end{equation}
The limit of $\epsilon\downarrow0$ remains $\P[R_0|M_0]=\frac{1}{2}\left(\cos a-\cos b\right)$.
\end{proof}

Note that the probability in proposition~\ref{prop:BorelMerBayes} has $\frac{1}{2}$ as normalization factor instead of $\frac{1}{4}$, as $M_\epsilon$ converges to only a half meridian. By symmetry the distribution on the whole meridian follows the absolute cosine function.

The conditional probabilities on $\mathfrak{C}$ and $\mathfrak{M}$ therefore can be approached by traditional conditional probabilities on $\B$. This gives rise to the following conjecture.
\begin{conjecture}\label{con:BorelConjecture}
Let $(X,\F,\P)$ be a probability space. Let $F\in\mathcal{F}$ have zero measure and let $E\subseteq F$ be measurable. Let $\G\subset\F$ be a sub-$\sigma$-algebra containing $E$ and $F$ with $\G\neq\F$. Let $\{F_n\}_{n\in\N}\subset\G$ be a sequence converging to $F$ and $\{E_n\}_{n\in\N}\subset\G$ be a sequence converging to $E$ with for all $n\in\N$ the sets $E_n$ and $F_n$ have positive measure, $E_n\subseteq F_n$ holds and the limit $\lim_{n\to\infty}\P[E_n|F_n]$ exists. Then
\begin{equation}
\P[E|F]:=\lim_{n\to\infty}\P[E_n|F_n]=\int_X\E[\1_F|\mathcal{G}]d\P'(E)
\end{equation}
where $\P'$ is the probability measure taking $1$ on $F$ and $\E[\cdot|\G]$ is a version of the conditional expectation on $\G$.
\end{conjecture}

If conjecture~\ref{con:BorelConjecture} is true, then all conditional probabilities on null sets can be computed by limiting traditional conditional probabilities. It seems to hold as for the Borel-Kolmogorov paradox when considering longitudes we can take $\F=\B$ and $\G=\mathfrak{C}$ and when considering meridians we can take $\F=\B$ and $\G=\mathfrak{M}$. It however is false as the following proposition will point out.

\begin{proposition}\label{prop:BorelConFalse}
Conjecture~\ref{con:BorelConjecture} is false.
\end{proposition}
\begin{proof}
In sections \ref{sec:BorelLong} and \ref{sec:BorelMer} we have seen that the parametrization using longitudes gave the uniform distribution and the parametrization using meridians gave a cosine. However, in section~\ref{sec:BorelLong} the set $\{0,2\pi\}\times\left\{\frac{\pi}{2}\right\}$ is considered, where section~\ref{sec:BorelMer} treats the set $\{0,\pi\}\times[0,\pi]$. Those sets differer, however we can find a set which has two different probabilities using a rotation.

Call $f$ the transformation from polar to Euclidean coordinates from equation~\ref{eq:BorelPolar}. Let $O$ be a rotation such that
\begin{equation}
(f^{-1}\circ O\circ f)\left([0,\pi]\times\left\{\frac{\pi}{2}\right\}\right)=\{\pi\}\times[0,\pi],
\end{equation}
thus where the largest half longitude is rotated to a half meridian. Recall that $\mathfrak{C}$ is the $\sigma$-algebra of longitudes. We now rotate those longitudes to get the following $\sigma$-algebra:
\begin{equation}
\mathfrak{C}'=\left\{F'\in\B\mid F'=\left(f^{-1}\circ O^{-1}\circ f\right)(F),F\in\mathfrak{C}\right\}.
\end{equation}
Now $F=\{\pi\}\times[0,\pi]$ is an element of both $\mathfrak{C}'$ and $\mathfrak{M}$. We will prove that different methods of converging to $F$ yield to different $\P[E|F]$ for a measurable $E$. We will take the set $E=\{\pi\}\times\left[\frac{1}{4}\pi,\frac{3}{4}\pi\right]$ as our example.

First take a look at $\mathfrak{C}'$. Let $\epsilon\in\left(0,\frac{\pi}{2}\right)$ be arbitrary and consider the sets $C_\epsilon=[0,\pi]\times\left[\frac{\pi}{2}-\epsilon,\frac{\pi}{2}+\epsilon\right]$ and $R_\epsilon=\left[\frac{1}{4}\pi,\frac{3}{4}\pi\right]\times\left[\frac{\pi}{2}-\epsilon,\frac{\pi}{2}+\epsilon\right]$. Let $\{R'_{n^{-1}}\}_{n\in\N}$ be with $R'_{n^{-1}}=(f^{-1}\circ O^{-1}\circ f)(R_{n^{-1}})$ and let the sequence $\{C'_{n^{-1}}\}_{n\in\N}$ be with $C'_{n^{-1}}=(f^{-1}\circ O^{-1}\circ f)(C_{n^{-1}})$. Since $R_{n^{-1}},C_{n^{-1}}\in\mathfrak{C}$ holds for all $n\in\N$, we get $\{R'_{n^{-1}}\}_{n\in\N},\{C'_{n^{-1}}\}_{n\in\N}\subset\mathfrak{C'}$. Furthermore, $f^{-1}\circ O^{-1}\circ f$ has determinant $1$ since $O$ is a rotation and $f$ is almost surely a bijection on $F$, resulting to
\begin{align}
\P[R'_\epsilon|C'_\epsilon]&=\frac{\P[R'_\epsilon]}{\P[C'_\epsilon]}=\frac{\iint_{R'_\epsilon}\sin\psi d\psi d\phi}{\iint_{C'_\epsilon} \sin\psi d\psi d\phi}=\frac{\iint_{R_\epsilon}g(\sin\psi)\cdot1 d\psi d\phi}{\iint_{C_\epsilon}g(\sin\psi)\cdot1d\psi d\phi}\\
&=\frac{\int_{\frac{1}{4}\pi}^{\frac{3}{4}\pi}\int_{\frac{\pi}{2}-\epsilon}^{\frac{\pi}{2}+\epsilon}g(\sin\psi)\cdot1 d\psi d\phi}{\int_{0}^{\pi}\int_{\frac{\pi}{2}-\epsilon}^{\frac{\pi}{2}+\epsilon}g(\sin\psi)\cdot1d\psi d\phi}=\frac{\frac{3}{4}\pi-\frac{1}{4}\pi}{\pi}=\frac{1}{2}
\end{align}
where $g(x)$ is the result of the transformation $f^{-1}\circ O^{-1}\circ f$ on the integrand. The function $g$ is not written out explicitly, as the integral can be split up as a multiplication of integrals and the resulting integrals in the numerator en denominator with $g$ in the integrand are equal, thus cancel out. The result is $\P[R'_0|C'_0]=\frac{1}{2}$, as $\P[R'_\epsilon|C'_\epsilon]$ is constant in $\epsilon$. The sequence $\{R'_{n^{-1}}\}_{n\in\N}$ converges to $E$ and the sequence $\{C'_{n^{-1}}\}_{n\in\N}$ converges to $F$, thus we have $\P[E|F]=\frac{1}{2}$ as well.

For $\mathfrak{M}$ we will use proposition~\ref{prop:BorelMerBayes}. Let $x=\pi$ and $\epsilon\in(0,\pi)$, then we have $M_\epsilon\in\mathfrak{M}$ and $R_\epsilon=[\pi-\epsilon,\pi+\epsilon]\times\left[\frac{1}{4}\pi,\frac{3}{4}\pi\right]\subset M_\epsilon$. Furthermore, the sequence $\{M_{n^{-1}}\}_{n\in\N}$ converges to $F$ and $\{R_{n^{-1}}\}_{n\in\N}$ converges to $E$, thus proposition~\ref{prop:BorelMerBayes} states that
\begin{equation}
\P[E|F]=\frac{1}{2}\left(\cos\left(\frac{1}{4}\pi\right)-\cos\left(\frac{3}{4}\pi\right)\right)=\frac{\sqrt{2}}{2}.
\end{equation}

We now have both $\P[E|F]=\frac{1}{2}$ and $\P[E|F]=\frac{1}{2}\sqrt{2}$, which are clearly not equal. Conjecture~\ref{con:BorelConjecture} is therefore false.
\end{proof}

This demonstrates that when having an $F\in\F$ with zero measure, one cannot uniquely define $\P[E|F]$. A conditional expectation needs to be accompanied by a sub-$\sigma$-algebra, otherwise contradicting results can arise.

Conjecture~\ref{con:BorelConjecture} is however useful to guess a conditional expectation. The measure-theoretic definition of conditional expectation is only a list of properties, not a constructive definition. In the case of a $\mathfrak{C}$-conditional expectation, looking at proposition~\ref{prop:BorelLongBayes} and its proof we observe that the resulting probability is $\frac{b-a}{2\pi}$, thus uniform. Furthermore, $\phi$ is the only variable that is integrated, thus one can guess $\phi$ is the only needed variable. Therefore one can suspect that
\begin{equation}
\E\left[\1_{[a,b]\times\left\{\frac{\pi}{2}\right\}}\middle|\mathfrak{C}\right](\phi,\psi)=\frac{1}{2\pi}\int_0^{2\pi}\1_{[a,b]\times\left\{\frac{\pi}{2}\right\}}(\phi',\psi)d\phi'=\frac{b-a}{2\pi}\1_{\left\{\frac{\pi}{2}\right\}}(\psi)
\end{equation}
holds and the guess can be generalized to
\begin{equation}
\E[X|\mathfrak{C}](\phi,\psi)=\frac{1}{2\pi}\int_0^{2\pi}X(\phi',\psi)d\phi'.
\end{equation}
This is in fact the same $\mathfrak{C}$-conditional expectation of $X$ by proposition~\ref{prop:BorelLongBayes}. The same steps can be taken to guess $\E[X|\mathfrak{M}]$ using proposition~\ref{prop:BorelMerBayes}. 

Thus we can conclude that it is wise to look at a converging sequence of traditional conditional probabilities for a potential conditional expectation when conditioning on zero sets. However, it must be noted that the resulting conditional probability is not uniquely defined, must be checked whether it complies to the actual definition of conditional probability and depends on the chosen sub-$\sigma$-algebra.

\section{The Borel-Kolmogorov paradox explained}\label{sec:BorelExplained}
Following the arguments of \cite{Gyenis17} we will argue that the difference in conditional distribution is no paradox, but a misinterpretation of conditional probability.

A first explanation is quite intuitive. All meridians intersect at the north and south pole of the sphere, whereas longitudes do not intersect. Therefore the $\sigma$-algebras $\mathfrak{M}$ and $\mathfrak{C}$ are vastly different. If points are spread uniformly on the meridians, the distribution of mass on the whole sphere will not be uniform and the density of mass will be highest at the poles. This is simulated by \cite{Weisstein} and his simulations support this statement.

Let $\bar{\P}$ be the conditional probability measure on a longitude, defined by equation~\ref{eq:BorelLongConProb} and let $\hat{\P}$ be the conditional probability measure on a meridian, defined by equation~\ref{eq:BorelMerConProb}. If the spaces $(S, \mathfrak{C}, \bar{\P})$ and $(S, \mathfrak{M}, \hat{\P})$ are isomorphic with a measurable bijection, then $\hat{\P}$ and $\bar{\P}$ must have equal distribution and the results of sections~\ref{sec:BorelLong} and \ref{sec:BorelMer} must be paradoxical and contradictory. If there is a measurable bijection from a meridian to a longitude, their conditional probability distributions must agree. This is not the case here.
\begin{theorem}[\cite{Gyenis17}]
Let $f\colon S\to S$ be a measurable bijection with measurable inverse. There is no Boolean algebra isomorphism $h_f\colon\mathfrak{C}\to\mathfrak{M}$ that is determined by $f$.
\end{theorem}
\begin{proof}
The proof can be found in \cite{Gyenis17}, but we will repeat it here. Suppose such isomorphism $h_f$ exists. All longitudes $C$ in $\mathfrak{C}$ are the only atoms of $\mathfrak{C}$, therefore $h_f(C)$ are the only atoms of $\mathfrak{M}$ as well, making $h_f(C)$ meridians. We will now prove that $h_f(C)$ cannot be a meridian.

Let $m_0=\{(0,0), (0,\pi)\}$ be the set of north and south poles. Let $c_0\in\mathfrak{C}$ be such that $h_f(c_0)=m_0$. Then $c_0$ consists of two elements as well, thus we can choose longitude $C$ not on the north or south poles such that $C\cap c_0=\emptyset$. Note that $h_f(C)$ is a meridian, thus $m_0\subset h_f(C)$ as all meridians pass through the north and south poles. Since $h_f$ is a Boolean algebra isomorphism, we have $h_f(C\cap c_0)=h_f(C)\cap h_f(c_0)$. This all can be combined in the following line:
\begin{equation}
\emptyset=h_f(\emptyset)=h_f(C\cap c_0)=h_f(C)\cap h_f(c_0)=h_f(C)\cap m_0=m_0.
\end{equation}
This is clearly a contradiction.

Let $C$ be the north or south pole of the sphere. Since $h_f$ is a bijection, $h_f(C)$ is only a single point as well. Thus $h_f(C)$ cannot be an atom of $\mathfrak{M}$. Therefore there is a contradiction here as well.

Thus no single atom of $\mathfrak{C}$ result into an atom of $\mathfrak{M}$ by $h_f$ and therefore function $h_f$ cannot exist.
\end{proof}

Therefore every measurable bijection between $(S,\mathfrak{C})$ and $(S,\mathfrak{M})$ has no Boolean algebra isomorphism $h_f\colon\mathfrak{C}\to\mathfrak{M}$ such that longitudes in $\mathfrak{C}$ are mapped to meridians on $\mathfrak{M}$. This theorem holds on all subalgebras of $\mathfrak{C}$ and $\mathfrak{M}$ as well, as is proven by \cite{Gyenis17}.

Now it should be clear why the conditional distribution on the longitudes and meridians of the sphere differ, since we are dealing with two entirely different structured spaces. Therefore, the question `why is the conditional distribution on the longitudes different from the conditional distribution on the meridians' is easy to answer; difference in spaces. To verify that two different methods of conditioning on the same set yield to the correct conditional probability, one must ask the following question: `if one models a distribution on the sample space using conditional expectation on a sub-$\sigma$-algebra, does the resulting conditional probability distribution extend to the original distribution on the whole sample space?' As earlier mentioned, Weisstein \cite{Weisstein} demonstrated that the answer to that question is yes. The uniform distribution on longitudes and the cosine on meridians both extend to the uniform distribution on the whole sphere. Thus when dealing with different conditional distributions on measure zero sets, one should rather ask a question like the last one, as the answer to that question must always be yes. Further reading and a more in-depth analysis can be found in \cite{Gyenis17}.


\section{Conclusion}
After analysing the Borel-Kolmogorov paradox, the first and most important conclusion we can take is that when conditioning, especially on a set $F$ of measure zero, one needs to give the accompanying sub-$\sigma$-algebra of the event that is conditioned on. Otherwise the Borel-Kolmogorov phenomenon appears, where one can give different sub-$\sigma$-algebras containing $F$ such that the resulting conditional probability distribution is different.

Secondly, we must accept that conditional probability on sets of measure zero are not uniquely defined. That uniqueness only appears when conditioning on sets of positive measure, as then the classical rule $\P[E|F]=\frac{\P[E\cap F]}{\P[F]}$ gives a version of the conditional probability. If a sub-$\sigma$-algebra can be used to model the entire sample space, for example the sub-$\sigma$-algebra of longitudes with the sphere as sample space, then the conditional probability must be able to extend to the original probability distribution. For example providing the uniform distribution on all longitudes gives back the uniform distribution on the sphere, as well as applying the cosine distribution on the meridians results into the uniform distribution on the sphere.

Thirdly, a conditional distribution on a zero set must be calculated using the measure-theoretic conditional expectation. Conjecture~\ref{con:BorelConjecture} can help with finding the correct conditional expectation, however proposition~\ref{prop:BorelConFalse} proves that the resulting conditional probability does not need to be unique. Therefore conjecture~\ref{con:BorelConjecture} cannot be used to calculate a conditional probability and then call it a day, you need to check whether the distribution you found actually satisfies all conditions of conditional probability.

At last, as Kolmogorov \cite{Kolmogorov33} already stated in his work, there is nothing paradoxical going on in the Borel-Kolmogorov paradox. The space of the sphere with the longitudes as $\sigma$-algebra is not homeomorphic with the space of the sphere and the meridians as $\sigma$-algebra. The not-uniqueness of the conditional probability on a zero set is thus as expected and we must give the sub-$\sigma$-algebra when conditioning on sets of measure zero.


\chapter{Safe probability}\label{chap:SafeProp}
To introduce safe probability, take a look at a game of dice. This game is devised by Grünwald \cite{Grunwald13}. A player and game master are present and the game master casts a fair die. He peeks the value of the die and must either state that the value is 4 or lower or state that the value is 3 or higher. The game master is not able to lie. Upon hearing the game master's advice, what is the probability the die has landed 4?\\
Suppose an alternative game is considered, where the game master can only say that the value is either from 1 to and including 4 or the value is 5 or 6. The question of the probability of the die having 4 becomes easy to answer. When 5 or 6 is revealed, the answer is 0 and when 1 to 4 is revealed, Bayes gives us that the probability is 25\%.\\
However, when either `4 and lower' or `3 and higher' is stated, the game master can choose his statement when 3 or 4 is cast. His method of choosing the statement influences the probability of the die having cast 4 upon hearing the game master's statement.

What can we thus state about the probability of the die having 4? Traditional conditional probability will not suffice hear. If we loose our notion of conditional probability, we can resort to \emph{safe probability}. Safe probability is introduced by Grünwald in 2018 \cite{Grunwald18}.\\
Suppose there are two random variables, $U$ and $V$, and we want to use the distribution $U|V$, but there is no method of finding this distribution. Then one creates a model of all possible probability distributions containing hopefully the correct distribution as well. A new conditional distribution is created, not necessarily one from our model, which gives a safe probability of $U$ given $V$ and performs equal for all distributions in our model.\\
The advantage is that, while not knowing the correct probability distribution, we can use the safe distribution as guess. This guess will perform equal against all distributions in our model. We will make this more precise.
\section{Definitions}
Before we will define safe probability, we need to introduce some notation. A set of probability distributions $\Pmod$ will be called our model.

\begin{definition}[Range and support]
Let $S\colon\X\to\Y$ be a random variable. The \emph{range} of $S$ is
\begin{equation}
\range(S)=\{s\in\Y\mid s=S(x),x\in\X\}.
\end{equation}
Let $\P$ be a probability distribution on $\X$. The \emph{support} of $S$ under $\P$ is
\begin{equation}
\supp_{\P}(S)=\{s\in \Y\mid\P[S=s]>0\}.
\end{equation}
\end{definition}

Let $U$ and $V$ be two random variables and let $f$ be a function such that $f(U)=V$. We will then write $U\stackrel{f}{\rightsquigarrow}V$. If a function $f$ exists such that $U\stackrel{f}{\rightsquigarrow}V$, we will write $U\rightsquigarrow V$. Random variable $V$ is then called a \emph{coarsening} of $U$ or equivalently we state that $U$ determines $V$.

Lastly, if we keep $U$ and $V$ as random variables, the distribution of $U$ under $\P$ is written as $\P[U]$. The distribution of $U$ given $V$ under $\P$ is written as $\P[U|V]$.

We are now able to define conditional probability. The following definitions and propositions are all taken from \cite{Grunwald18}.

\begin{definition}[Safe probabilities]\label{def:SafeProp}
Let $\Omega$ be a sample space and let $\Pmod$ be a set of distributions on $\Omega$. Let $U$ be a real-valued random variable with finite expectation and let $V$ be a random variable on $\Omega$. Let $\Psafe$ be a probability distribution on $\Omega$. Then $\Psafe$ is \emph{safe} for
\begin{itemize}
    \item $\langle U\rangle|\langle V\rangle$ if $\E_{\P}[U]=\E_{\P}[\E_{\Psafe}[U|V]]$ holds for all $\P\in\Pmod$. This $\Psafe$ is called \emph{unbiased} for $U|V$.
    \item $\langle U\rangle|[V]$ if $\E_{\P}[U]=\E_{\Psafe}[U|V=v]$ holds for all $v\in\supp_{\Psafe}(V)$. This $\Psafe$ is called \emph{marginally valid} for $U|V$. 
\end{itemize}
\end{definition}
\begin{definition}[Stronger safety]\label{def:SafeStrongProp}
Let $\Omega,\Pmod,U,V$ and $\Psafe$ as above. Then $\Psafe$ is \emph{safe} for
\begin{itemize}
    \item $U|\langle V\rangle$ if for all random variables $U'$ on $\Omega$ with $U\rightsquigarrow U'$ distribution $\Psafe$ is safe for $\langle U'\rangle|\langle V\rangle$.
    \item $U|[V]$ if for all random variables $U'$ on $\Omega$ with $U\rightsquigarrow U'$ distribution $\Psafe$ is safe for $\langle U'\rangle|[V]$.
\end{itemize}
\end{definition}

Both definitions introduce a lot of new notation. In practice, $\langle V\rangle$ means that $\Psafe$ must be unbiased. More specifically, $\langle V\rangle$ implies that $\E_{\Psafe}[U|V]$ must be an unbiased estimator for $U$ for all $\P\in\Pmod$.\\
When $[V]$ is considered, the expected value of $U|V=v$ under $\Psafe$ must equal the expected value of $U$ under $\P$ for all $v\in\supp_{\P}(V)$ and $\P\in\Pmod$. On average, when considering $\Psafe$ the outcome of $V$ will not influence the the expectation value of $U$.\\
Lastly, the difference between $\langle U\rangle$ and $U$ is that for $\langle U\rangle$ the requirement for safety does only have to be met for $U$. When $U$ is considered, the requirement must be met for all coarsenings $V$ of $U$.

When we say that $\Psafe$ is safe for $U|[V]$, we actually mean that `for all $\P\in\Pmod$ we have $\supp_{\P}(V)\subseteq\supp_{\Psafe}(V)$, $\E_{\P}[U]$ is well-defined, $\E_{\Psafe}[U|V=v]$ is well-defined for all $v\in\supp_{\Psafe}(V)$, both expectations are equal and this holds for all $U'$ with $U\rightsquigarrow U'$'. This statement is quite lengthy, therefore we will abbreviate it by stating that $\Psafe$ is safe for $U|[V]$.

Definition~\ref{def:SafeStrongProp} is not easy to work with. The following proposition from \cite{Grunwald18} gives us tools to actually create $\Psafe$ that is safe for $U|\langle V\rangle$ or $U|[V]$.

\begin{proposition}[Basic interpretations of safety]\label{prop:SafeProperties}
Let $\Omega,\Pmod,U,V$ and $\Psafe$ as above. Then
\begin{enumerate}
    \item $\Psafe$ is safe for $U|\langle V\rangle$ if and only if for all $\P\in\Pmod$ there is a distribution $\P'$ on $\Omega$ with $\P'[U=u,V=v]=\Psafe[U=u|V=v]\P[V=v]$ for all $(u,v)\in\range((U,V))$ such that $\P'[U]=\P[U]$.
    \item $\Psafe$ is safe for $\langle U\rangle|V$ if and only if 
    \begin{equation}
    \E_{\P}[U|V]=\E_{\Psafe}[U|V]
    \end{equation}
    holds with probability $1$ for all $\P\in\Pmod$. This $\Psafe$ is called \emph{squared error-optimal} for $U|V$.
    \item $\Psafe$ is safe for $U|V$ if and only if 
    \begin{equation}
    \P[U|V]=\Psafe[U|V]
    \end{equation}
    holds with probability $1$ for all $\P\in\Pmod$. This $\Psafe$ is called \emph{valid} for $U|V$.
    \item $\Psafe$ is safe for $U|[V]$ if and only if $\P[U]=\Psafe[U|V=v]$ for all $\P\in\Pmod$ and $v\in\supp_{\Psafe}(V)$.
\end{enumerate}
\end{proposition}
\begin{proof}
The proof is in \cite{Grunwald16}, section A.2.
\end{proof}

It is easy to see that safety for $\langle U\rangle|[V]$ implies safety for $\langle U\rangle|\langle V\rangle$. If we have $\E_{\Psafe}[U|V=v]=\E_{\P}[U]$ for all $v\in\supp_{\Psafe}(V)$, then
\begin{equation}
\E_{\P}[\E_{\Psafe}[U|V]]=\E_{\P}[\E_{\P}[U]]=\E_{\P}[U]
\end{equation}
must hold. All implications are stated in the following proposition.

\begin{figure}
\centering{
\input{figures/SafeDiagram.tikz}
}
\caption{A diagram of implications between safe probabilities. If $\Psafe$ is safe for X, then it is safe for all statements that can be reached from X as well.}
\label{fig:SafeDiagram}
\end{figure}

\begin{proposition}\label{prop:SafeImply}
Let $U$, $V$ and $\Psafe$ be as above.
\begin{enumerate}
\item If $\Psafe$ is safe for $U|V$, it is safe for $\langle U\rangle|V$ and $U|[V]$.
\item If $\Psafe$ is safe for $U|[V]$, it is safe for $U|\langle V\rangle$ and $\langle U\rangle|[V]$.
\item If $\Psafe$ is safe for $\langle U\rangle|V$, $U|\langle V\rangle$ or $\langle U\rangle|[V]$, it is safe for $\langle U\rangle|\langle V\rangle$.
\end{enumerate}
All relations are visualised in figure~\ref{fig:SafeDiagram}.
\end{proposition}
\begin{proof}
All implications are found and proven in \cite{Grunwald18}.
\end{proof}

\section{The dice game}\label{sec:SafeDice}
Recall the dice game from the introduction of this chapter, introduced by \cite{Grunwald13}. The game master casts a fair die and observes a value. This value is either in the set $\{1,2,3,4\}$ or in the set $\{3,4,5,6\}$. The game master must reveal one of the two sets to the player and cannot lie. The player must then guess the probability that the die has landed on $4$.

It is insufficient to take $\Omega=\{1,2,3,4,5,6\}$ as our sample space. Let $U$ be a random variable denoting the die's value and let $V$ be the statement of the game master, then $\P[U=3|V=\{1,2,3,4\}]$ cannot be calculated. When the die has rolled to a $3$, the game master can choose between $\{1,2,3,4\}$ and $\{3,4,5,6\}$ and this must be taken into account. Furthermore, the $\sigma$-algebra containing both $\{1,2,3,4\}$ and $\{3,4,5,6\}$ is
\begin{equation}
\G=\sigma(\{1,2\},\{3,4\},\{5,6\}),
\end{equation}
thus $\{3\}$ is no member of $\G$. When adding $\{3\}$ to $\G$, we get $\G'=\sigma(\G\cup\{3\})=2^\Omega$, the power set of $\Omega$. Therefore we cannot condition in $\Omega$ to compute $U=3$ given $V=\{1,2,3,4\}$.

Therefore we need to extend our sample space. We will use the extention of \cite{Grunwald13}. Take $\X=\{1,2,3,4,5,6\}$ and $\Y=\{\{1,2,3,4\},\{3,4,5,6\}\}$ and let our sample space be $\Omega=\X\times\Y$. Let $U$ be an $\X$-valued random variable and let $V$ be a $\Y$-valued random variable. In this space conditioning on $\{1,2,3,4\}$ is valid. Let $\P$ be a probability measure with the uniform distribution on $U$. To model the strategy of the game master, we need $p,q\in[0,1]$ such that 
\begin{align}
\P[V=\{3,4,5,6\}|U=3]&=p,&\P[V=\{3,4,5,6\}|U=4]&=q.
\end{align}

We can now for example calculate the probability of the die landing on $6$ after the game master reveals $\{3,4,5,6\}$. This is done by \cite{Grunwald13} and it states that
\begin{equation}\label{eq:SafeDiceCon6}
\P[U=6|V=\{3,4,5,6\}]=\frac{1}{p+q+2}.
\end{equation}
Thus the conditional probability of rolling $6$ ranges from $\frac{1}{4}$ to $\frac{1}{2}$ depending on the game masters strategy.

Can we create a distribution $\Psafe$ such that the probability of $U=3$ given $V$ is independent of $p$ and $q$? The answer is yes and we need to use safe probability. Consider as model
\begin{equation}
\Pmod=\left\{\P\middle|\forall u\in\X:\P[U=u]=\frac{1}{6},\forall v\in\Y:\P[U\in v|V=v]=1\right\}.
\end{equation}
As we do not know the strategy of the game master when $3$ or $4$ is rolled. We do know that the die is fair and that the game master is not able to lie, thus we put this information in our model.

For shorthand purposes, we write $y_1=\{1,2,3,4\}$ and $y_2=\{3,4,5,6\}$ such that $\Y=\{y_1,y_2\}$ from now on.

\begin{proposition}\label{prop:SafeDice}\footnote{Herschrijf bewijs naar equivalenties.}
Let $\X$, $\Y$, $\Omega$, $U$, $V$ and $\Pmod$ be as before. Let 
\begin{equation}
\tilde{\mathcal{P}}=\left\{\P\middle|
\begin{array}{l}
\P[U=1|V=y_1]=\P[U=2|V=y_1],\\
\P[U=3|V=y_1]=\frac{1}{2}-5\P[U=1|V=y_1],\\
\P[U=5|V=y_2]=\P[U=6|V=y_2],\\
\P[U=3|V=y_2]=3\P[U=5|V=y_2]+\frac{1}{2},\\
\P[U=1|V=y_1],\P[U=5|V=y_1]\in\left[0,\frac{1}{10}\right]
\end{array}
\right\}
\end{equation}
be a set of probability distributions on $\Omega$ and let $\Psafe$ be a probability distribution on $\Omega$. The following are equivalent:
\begin{enumerate}
    \item $\Psafe$ is safe for $\langle U\rangle|\langle V\rangle$.
    \item $\Psafe$ is safe for $\langle U\rangle|[V]$.
    \item $\Psafe$ is a member of $\tilde{\mathcal{P}}$.
\end{enumerate}

No distribution from $\tilde{\mathcal{P}}$ is safe for $U|[V]$.
\end{proposition}
\begin{proof}
The proof is by construction. We first consider safety for $\langle U\rangle|\langle V\rangle$, as that will be the easiest method to create $\tilde{\mathcal{P}}$. We will then check the created $\tilde{\mathcal{P}}$ against safety for $\langle U\rangle|[V]$. Let $\Psafe$ be an arbitrary distribution. Let $\P\in\Pmod$ be arbitrary and take $p,q\in[0,1]$ with $\P[V=y_2|U=3]=p$ and $\P[V=y_2|U=4]=q$.

Definition~\ref{def:SafeProp} states that we need $\E_{\P}[\E_{\Psafe}[U|V]]=\E_{\P}[U]$. The right-hand side is easy to compute:
\begin{equation}
\E_{\P}[U]=\sum_{u=1}^6 u\P[U=u]=\frac{1+2+3+4+5+6}{6}=\frac{7}{2}.
\end{equation}
For $\E_{\P}[\E_{\Psafe}[U|V]]$, we first focus on $\P[V=y_2]$. Conditioning on $U$ yields

\begin{equation}
\P[V=y_2]=\sum_{u=1}^6\P[V=y_2|U=u]\P[U=u]=\frac{1}{6}\sum_{u=1}^6\P[V=y_2|U=u].
\end{equation}
The game master cannot lie, thus $\P[V=y_2|U=5]=\P[V=y_2|U=6]=1$ and $\P[V=y_2|U=1]=\P[V=y_2|U=2]=0$ immediately hold. We further have $\P[V=y_2|U=3]=p$ and $\P[V=y_2|U=4]=q$, thus the value of $\P[V=y_2]$ is

\begin{equation}
\P[V=y_2]=\frac{1}{6}\sum_{u=1}^6\P[V=y_2|U=u]=\frac{2}{3}+\frac{p+q}{6}.
\end{equation}
Since $\Y$ only has two elements, $\P[V=y_1]=\frac{1}{3}-\frac{p+q}{6}$ follows. We can now write out $\E_{\P}[\E_{\Psafe}[U|V]]$, however we must shorten our notation. Let $\Psafe[u|v]$ be shorthand for $\Psafe[U=u|V=v]$ for all $(u,v)\in\Omega$. Writing out yields
\begin{align}
\E_{\P}[\E_{\Psafe}[U|V]]&=\E_{\Psafe}[U|V=y_1]\P[V=y_1]+\E_{\Psafe}[U|V=y_2]\P[V=y_2]\\
&=\left(\Psafe(1|y_1)+2\Psafe(2|y_1)+3\Psafe(3|y_1)+4\Psafe(4|y_1\right)\left(\frac{2}{3}-\frac{p+q}{6}\right)\\
&\phantom{=}+\left(3\Psafe(3|y_2)+4\Psafe(4|y_2)+5\Psafe(5|y_2)+6\Psafe(6|y_2\right)\left(\frac{1}{3}+\frac{p+q}{6}\right)\\
&=\frac{p+q}{6}\left(\sum_{u=3}^{6}u\Psafe[u|y_2]-\sum_{u=1}^4y\Psafe[u|y_1]\right)+\sum_{u=1}^4\frac{2u}{3}\Psafe[u|y_1]\\
&\phantom{=}+\sum_{u=3}^6\frac{u}{3}\Psafe[u|y_2].
\end{align}
As we have $\E_{\P}[U]=\frac{7}{2}$, we need to get rid of the dependence on $p+q$. This results into the condition
\begin{equation}
\sum_{u=3}^{6}u\Psafe[u|y_2]=\sum_{u=1}^4u\Psafe[u|y_1].
\end{equation}
When this condition is satisfied, we need 
\begin{equation}
\sum_{u=1}^4\frac{2u}{3}\Psafe[u|y_1]+\sum_{u=3}^6\frac{u}{3}\Psafe[u|y_2]=\frac{7}{2}
\end{equation}
for $\Psafe$ to be safe for $\langle U\rangle|\langle V\rangle$. We impose one further assumption on our safe probability to reduce the amount of possible safe probabilities. Suppose the die shows $5$ or $6$, the game master has to tell $y_2$. Thus it is logical to assume that $\Psafe[5|y_2]=\Psafe[6|y_2]$ and $\Psafe[1|y_1]=\Psafe[2|y_1]$, as when an $y_2$ is revealed there is no reason why $5$ is preferred more than $6$. Also as the game master cannot lie, thus
\begin{equation}
\Psafe[5|y_1]=\Psafe[6|y_1]=\Psafe[1|y_2]=\Psafe[2|y_2]=0
\end{equation}
can be assumed. This all results into the following system of equations:
\begin{equation}\label{eq:SafeDiceSystem}
\begin{pmatrix}
\frac{2}{3}&\frac{4}{3}&2&\frac{8}{3}&1&\frac{4}{3}&\frac{5}{3}&2\\
-1&-2&-3&-4&3&4&5&6\\
1&-1&0&0&0&0&0&0\\
0&0&0&0&0&0&1&-1\\
1&1&1&1&0&0&0&0\\
0&0&0&0&1&1&1&1
\end{pmatrix}\begin{pmatrix}
\Psafe[1|y_1]\\\Psafe[2|y_1]\\\Psafe[3|y_1]\\\Psafe[4|y_1]\\
\Psafe[3|y_2]\\\Psafe[4|y_2]\\\Psafe[5|y_2]\\\Psafe[6|y_2]
\end{pmatrix}=\begin{pmatrix}
\frac{7}{2}\\0\\0\\0\\1\\1
\end{pmatrix}.
\end{equation}
The set $\tilde{\mathcal{P}}$ is the set of all probability distributions that are solution to this system. As $\Psafe\in\tilde{\mathcal{P}}$ needs to be a valid probability distribution, we need to have $\Psafe[u|v]\in[0,1]$ for all $(u,v)\in\Omega$. Solving \eqref{eq:SafeDiceSystem} gives
\begin{align}
\Psafe[3|y_1]&=3\Psafe[1|y_1]+\frac{1}{2},\\
\Psafe[4|y_1]&=5\Psafe[1|y_1]-\frac{1}{2},
\end{align}
thus $\Psafe[3|y_1]\in[0,1]$ holds when $\Psafe[1|y_1]\in\left[0,\frac{1}{6}\right]$ and $\Psafe[4|y_1]\in[0,1]$ holds when $\Psafe[1|y_1]\in\left[0,\frac{1}{10}\right]$. Without loss of generality $\Psafe[5|y_2]\in\left[0,\frac{1}{10}\right]$ must hold as well. This proves the last requirement $\Psafe[1|y_2],\Psafe[5|y_2]\in\left[0,\frac{1}{10}\right]$ from $\tilde{\mathcal{P}}$ and completes our proof that all distributions from $\tilde{\mathcal{P}}$ are safe for $\langle U\rangle|\langle V\rangle$.

We will now focus on safety for $\langle U\rangle|[V]$. Proposition~\ref{prop:SafeImply} implies that a distribution from $\tilde{\mathcal{P}}$ can be safe for $\langle U\rangle|[V]$. Take an arbitrary $\Psafe\in\tilde{\mathcal{P}}$. The requirement
\begin{equation}
\E_{\Psafe}[U|V=y_2]=\sum_{u=3}^{6}u\Psafe[u|y_2]=\sum_{u=1}^4u\Psafe[u|y_1]=\E_{\Psafe}[U|V=y_1]
\end{equation}
is already satisfied for $\Psafe$, thus we only need to prove that $\E_{\Psafe}[U|V=y_2]=\frac{7}{2}$. Writing out and using properties of $\Psafe$ results to
\begin{align}
\E_{\Psafe}[U|V=y_2]&=3\left(3\Psafe[5|y_2]+\frac{1}{2}\right)+4\left(\frac{1}{2}-5\Psafe[5|y_2]\right)+11\Psafe[5|y_2]\\
&=\frac{7}{2}+20\Psafe[5|y_2]-20\Psafe[5|y_2]=\frac{7}{2},
\end{align}
proving that all distributions in $\tilde{\mathcal{P}}$ are safe for $\langle Y\rangle|[X]$ as well.

Lastly we turn to safety for $U|[V]$. Pick $\Psafe\in\tilde{\mathcal{P}}$ arbitrarily. Proposition~\ref{prop:SafeProperties} implies $\Psafe[1|y_1]=\frac{1}{6}$, which is a clear contradiction to $\Psafe[1|y_1]\in\left[0,\frac{1}{10}\right]$. Thus there is no distribution $\Psafe\in\Pmod$ that is safe for $Y|[X]$.
\end{proof}

Does a distribution exist that is safe for $U|[V]$? Suppose $\Psafe$ is safe for $U|[V]$, then proposition~\ref{prop:SafeProperties} states that for all $v\in\Y$ we must have $\Psafe[U=u|V=v]=\frac{1}{6}$ for all $u\in v$. However, this results to $\Psafe[U\in v|V=v]=\frac{2}{3}$ for both $v\in\Y$. Thus for our safe strategy, we must assume that the game master lies with probability~$\frac{1}{3}$. We will regard such distribution as inadmissible and conclude that no safe distribution for $U|[V]$ exists.

The original question of the dice game did not concern an overall safe probability distribution for the whole die, but rather the probability of rolling $3$. Proposition~\ref{prop:DiceIndSafe} states that a distribution $\Psafe$ is safe for $\langle\DieInd\rangle|\langle V\rangle$ as well as $\DieInd|[V]$ if and only if $\Psafe[U=3|V=v]=\frac{1}{6}$ holds for all $v\in\Y$. It is thus safe to ignore the statement of the game master when guessing the probability of the die rolling $3$ and to state that the probability of rolling $3$ remains $\frac{1}{6}$. 

A safe distribution for $\1_{\{U=1\}}|V$ cannot exist, as the following proposition will prove.

\begin{proposition}
There is no safe probability distribution for $\langle\1_{\{U=u\}}\rangle|\langle V\rangle$ with $u\in\{1,2,5,6\}$.
\end{proposition}
\begin{proof}
Pick $u=1$. As $V=y_1$ is the only possible option when rolling $1$, we have
\begin{align}
\E_{\P}[\E_{\Psafe}[\1_{\{U=1\}}|V]]&=\Psafe[U=1|V=y_1]\P[V=y_1]\\
&=\Psafe[U=1|V=y_1]\left(\frac{2}{3}-\frac{p+q}{6}\right).
\end{align}
We need $\E_{\P}[\E_{\Psafe}[\1_{\{U=1\}}|V]]=\P[U=1]=\frac{1}{6}$, but there is no possible value for $\Psafe[U=1|V=y_1]$ such that 
\begin{equation}
\Psafe[U=1|V=y_1]\left(\frac{2}{3}-\frac{p+q}{6}\right)=\frac{1}{6}
\end{equation}
holds for all $p,q\in[0,1]$.

This proof applies to all $v\in\{1,2,5,6\}$, where in the case of $v\in\{5,6\}$ we need to condition on $V=y_2$ instead of $V=y_1$. 
\end{proof}


To conclude, in the example of the dice game, we cannot compute a unique distribution for $U|V$. However, we were able to create a set of probability distributions that are all unbiased estimators for $\E_{\P}[U]$ for all $\P\in\Pmod$. When only considering the probability of the die rolling to a specific value, we can create a safe distribution for the values 3 or 4. This distribution is only safe and only an unbiased estimator when the value of $V$ is ignored, such that for example $\Psafe[U=3|V=v]=\frac{1}{3}$ for all $v\in\Y$ holds. A safe distribution for the values $1$, $2$, $5$ and $6$ does not exist.

\chapter{Discrete conditional paradoxes}\label{chap:DiscPara}
Consider the following three problems:
\begin{enumerate}
    \item[Monty Hall:] There are three doors. Two doors open to a goat and one door opens to a car. The player chooses initially a door, say door $a$. The game master then opens either door $b$ or $c$, however he never opens the door with the car. After opening a door, the player is asked if he should switch. Should the player switch to the other door?
    \item[Boy or girl:] Suppose you encounter a stranger in a bar and you get in conversation. Suddenly, the stranger states that he has two children and at least one is a girl. What is the probability he has two daughters?
    \item[Dice game:] This game is already introduced in section~\ref{sec:SafeDice}. In short a die is rolled. Only the game master is able to observe the die's value. He then tells you whether the value of the die is in the set $\{1,2,3,4\}$ or in the set $\{3,4,5,6\}$. What is the probability you rolled $3$ given the game master's statement?
\end{enumerate}
All three problems essentially have the same structure. There is a finite set of outcomes in each problem. Monty Hall's problem has three doors, the boy or girl problem has three different family compositions and there are six values on a die. The game master must reveal some information, where it is essential that there is at least one outcome that remains possible no matter what the game master reveals. For that outcome the game master is free to choose his revelation. In Monty Hall's problem if door $a$ has the car, the game master can choose to leave doors $a$ and $b$ shut or leave doors $a$ and $c$ shut. If for the boy or girl problem the stranger has one son and one daughter, he is free to choose whether at least one child is a boy or girl. If the die rolled 3 in the dice game, the game master is free to choose $\{1,2,3,4\}$ or $\{3,4,5,6\}$. However, since there is at least one outcome remaining possible no matter which revelation the game master makes, traditional conditional probability does not suffice. This is seen for the dice game in section~\ref{sec:SafeDice} where there is no sub-$\sigma$-algebra on $\{1,2,3,4,5,6\}$ that both contains the sets $\{3\}$ and the subset $\{\{1,2,3,4\},\{3,4,5,6\}$. We cannot simply use Bayes theorem anymore to compute the probability of $3$ given $\{1,2,3,4\}$, as we need to take the probability of the game master stating $\{3,4,5,6\}$ instead into account.

Therefore all three problems can be analysed using the same methods; they are essentially equal. The game master has a choice and therefore is able to employ a strategy. When using traditional conditional probability one sees that such strategy is of influence. Equation~\ref{eq:SafeDiceCon6} states for example that the probability of rolling $6$ given $\{3,4,5,6\}$ dilates from $\frac{1}{4}$ to $\frac{1}{2}$.

To counteract such strategies we need to first model all possible probability distributions. We can then create a pragmatic probability distribution where the decision maker can act on, which on average performs as well as all probability distributions in the model. We can use safe probability here.

In this chapter we first introduce theorem~\ref{thm:DiscMainThm} to create safe probability distributions in the setting of the three introduced problems. Then each problem is treated individually where theorem~\ref{thm:DiscMainThm} is applied and we will discuss the results. We will conclude by stating that there is nothing paradoxical going on with the three problems. The paradoxical statements are just misapplications of conditional probability.

\section{The main theorem}
In the general setting we consider a set $\X$ of all possible outcomes called the \emph{outcome space} and a set $\Y$ of observations the player can make called the \emph{observation space}. Our sample space then becomes $\Omega=\X\times\Y$. Let $U$ be an $\X$-valued random variable on $\X$ denoting the outcome of the game, e.g.~the door containing the car in Monty Hall or the family composition in the boy or girl problem. Let $V$ be a $\Y$-valued random variable denoting the statement of the game master. Using safe probability we will create a safe distribution for $\1_{\{U=u'\}}|[V]$ for a particular $u'\in\X$.

\begin{theorem}\label{thm:DiscMainThm}
Let $\X$ and $\Y$ be countable. Let $U$ be a random variable on $\X$ and $V$ be a random variable on $\Y$. Let $p_u\in[0,1]$ with $\sum_{u\in\X}p_u=1$ and let 
\begin{equation}
\Pmod\subseteq\{\P\mid\forall u\in\X:\P[U=u]=p_u,\E_{\P}[U]<\infty\}
\end{equation}
be our model of probability distributions on $\X\times\Y$. Suppose there is a
\begin{equation}\label{eq:DiscMainThmUPrime}
u'\in\bigcap_{\P\in\Pmod}\bigcap_{v\in\supp_{\P}(V)}\range(U|V=v).
\end{equation}
Let $\Psafe$ be a distribution on $\X\times\Y$. If $\Y=\bigcup_{\P\in\Pmod}\supp_{\P}(V)$, then the following are equivalent:
\begin{enumerate}
    \item For all $v\in\Y$ we have $\Psafe[U=u'|V=v]=p_{u'}$.
    \item $\Psafe$ is safe for $\GeneralInd|[V]$.
    \item $\Psafe$ is safe for $\langle\GeneralInd\rangle|\langle V\rangle$.
\end{enumerate}
\end{theorem}
\begin{proof}
The implication from 1 to 2 is satisfied by proposition~\ref{prop:SafeProperties}. Let $v\in\Y$ and $\P\in\Pmod$ be arbitrary, then we have
\begin{equation}
\Psafe[U=u'|V=v]=p_{u'}=\P[U=u'].
\end{equation}
Thus proposition~\ref{prop:SafeProperties} states that $\Psafe$ is safe for $\GeneralInd|[V]$.

The implication from 2 to 3 is by proposition~\ref{prop:SafeImply}.

Consider lastly the implication from 3 to 1. Safety for $\langle\GeneralInd\rangle|\langle V\rangle$ implies 
\begin{equation}
\E_{\P}[\GeneralInd]=\E_{\P}[\E_{\Psafe}[\GeneralInd|V]]
\end{equation}
for all $\P\in\Pmod$. We will construct a sufficient $\Psafe$ from this requirement and prove that $\Psafe[U=u'|V=v]=p_{u'}$ for all $v\in\Y$ is necessary.\\
Let $\P\in\Pmod$ be arbitrary. Take a first look at $\E_{\P}[\GeneralInd]$, then
\begin{equation}
\E_{\P}[\GeneralInd]=\P[U=u']=p_{u'}.
\end{equation}
Let $v\in\Y$ be arbitrary, then we can write out
\begin{equation}
\E_{\Psafe}[\GeneralInd|V=v]=\Psafe[U=u'|V=v].
\end{equation}
The expectation $\E_{\P}[\E_{\Psafe}[\GeneralInd|V]]$ can now be expanded to
\begin{align}
\E_{\P}[\E_{\Psafe}[\GeneralInd|V]]&=\sum_{v\in\supp_{\P}(V)}\E_{\Psafe}[U=u'|V=v]\P[V=v]\\
&=\sum_{v\in\supp_{\P}(V)}\Psafe[U=u'|V=v]\P[V=v].
\end{align}
We now need to abbreviate our notation. From now on we write $\Psafe[u|v]$ instead of $\Psafe[U=u|V=v]$ for all $(u,v)\in\X\times\Y$. Take a $v'\in\supp_{\P}(V)$ and define $\Y'=\supp_{\P}(V)\setminus\{v'\}$. Let $\Psafe$ be an arbitrary distribution on $\X\times\Y$, then we have
\begin{align}
\E_{\P}[\E_{\Psafe}[\GeneralInd|V]]&=\sum_{v\in\Y}\Psafe[u'|v]\P[V=v]\\
&=\Psafe[u'|v']\P[V=v']+\sum_{v\in\Y'}\Psafe[u'|v]\P[V=v]\\
&=\Psafe[u'|v']\left(1-\sum_{v\in\Y'}\P[V=v]\right)+\sum_{v\in\Y'}\Psafe[u'|v]\P[V=v]\\
&=\Psafe[u'|v']+\sum_{v\in\Y'}\left(\Psafe[u'|v]-\Psafe[u'|v']\right)\P[V=v].
\end{align}
Two conditions must be satisfied for this expectation to equal $p_{u'}$:
\begin{equation}
\begin{cases}
\sum_{v\in\Y'}\left(\Psafe[u'|v]-\Psafe[u'|v']\right)\P[V=v]=0,\\
\Psafe[u'|v']=\P[U=u'].
\end{cases}
\end{equation}
The first condition is because the expectation value should be independent of the chosen $\P\in\Pmod$, thus all terms concerning $\P$ must disappear. The second requirement forces the expectation to have value $\E_{\P}[\GeneralInd]$.\\
Note that earlier $\P\in\Pmod$ and $v'\in\supp_{\P}(V)$  were taken arbitrarily and that for all $v\in\Y$ there is a $\P\in\Pmod$ such that $\P[V=v]>0$ by $\Y=\bigcup_{\P\in\Pmod}\supp_{\P}(V)$, thus we need $\Psafe[u'|v]=\Psafe[u'|v']$ for all $v\in\Y$ to meet the first condition for all $\P\in\Pmod$. Fix now $\Psafe[u'|v']=p_{u'}$, then the second requirement is met. Note that since $u'\in\bigcap_{\P\in\Pmod}\bigcap_{v\in\supp_{\P}(V)}\range(U|V=v)$ the probabilities $\P[U=u'|V=v']>0$ and $\P[U=u']=p_{u'}>0$ are positive.\footnote{Ik ben nog niet tevreden met deze argumentatie. De voorwaarde voor $u'$ is volgens mij echt nodig, maar ik kan niet goed hard maken waarom.}\\
Therefore if $\Psafe$ is safe for $\langle\GeneralInd\rangle|\langle V\rangle$, we need $\Psafe[u'|v]=p_{u'}$ for all $v\in\Y$.
\end{proof}

It looks like theorem~\ref{thm:DiscMainThm} can only applied in a few circumstances. However, the restriction on model $\Pmod$ is in many cases quickly satisfied, as in most cases one already has an impression of the initial probability distribution on outcome space $\X$. Furthermore, the existence of $u'$ as in equation~\ref{eq:DiscMainThmUPrime} is in all three problems in the introduction the actual core of the problem. In Monty Hall's problem the $u'$ is the initially chosen door $a$, as it will remain closed for all possible strategies of the game master. In the boy or girl problem the $u'$ is the family composition of one boy and one girl, as this composition is a possibility both when the stranger tells you that he has at least one son or when he says he has at least one daughter. If a $u'$ as in equation~\ref{eq:DiscMainThmUPrime} does not exist, then either $\{\range(U|V=v)|v\in\Y\}$ is a partition of $\X$ for all $\P\in\Pmod$ or the intersection of all $\range(U|V=v)$ for all $v\in\Y$ and $\P\in\Pmod$ is empty for a different reason. If the conditional ranges always form a partition, you can use traditional conditional probability removing the need of theorem~\ref{thm:DiscMainThm}. In the second case when the intersection of all conditional ranges is empty for a different reason, you simply need to adjust the problem. Computing a safe conditional distribution for $\GeneralInd|V$ against all probabilities in $\Pmod$ is not possible when for one $\P\in\Pmod$ the probability $\P[\GeneralInd=1|V=v]=0$ is zero and for the other $\P'\in\Pmod$ the probability $\P'[\GeneralInd=1|V=v]>0$ is non-zero.

The requirement $\Y=\bigcup_{\P\in\Pmod}\supp_{\P}(V)$ is however restrictive. The following proposition drops this requirement, but the equivalence in theorem~\ref{thm:DiscMainThm} disappears as well leaving only the implications from 1 to 2 and from 2 to 3.

\begin{proposition}\label{prop:DiscSafeMargGen}
Let $\X$, $\Y$, $U$, $V$, $\{p_{u}\}_{u\in\X}$, $u'$ and $\Pmod$ be as before and let $\Psafe$ be a distribution on $\X\times\Y$. This $\Psafe$ is safe for $\GeneralInd|[V]$ if
\begin{equation}
\Psafe[U=u'|V=v]=p_{u'}
\end{equation}
holds for all $v\in\Y$.
\end{proposition}
\begin{proof}
Let $\P\in\Pmod$ and $v\in\Y$ be arbitrary, then we have
\begin{equation}
\Psafe[U=u'|V=v]=p_{u'}=\P[U=u'].
\end{equation}
Proposition~\ref{prop:SafeProperties} states that this $\Psafe$ is safe for $\GeneralInd|[V]$.
\end{proof}

Note that from the proof of proposition~\ref{prop:DiscSafeMargGen} one can deduce that the elaborate set-up in theorem~\ref{thm:DiscMainThm} is not necessarily needed and that proposition~\ref{prop:DiscSafeMargGen} is nothing more than a restricted restatement of the fourth property of proposition~\ref{prop:SafeProperties}.\\
On the other hand, pick a $v\in\Y$ suppose we can pick a different 
\begin{equation}
u^*\in\X\setminus\bigcap_{\P\in\Pmod}\supp_{\P}(U|V=v)
\end{equation}
with $p_{u^*}>0$. Now for all $\P\in\Pmod$ we have $\P[U=u^*|V=v]=0$. The safe distribution for $\1_{\{U=u^*\}}|[V]$ is now dictated by $\Psafe[U=u^*|V=v]=p_{u^*}$ according to proposition~\ref{prop:SafeProperties}, but not many will accept this as an `acceptable' safe distribution since $p_{u^*}>0$. In other words, our safe distribution will always suggest to choose $u^*$ with probability $p_{u^*}$, where actually you already know that $u^*$ is not a possibility when observing $v$. This contradicting result is a mere consequence of $\Psafe$ being a distribution on $\Omega$ instead of being a member of $\Pmod$. This is why in proposition~\ref{prop:DiscSafeMargGen} we still want to pick $u'$ as in theorem~\ref{thm:DiscMainThm}.

Theorem~\ref{thm:DiscMainThm} only considers single $u'$. We can construct safe probability distributions for $\GeneralGenInd|V$ for certain larger $\X'\subset\X$ as well in the following corollary.
\begin{corollary}\label{cor:DiscSafeGeneral}
Let $\X$, $\Y$, $U$, $V$, $\{p_u\}_{u\in\Y}$ and $\Pmod$ be as in theorem~\ref{thm:DiscMainThm}. Suppose there is a non-empty
\begin{equation}
\X'\subseteq\bigcap_{\P\in\Pmod}\bigcap_{v\in\supp_{\P}(V)}\range(U|V=v).
\end{equation}
Let $\Psafe$ be a distribution on $\X\times\Y$ with
\begin{equation}
\Psafe[U=u|V=v]=p_{u}
\end{equation}
for all $v\in\Y$ and $u\in\X'$. If $\Y=\bigcup_{\P\in\Pmod}\supp_{\P}(V)$, then the following are equivalent:
\begin{enumerate}
    \item For all $u\in\X'$ and $v\in\Y$ we have $\Psafe[U=u|V=v]=p_{u}$.
    \item $\Psafe$ is safe for $\GeneralGenInd|[V]$.
    \item $\Psafe$ is safe for $\langle\GeneralGenInd\rangle|\langle V\rangle$.
\end{enumerate}
\end{corollary}
\begin{proof}
The proof is in essence a repetition of the proof of theorem~\ref{thm:DiscMainThm} where $\GeneralGenInd=\sum_{u'\in\X'}\GeneralInd$ must be used.
\end{proof}

\subsection{Accuracy}
Suppose you are in a situation where you need to guess $\GeneralGenInd|V$ for an $\X'\subset\X$.\footnote{Begrip accuracy introduceren.} One example is the Monty Hall game where on live television you need to guess whether your door $a$ has the car, thus whether $\1_{\{U=a\}}=1$ given the opened door $V$. Let $\GeneralGenInd^*$ be a random variable taking values in $\{0,1\}$. The goal is pick a probability distribution on $\GeneralGenInd^*$ in order to maximize $\P[\GeneralGenInd^*=\GeneralGenInd]$ for all $\P\in\Pmod$. The following theorem gives the optimal distribution.
\begin{theorem}\label{thm:DiscAccOpt}
Let $\X$, $\X'$, $\Y$, $U$, $V$, $\{p_u\}_{u\in\Y}$ and $\Pmod$ be as before. The probability distribution on $\GeneralGenInd^*$ maximizing $\P[\GeneralGenInd^*=\GeneralGenInd]$ for all $\P\in\Pmod$ is
\begin{equation}\
\GeneralGenInd^*=\begin{cases}
1,&\sum_{u\in\X'}p_u>\frac{1}{2},\\
0,&\sum_{u\in\X'}p_u<\frac{1}{2},\\
\mathrm{Ber}(p),&\sum_{u\in\X'}p_u=\frac{1}{2},p\in[0,1].
\end{cases}
\end{equation}
where $\mathrm{Ber}(p)$ is the Bernoulli distribution with parameter $p$. This distribution guesses $\GeneralGenInd$ correctly with probability 
\begin{equation}
\P[\GeneralGenInd^*=\GeneralGenInd]=\max\left\{\sum_{u\in\X'}p_u,1-\sum_{u\in\X'}p_u\right\}
\end{equation}
for all $\P\in\Pmod$.
\end{theorem}
\begin{proof}
Let $\P\in\Pmod$ be arbitrary. Expanding $\P[\GeneralGenInd^*=\GeneralGenInd]$ yields
\begin{equation}
\P\left[\GeneralGenInd^*=\GeneralGenInd\right]=\sum_{k=0}^1\P\left[\GeneralGenInd^*=k\middle|\GeneralGenInd=k\right]\P\left[\GeneralGenInd=k\right]
\end{equation}
Note that $\P[\GeneralGenInd=1]=\sum_{u\in\X'}p_u$ and $\P[\GeneralGenInd=0]=1-\sum_{u\in\X'}p_u$ hold by $\P\in\Pmod$. Furthermore, as we want to guess $\GeneralGenInd$, it is not reasonable to make $\GeneralGenInd^*$ dependent on $\GeneralGenInd$. If we know the value of $\GeneralGenInd$, we can just pick that value as guess. Thus we have
\begin{equation}
\P\left[\GeneralGenInd^*=k\middle|\GeneralGenInd=k\right]=\P\left[\GeneralGenInd^*=k\right]
\end{equation}
for all $k\in\{0,1\}$. This results in the following sum:
\begin{align}
&\phantom{=}\P\left[\GeneralGenInd^*=\GeneralGenInd\right]\\
&=\P\left[\GeneralGenInd^*=1\right]\left(\sum_{u\in\X'}p_u\right)+\P\left[\GeneralGenInd^*=0\right]\left(1-\sum_{u\in\X'}p_u\right).
\end{align}
Rewrite $\sum_{u\in\X'}p_u=x$ and $\P[\GeneralGenInd^*=1]=p$. We now need to maximize
\begin{equation}
f(p):=px+(1-p)(1-x)=p(2x-1)+1-x.
\end{equation}
Note that both $x,p\in[0,1]$. Consider the following different values of $x$.
\begin{itemize}
    \item[$x>\frac{1}{2}$:] When $x>\frac{1}{2}$ holds, the function $f$ is strictly increasing in $p$. The maximum is then achieved at $p=1$, which is $p$. Thus $\1_{\{U\in\X'\}}^*=1$ must always be guessed.
    \item[$x<\frac{1}{2}$:] When $x<\frac{1}{2}$ holds, the function $f$ is strictly decreasing in $p$. The maximum is then achieved at $p=0$, which is $1-x$. Thus $\1_{\{U\in\X'\}}^*=0$ must always be guessed.
    \item[$x=\frac{1}{2}$:] When $x=\frac{1}{2}$ holds, the function $f$ is constant. Its value is $f(p)=\frac{1}{2}=x$ for all $p\in[0,1]$. Therefore we can put $\GeneralGenInd^*=\mathrm{Ber}(p)$ as guessing strategy for all $p\in[0,1]$.
\end{itemize}
Using these distributions $\P[\GeneralGenInd^*=\GeneralGenInd]$ is maximal for all $\P\in\Pmod$ with value 
\begin{equation}
\P\left[\GeneralGenInd^*=\GeneralGenInd\right]=\max\left\{\sum_{u\in\X'}p_u,1-\sum_{u\in\X'}p_u\right\}.
\end{equation}
\end{proof}

Suppose the player applies a safe distribution $\Psafe$ on his guess $\GeneralGenInd^*$ for $\GeneralGenInd$ given $V$, what will the frequency of guessing correct be? The following proposition answers this question.

\begin{theorem}\label{thm:DiscAccSafe}
Let $\X$, $\X'$, $\Y$, $U$, $V$, $\{p_u\}_{u\in\Y}$ and $\Pmod$ be as in corollary~\ref{cor:DiscSafeGeneral}. Let $\Psafe$ be a safe distribution for $\GeneralGenInd|\langle V\rangle$. Let $\GeneralGenIndSafe$ be a random variable denoting a guess for $\GeneralGenInd$ given $V$. Employ distribution $\Psafe$ on $\GeneralGenIndSafe$, such that
\begin{equation}
\P\left[\GeneralGenIndSafe=1\right]=\Psafe[\GeneralGenInd=1|V=v]=\sum_{u\in\X'}p_{u}
\end{equation}
holds for all $\P\in\Pmod$ and $v\in\Y$. Then $\GeneralGenIndSafe$ predicts $\GeneralGenInd$ correctly with probability
\begin{equation}
\P[\GeneralGenIndSafe=\GeneralGenInd]=\left(\sum_{u\in\X'}p_{u}\right)^2+\left(1-\sum_{u\in\X'}p_{u}\right)^2
\end{equation}
for all $\P\in\Pmod$.
\end{theorem}
\begin{proof}
Let $\P\in\Pmod$ be arbitrary. As in the proof of theorem~\ref{thm:DiscAccOpt} we have
\begin{equation}
\P\left[\GeneralGenIndSafe=\GeneralGenInd\right]=\sum_{k=0}^1\P\left[\GeneralGenIndSafe=k\right]\P[\GeneralGenInd=k].
\end{equation}

Pick first $k=1$. Since $\Psafe$ is safe for $\GeneralGenInd|[V]$, we have
\begin{equation}
\P\left[\GeneralGenIndSafe=1\right]=\Psafe[U\in\X']=\sum_{u\in\X'}p_u
\end{equation}
according to corollary~\ref{cor:DiscSafeGeneral}. In addition, the fact $\P\in\Pmod$ gives
\begin{equation}
\P[\GeneralGenInd=1]=\P[U\in\X']=\sum_{u\in\X'}p_u.
\end{equation}
Thus we have $\P[\GeneralGenIndSafe=1]=\P[\GeneralGenInd=1]$. Without loss of generality the same holds for $k=0$. Filling everything in concludes the proof:
\begin{align}
\P\left[\GeneralGenIndSafe=\GeneralGenInd\right]&=\sum_{k=0}^1\P\left[\GeneralGenIndSafe=k\right]\P[\GeneralGenInd=k]\\
&=\left(1-\sum_{u\in\X'}p_u\right)^2+\left(\sum_{u\in\X'}p_u\right)^2.
\end{align}
\end{proof}

Now we know when a safe distribution $\Psafe$ for $\GeneralGenInd|[V]$ results into the most accurate guess for $\GeneralGenInd$ given $V$ as well.

\begin{corollary}
Let $\X$, $\X'$, $\Y$, $U$, $V$, $\{p_u\}_{u\in\Y}$ and $\Pmod$ be as in corollary~\ref{cor:DiscSafeGeneral}. Let $\Psafe$ be safe for $\GeneralGenInd|[V]$ and let $\GeneralGenIndSafe$ be as in theorem~\ref{thm:DiscAccSafe}. The probability of guessing $\GeneralGenInd$ given $V$ correctly using $\GeneralGenIndSafe$ is only optimal when $\sum_{u\in\X'}p_u\in\left\{0,\frac{1}{2},1\right\}$.
\end{corollary}
\begin{proof}
Let $\P\in\Pmod$ be arbitrary. According to theorem~\ref{thm:DiscAccSafe} the probability of guessing $\GeneralGenInd$ given $V$ correctly using $\GeneralGenIndSafe$ is
\begin{equation}
\P[\GeneralGenIndSafe=\GeneralGenInd]=\left(\sum_{u\in\X'}p_{u}\right)^2+\left(1-\sum_{u\in\X'}p_{u}\right)^2.
\end{equation}
Theorem~\ref{thm:DiscAccOpt} states that optimal accuracy for predicting $\GeneralGenInd$ given $V$ is
\begin{equation}
\P[\GeneralGenInd^*=\GeneralGenInd]=\max\left\{\sum_{u\in\X'}p_u,1-\sum_{u\in\X'}p_u\right\}
\end{equation}
when using
\begin{equation}
\GeneralGenInd^*=\begin{cases}
1,&\sum_{u\in\X'}p_u>\frac{1}{2},\\
0,&\sum_{u\in\X'}p_u<\frac{1}{2},\\
\mathrm{Ber}(p),&\sum_{u\in\X'}p_u=\frac{1}{2},p\in[0,1].
\end{cases}
\end{equation}

Note that $\GeneralGenIndSafe$ and $\GeneralGenInd^*$ are only equal when $\sum_{u\in\X'}p_u\in\{0,1\}$ or when $\sum_{u\in\X'}p_u=\frac{1}{2}$ and $\GeneralGenInd^*=\mathrm{Ber}\left(\frac{1}{2}\right)$. The first case is quite trivial as when $\sum_{u\in\X'}p_u\in\{0,1\}$ we have $\P[\GeneralGenInd^*=\GeneralGenInd]=1$ as result.\\
When $\sum_{u\in\X'}p_u=\frac{1}{2}$ holds, we have $\P[\GeneralGenIndSafe=1]=\frac{1}{2}$, resulting into $\GeneralGenIndSafe=\mathrm{Ber}\left(\frac{1}{2}\right)$. Putting $\GeneralGenInd^*=\mathrm{Ber}\left(\frac{1}{2}\right)$ as well leads to an optimal prediction of $\GeneralGenInd$ given $V$.
\end{proof}

\section{Dice game}
Now the main theorems are stated, we can apply them to some well-known paradoxes. We will first continue our example of the dice game, started in section~\ref{sec:SafeDice}. Recall that a die is cast with values in $\X=\{1,2,3,4,5,6\}$ and the game master is after observing the die's value only able to reveal a member of the set $\Y=\{\{1,2,3,4\},\{3,4,5,6\}\}=\{y_1,y_2\}$. Let $U$ be an $\X$-valued random variable and $V$ be a $\Y$-valued random variable. The game master is not allowed to lie and the die is fair, thus any probability distribution for this game must be a member of \begin{equation}
\Pmod=\left\{\P\middle|\forall u\in\X:\P[U=u]=\frac{1}{6},\forall y\in\Y:\P[U\in y|Y=y]=1\right\}.
\end{equation}
Proposition~\ref{prop:SafeDice} gives a set $\tilde{\mathcal{P}}$ of distributions that are all safe for $\langle U\rangle|[V]$. However, in the original problem we are not interested in the whole distribution of $U$ given $V$, but only want to know the probability of rolling $3$ after the game master's statement. Therefore finding safe distributions for $\DieInd|[V]$ is sufficient for finding a safe probability of rolling $3$.

\begin{proposition}\label{prop:DiceIndSafe}
Let $\X$, $\Y$, $\Omega$, $U$, $V$ and $\Pmod$ be as in the dice game. Let $\Psafe$ be a distribution on $\Omega$. The following statements are equivalent:
\begin{enumerate}
\item For all $v\in\Y$ we have $\P[U=3|V=v]=\frac{1}{6}$.
\item $\Psafe$ is safe for $\langle \DieInd\rangle|\langle V\rangle$.
\item $\Psafe$ is safe for $\DieInd|[V]$.
\end{enumerate}

The same holds for $\1_{\{Y=4\}}$ instead of $\1_{\{Y=3\}}$.
\end{proposition}
\begin{proof}
This proposition is a direct application of theorem~\ref{thm:DiscMainThm}. The sets $\X$ and $\Y$ are respectively of sizes $2$ and $6$ and therefore countable. $U$ is a random variable on $\X$ and $V$ is a random variable on $\Y$. In this dice game we have $p_u=\frac{1}{6}$ for all $u\in\X$ as $\P\in\Pmod$ implies $\P[U=u]=\frac{1}{6}$ for all $u\in\X$. The expectation of $U$ under an arbitrary $\P\in\Pmod$ is finite since $\X$ is a finite set.

Take $\P\in\Pmod$ arbitrary. Note that in all cases we have $\supp_{\P}(V)=\{y_1,y_2\}$, as when rolling $1$ or $2$ the game master must give $y_1$ and when rolling $5$ or $6$ the game master must give $y_2$. The ranges of $U|V=v$ under $\P$ are
\begin{align}
\supp_{\P}(U|V=y_1)&=\{1,2,3,4\},&\supp_{\P}(U|V=y_2)&=\{3,4,5,6\}
\end{align}
Note that $3$ is in the intersection, thus we can take $u'=3$.

Now we can directly apply theorem~\ref{thm:DiscMainThm} to conclude the proof. Without loss of generality this proof is valid for $\1_{\{U=4\}}$ instead of $\DieInd$ as well.
\end{proof}

Suppose you want to bet on this game and want to know whether you must put money on rolling $3$. As the following proposition will state, after observing the game master's statement, always stating that the die has not rolled to a $3$ has a $\frac{5}{6}$ chance of being correct independently on the game master's strategy.

\begin{proposition}
Let $\X$, $\Y$, $U$, $V$ and $\Pmod$ be as in the dice game. Let $\DieInd^*=0$, then this $\DieInd^*$ maximizes the probability
\begin{equation}
\P\left[\DieInd^*=\DieInd\right]=\frac{5}{6}.
\end{equation}
\end{proposition}
\begin{proof}
We want to apply theorem~\ref{thm:DiscAccOpt}. As the proof of proposition~\ref{prop:DiceIndSafe} already pointed out, we can take $\X'=\{3\}\subset\X$. Furthermore, in the dice game we have $p_3=\frac{1}{6}$. Since we have $\sum_{u\in\X'}p_u=p_3=\frac{1}{6}$, theorem~\ref{thm:DiscAccOpt} states that $\DieInd^*=0$ maximizes the probability of guessing $\DieInd$ correctly. Theorem~\ref{thm:DiscAccOpt} also states that the probability of guessing $\DieInd$ correctly is
\begin{equation}
\P\left[\DieInd^*=\DieInd\right]=\max\left\{\sum_{u\in\X'}p_u,1-\sum_{u\in\X'}p_u\right\}=1-p_3=\frac{5}{6}.
\end{equation}
\end{proof}

We now observe that using the information given by the game master does not lead to a higher probability of guessing $\DieInd$ correct. When always stating that the die has not rolled $3$ you maximize the probability of having a correct guess, which is a fraction of $\frac{5}{6}$ of the played games.

We have seen that the conditional probability of rolling $3$ given the game master's statement is not uniquely defined; it dilates from $0$ to $\frac{1}{4}$ depending on the to the player unknown strategy of the game master. However, when playing like the probability of rolling $3$ is always $\frac{1}{6}$ disregarding the game master's statement, you are able to simulate the probability distribution of $\DieInd$. Furthermore, when always stating that the die has not rolled $3$, you are correct in five out of six games, which is the maximal fraction of games you can be correct on.

\section{Monty Hall}

\section{Boy or girl problem}

\section{Conclusion}

\chapter{The two envelopes problem}\label{chap:TwoEnvelope}

\bibliographystyle{alpha}
\bibliography{../Referenties/Referenties}

\appendix
\chapter{Corrections to [GHSR17]}
\section{Corrections on appendix 1}\label{app:CorLong}
\pagenumbering{Alph}
In appendix 1, equation 94 of the proof of the conditional expectation on longitudes in \cite{Gyenis17}, they claim that the volume of the unit sphere is $2\pi$, whereas they should have claimed that the surface area of the unit sphere is $4\pi$. Luckily this constant is not actually used in the computations, thus their mistake has no impact on the rest of their article.

\section{Corrections on appendix 2}\label{app:CorMer}
I suggest the following corrections on appendix 2, the proof of conditional expectation on meridians:
\begin{enumerate}
\item In the verification \cite{Gyenis17} claims that $\int_0^\pi\sin\theta d\theta=1$ holds between equations (107) and (108) and between equations (115) and (116), while that integral actually has value $2$.
\item The conditional distribution of \cite{Gyenis17} is verified on a half meridian arc on page 2614, while this distribution must be verified on a full circle in order to be compared with the longitudes. Verification on a full meridian, e.g.~computation of $q(C)$ in formula (85), quickly reveals that the conditional distribution of \cite{Gyenis17} integrates to $2$.
\item The random variable $X$ is integrated on the domain $[0,2\pi)$, while $X$ is only defined on $[0,2\pi)\times[0,\pi]\in\mathfrak{M}$. The set $A\times[0,2\pi)$ is not an element of $\mathfrak{M}$.
\item Between equations (105) and (106) of \cite{Gyenis17}, they implicitly claim that the identity $X(\phi,\theta)=X(\phi,\theta+\pi)$ holds for all $\F$-measurable $X$, where now notation of \cite{Gyenis17} is used. This is most certainly false without any more restrictions on $X$.
\item Before equation (85) a measure $q_\mathcal{M}$ is defined on a whole meridian. Since the integral is taken from $\psi=0$ to $\psi=2\pi$, the integral of $q_\mathcal{M}$ over $S$ becomes $1$. However, as pointed out earlier, $\psi>\pi$ is not in our domain. Thus the integral must be split up in two arcs with $q_{\mathcal{M}}$ taking value $\frac{1}{2}$ on each arc.
\item The normalization constant is used in equation (103) of \cite{Gyenis17} is $2\pi$, where it must be $4\pi$. This does however not impact further calculations, like the same mistake made from equation (109) to (110).
\end{enumerate}

\newpage

\printindex

\end{document}